<!DOCTYPE html><html><head><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><meta name="author" content="GaÃ«tan Renaudeau"/><meta name="description" content="Here is Beez, a web real-time audio experiment using smartphones as synthesizer effect controllers. This is our second Web Audio API experiment made in one Hackday at Zenexity."/><meta name="keywords" content="WebRTC, audio, hackday"/><meta name="HandheldFriendly" content="True"/><meta name="MobileOptimized" content="320"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="twitter:card" content="summary"/><meta name="twitter:site" content="@greweb"/><meta name="twitter:title" content="Beez, WebRTC + Audio API"/><meta name="og:title" content="Beez, WebRTC + Audio API"/><meta name="twitter:description" content="Here is Beez, a web real-time audio experiment using smartphones as synthesizer effect controllers. This is our second Web Audio API experiment made in one Hackday at Zenexity."/><meta name="twitter:creator" content="@greweb"/><meta name="og:image" content="http://greweb.me//images/2013/09/beez.png"/><meta name="twitter:image" content="http://greweb.me//images/2013/09/beez.png"/><link rel="image_src" href="http://greweb.me//images/2013/09/beez.png"/><title>@greweb - Beez, WebRTC + Audio API</title><link href="http://fonts.googleapis.com/css?family=Fredericka+the+Great|Arapey|Roboto:400,700,400italic" rel="stylesheet" type="text/css"/><link rel="stylesheet" href="https://unpkg.com/@highlightjs/cdn-assets@10.7.2/styles/default.min.css"/><script src="https://unpkg.com/@highlightjs/cdn-assets@10.7.2/highlight.min.js"></script><script src="https://unpkg.com/@highlightjs/cdn-assets@10.7.2/languages/javascript.min.js"></script><script src="https://unpkg.com/@highlightjs/cdn-assets@10.7.2/languages/cpp.min.js"></script><script src="https://unpkg.com/@highlightjs/cdn-assets@10.7.2/languages/glsl.min.js"></script><link rel="stylesheet" href="/style/main.css"/><meta name="next-head-count" content="25"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-381dbb3c33243b4920e6.js"></script><script src="/_next/static/chunks/webpack-17597d20e291f72b2439.js" defer=""></script><script src="/_next/static/chunks/framework-bdc1b4e5e48979e16d36.js" defer=""></script><script src="/_next/static/chunks/main-4ac108dd57980e4159e9.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c981e0e3ce59f13eb8d0.js" defer=""></script><script src="/_next/static/chunks/5988-738c1ea5f97353b6463e.js" defer=""></script><script src="/_next/static/chunks/pages/%5Byear%5D/%5Bmonth%5D/%5Bslug%5D-b78f23c90ce5148ef413.js" defer=""></script><script src="/_next/static/3zj-Zlh8XI2kEDVp25Cd3/_buildManifest.js" defer=""></script><script src="/_next/static/3zj-Zlh8XI2kEDVp25Cd3/_ssgManifest.js" defer=""></script><style id="__jsx-2519965637">.block.jsx-2519965637{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.block.jsx-2519965637 .right.jsx-2519965637{padding:10px;}.block.jsx-2519965637 .social.jsx-2519965637{margin-top:10px;}.block.jsx-2519965637 .social.jsx-2519965637 a.jsx-2519965637{padding:10px;}.block.jsx-2519965637 .social.jsx-2519965637 img.jsx-2519965637{height:20px;}</style><style id="__jsx-3621368397">.container.jsx-3621368397{min-height:100vh;padding:0 0.5rem;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><style id="__jsx-3469673304">html,body{padding:0;margin:0;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto, Oxygen,Ubuntu,Cantarell,Fira Sans,Droid Sans,Helvetica Neue, sans-serif;}*{box-sizing:border-box;}a{color:inherit;-webkit-text-decoration:none;text-decoration:none;}a:hover,a:active{-webkit-text-decoration:underline;text-decoration:underline;}</style></head><body><div id="__next"><div class="jsx-3621368397 container"><div id="container"><div id="main"><div id="content"><article><header><h1><a href="/">Beez, WebRTC + Audio API</a></h1><time class="date" dateTime="2013-09-04">2013-09-04</time><span class="tags"><a class="tag">WebRTC</a><a class="tag">audio</a><a class="tag">hackday</a></span></header><div class="entry-content"><img src="/images/2013/09/beez.png" alt="" class="thumbnail-left" />

<p>Here is <strong>Beez</strong>, a web real-time audio experiment 
using smartphones as synthesizer effect controllers.</p>
<p>This is our second <a href="https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html">Web Audio API</a> experiment made in one Hackday at <a href="http://zenexity.com">Zenexity</a> (now Zengularity).</p>
<p>This time, we were much more focused on having the best <strong>latency performance</strong>:
we used the bleeding-edge <a href="http://www.w3.org/TR/webrtc/">WebRTC</a> technology,
which allows you to link clients in Peer-to-Peer instead of a classical Client-Server architecture.</p>
<ul>
<li><a href="http://github.com/gre/beez">The project on Github</a></li>
<li><a href="http://beez.greweb.fr/">Test it now!</a> (Chrome)</li>
</ul>
<h3 id="live-demo-of-the-hackday-application">Live demo of the Hackday application</h3>
<iframe width="640" height="480" src="//www.youtube.com/embed/QwU6IMNLF0o" frameborder="0" allowfullscreen></iframe>

<p><em>Bonus for the one who recognizes the melody :-)</em></p>
<!--more-->

<h2 id="the-experiment">The Experiment</h2>
<p>The experiment consists in controlling an audio stream running on a desktop web page with 
some audio effect pads running on phones via a mobile web interface.
<strong>Our main goal was to make the best real-time experience.</strong></p>
<h3 id="hive-and-bees">Hive and Bees</h3>
<p>An <strong>Hive</strong> is controlled by different <strong>Bees</strong>, eventhing connected in Peer-to-Peer (via WebRTC).</p>
<h4 id="the-hive">The Hive</h4>
<p>The <strong>Hive</strong> is a web page where the sound is generated and visualized <em>(Web Audio API)</em>.
It also shows you in real-time the different effects XY pads and allows you to control them.</p>
<p><img src="/images/2013/09/hive.png" alt=""></p>
<h4 id="a-bee">A Bee</h4>
<p>The <strong>Bee</strong> is a mobile web page which allows you to control the different sound effects with XY pads.
It only works on Android Chrome now <em>(WebRTC required)</em>.</p>
<p><img src="/images/2013/09/bee.png" alt=""></p>
<h3 id="audio-tech">Audio tech</h3>
<p>We used <a href="https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html">Web Audio API</a> for generating the sound client-side on the Hive:</p>
<p>We have a note sequencer which plays a <em><strong>famous melody</strong></em> through a <a href="/2013/08/FM-audio-api">Frequency Modulator</a> and different other effects.
Some controls allow you to change the <strong>BPM</strong>, <strong>gain</strong> of the carrier and the modulator, <strong>finetune</strong>, 
frequency <strong>multiplicator</strong> (0.25, 0.5, 1, 1.5, 2) of the carrier and the modulator,
<strong>reverbation</strong>, <strong>filter</strong> (frequency and resonance).
There is also a delay effect made on both left and right channels to produce a cool stereo effect.</p>
<p>That&#39;s quite basic audio stuff so far, I can&#39;t wait to experiment deeper and try to generate more complex sounds with that awesome Audio API.
Again, our main goal was to make a P2P connection between the hive and its bees.</p>
<h3 id="network-architecture">Network architecture</h3>
<img src="/images/2013/09/beez_arch.png" alt="" class="thumbnail-left" />
**Every bee are connected to the hive with a bi-directionnal Peer-to-Peer connection thanks to [WebRTC][webrtc].**

<p>Everytime a (bee) user moves an effect controller with his phone, a position event is sent to the hive.</p>
<p>Basically:</p>
<pre><code class="language-javascript">xyAxis.on(&quot;change:x change:y&quot;, function () {
  hive.send([&quot;tabxy&quot;, this.get(&quot;tab&quot;), this.get(&quot;x&quot;), this.get(&quot;y&quot;)]);
});
</code></pre>
<p><em>As you can see, we used Backbone.js models for events.</em></p>
<p>However, A lot of Touch events per second can be triggered by an Android device, and it may depends on the device speed. We shouldn&#39;t send to the network all of these events because it can saturate it and cause some lags. To avoid that we need to <strong>throttle the touch events before sending the event to the network</strong>.</p>
<p>This is done transparently with the <a href="http://underscorejs.org/#throttle"><code>_.throttle</code></a> function and we choose to <strong>throttle by 50 milliseconds</strong> which is <strong>about 20 events per second</strong> which is ok for human eye.</p>
<pre><code class="language-javascript">xyAxis.on(&quot;change:x change:y&quot;, _.throttle(function () {
  hive.send([&quot;tabxy&quot;, this.get(&quot;tab&quot;), this.get(&quot;x&quot;), this.get(&quot;y&quot;)]);
}, 50));
</code></pre>
<p>We use different other events:</p>
<ul>
<li>A bee can send <code>&quot;tabxychanging&quot;</code> and <code>&quot;tabopen&quot;</code> respectively to informs the cursor has been pressed/released and to inform a new tab has been opened.</li>
<li>When a Hive receive a <code>&quot;tabopen&quot;</code>, it will send back to the bee a <code>&quot;tabxy&quot;</code> event in order to inform what is the current value of that tab so we can init the cursor to the current xy axis position on the bee interface.</li>
</ul>
<h2 id="websockets-vs-webrtc">WebSockets vs WebRTC</h2>
<img src="/images/2013/09/websocket.png" class="thumbnail-left" />

<p>Most &quot;real-time&quot; web experiments you see on the Internet today use <a href="http://www.w3.org/TR/websockets/">WebSockets</a>.
WebSockets are good, it&#39;s a significant evolution from the Ajax years.</p>
<p>WebSocket is a protocol on top of TCP, which <strong>links a browser with a server in a bidirectional text communication</strong>.
Getting 2 clients to communicate generally consists in broadcasting messages from the server to all clients 
(see the schema).</p>
<p><strong>This architecture has some advantages:</strong></p>
<ul>
<li>Simple to understand, Easy to use.</li>
<li>We can easily implement some server validation.</li>
</ul>
<p><strong>But also has some drawbacks:</strong></p>
<ul>
<li>Not always easy to traverse <strong>proxies</strong>. <em>(e.g. through an nginx front server)</em></li>
<li>Only text communication.</li>
<li><strong>bandwith</strong> intensive. <em>(all the bandwidth goes back and forth with your server)</em></li>
<li><strong>CPU</strong> intensive. <em>(e.g. receiving 20 messages per second from 10 clients can be a lot for a small server, especially if you are doing some message processing)</em></li>
</ul>
<p>(The last two &quot;cons&quot; are scalability issues)</p>
<hr style="clear:both" />

<img src="https://upload.wikimedia.org/wikipedia/commons/a/ac/Logo-webrtc.png" class="thumbnail-left" />

<p><a href="http://www.webrtc.org/">WebRTC</a> (<em>Web Real Time Communication</em>), 
is a new web technology which helps to connect browsers in a <strong>Peer to Peer</strong> way.</p>
<p>WebRTC has been designed for transfering binary data like files, audio, video (e.g. a webcam stream).
Of-course, we can still use it for text.</p>
<br style="clear:both" />

<img src="/images/2013/09/webrtc.png" class="thumbnail-right" />

<p>Unlike WebSockets, multiple steps are required to <strong>establish a P2P connection between two web clients</strong>.
It is due to the fact that the two clients must resolve the closest network path to communicate with each other.
That resolving phase requires a communication between the clients, and for that we can use WebSockets as a <em>&quot;Control Channel&quot;</em>.</p>
<p>But once the two web clients are connected, they basically don&#39;t need the web server anymore and can <strong>communicate directly together</strong>.
If the two clients are in the same local network, they should directly communicate through that local network.
That <strong>reduces the server load</strong> and should significantly <strong>decrease the latency</strong>.</p>
<h3 id="playframework--akka-actors">Playframework / Akka Actors</h3>
<p><a href="http://playframework.com/">Playframework</a> has been used on the server side for making that WebSocket Control Channel,
and akka was convenient for handling peer communication and rooms management.</p>
<h2 id="about-the-melody">About the melody</h2>
<p>You still didn&#39;t guess where does the melody came from?</p>
<p>Well, here is the answer:</p>
<iframe width="640" height="480" src="//www.youtube.com/embed/3rU_ei_x0Ag" frameborder="0" allowfullscreen></iframe>

<h2 id="awesome-team">Awesome team!</h2>
<p>Finally I want to thanks my team-mates: <a href="http://twitter.com/mrspeaker">@mrspeaker</a>, <a href="http://twitter.com/etaty">@etaty</a>, <a href="http://twitter.com/NicuPrinFum">@NicuPrinFum</a>, <a href="http://twitter.com/drfars">@drfars</a>, <a href="http://twitter.com/srenaultcontact">@srenaultcontact</a>
with who we were able to make that demo from scratch in one day!</p>
</div><footer><div class="jsx-2519965637 block"><img src="http://greweb.me/logo.svg" width="100" class="jsx-2519965637"/><div class="jsx-2519965637 right"><div class="jsx-2519965637 description">generative artist. doodling with algorithms (creative coding). shaders &amp; fountain pens robot plotting.</div><div class="jsx-2519965637 social"><a href="https://twitter.com/greweb" class="jsx-2519965637"><img alt="" src="/icons/twitter.svg" class="jsx-2519965637"/></a><a href="https://instagram.com/greweb" class="jsx-2519965637"><img alt="" src="/icons/instagram.svg" class="jsx-2519965637"/></a><a href="https://twitch.tv/greweb" class="jsx-2519965637"><img alt="" src="/icons/twitch.svg" class="jsx-2519965637"/></a><a href="https://github.com/gre" class="jsx-2519965637"><img alt="" src="/icons/github.svg" class="jsx-2519965637"/></a><a href="https://opensea.io/greweb" class="jsx-2519965637"><img alt="" src="/icons/eth.svg" class="jsx-2519965637"/></a><a href="https://objkt.com/profile/tz1cgQAQfECg5bPASYTMyJ9QJQjSUi8rfL67" class="jsx-2519965637"><img alt="" src="/icons/tz.svg" class="jsx-2519965637"/></a><a href="https://fxhash.xyz/u/greweb" class="jsx-2519965637"><img alt="" src="/icons/tz.svg" class="jsx-2519965637"/></a><a href="https://greweb.itch.io" class="jsx-2519965637"><img alt="" src="/icons/iconmonstr-gamepad-3.svg" class="jsx-2519965637"/></a></div></div></div></footer></article></div></div></div><script>hljs.highlightAll();</script></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"id":"2013-09-04-beez","year":"2013","month":"09","day":"04","slug":"beez","content":"\u003cimg src=\"/images/2013/09/beez.png\" alt=\"\" class=\"thumbnail-left\" /\u003e\n\n\u003cp\u003eHere is \u003cstrong\u003eBeez\u003c/strong\u003e, a web real-time audio experiment \nusing smartphones as synthesizer effect controllers.\u003c/p\u003e\n\u003cp\u003eThis is our second \u003ca href=\"https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html\"\u003eWeb Audio API\u003c/a\u003e experiment made in one Hackday at \u003ca href=\"http://zenexity.com\"\u003eZenexity\u003c/a\u003e (now Zengularity).\u003c/p\u003e\n\u003cp\u003eThis time, we were much more focused on having the best \u003cstrong\u003elatency performance\u003c/strong\u003e:\nwe used the bleeding-edge \u003ca href=\"http://www.w3.org/TR/webrtc/\"\u003eWebRTC\u003c/a\u003e technology,\nwhich allows you to link clients in Peer-to-Peer instead of a classical Client-Server architecture.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://github.com/gre/beez\"\u003eThe project on Github\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://beez.greweb.fr/\"\u003eTest it now!\u003c/a\u003e (Chrome)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"live-demo-of-the-hackday-application\"\u003eLive demo of the Hackday application\u003c/h3\u003e\n\u003ciframe width=\"640\" height=\"480\" src=\"//www.youtube.com/embed/QwU6IMNLF0o\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\u003cp\u003e\u003cem\u003eBonus for the one who recognizes the melody :-)\u003c/em\u003e\u003c/p\u003e\n\u003c!--more--\u003e\n\n\u003ch2 id=\"the-experiment\"\u003eThe Experiment\u003c/h2\u003e\n\u003cp\u003eThe experiment consists in controlling an audio stream running on a desktop web page with \nsome audio effect pads running on phones via a mobile web interface.\n\u003cstrong\u003eOur main goal was to make the best real-time experience.\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"hive-and-bees\"\u003eHive and Bees\u003c/h3\u003e\n\u003cp\u003eAn \u003cstrong\u003eHive\u003c/strong\u003e is controlled by different \u003cstrong\u003eBees\u003c/strong\u003e, eventhing connected in Peer-to-Peer (via WebRTC).\u003c/p\u003e\n\u003ch4 id=\"the-hive\"\u003eThe Hive\u003c/h4\u003e\n\u003cp\u003eThe \u003cstrong\u003eHive\u003c/strong\u003e is a web page where the sound is generated and visualized \u003cem\u003e(Web Audio API)\u003c/em\u003e.\nIt also shows you in real-time the different effects XY pads and allows you to control them.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/2013/09/hive.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch4 id=\"a-bee\"\u003eA Bee\u003c/h4\u003e\n\u003cp\u003eThe \u003cstrong\u003eBee\u003c/strong\u003e is a mobile web page which allows you to control the different sound effects with XY pads.\nIt only works on Android Chrome now \u003cem\u003e(WebRTC required)\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/2013/09/bee.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003ch3 id=\"audio-tech\"\u003eAudio tech\u003c/h3\u003e\n\u003cp\u003eWe used \u003ca href=\"https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html\"\u003eWeb Audio API\u003c/a\u003e for generating the sound client-side on the Hive:\u003c/p\u003e\n\u003cp\u003eWe have a note sequencer which plays a \u003cem\u003e\u003cstrong\u003efamous melody\u003c/strong\u003e\u003c/em\u003e through a \u003ca href=\"/2013/08/FM-audio-api\"\u003eFrequency Modulator\u003c/a\u003e and different other effects.\nSome controls allow you to change the \u003cstrong\u003eBPM\u003c/strong\u003e, \u003cstrong\u003egain\u003c/strong\u003e of the carrier and the modulator, \u003cstrong\u003efinetune\u003c/strong\u003e, \nfrequency \u003cstrong\u003emultiplicator\u003c/strong\u003e (0.25, 0.5, 1, 1.5, 2) of the carrier and the modulator,\n\u003cstrong\u003ereverbation\u003c/strong\u003e, \u003cstrong\u003efilter\u003c/strong\u003e (frequency and resonance).\nThere is also a delay effect made on both left and right channels to produce a cool stereo effect.\u003c/p\u003e\n\u003cp\u003eThat\u0026#39;s quite basic audio stuff so far, I can\u0026#39;t wait to experiment deeper and try to generate more complex sounds with that awesome Audio API.\nAgain, our main goal was to make a P2P connection between the hive and its bees.\u003c/p\u003e\n\u003ch3 id=\"network-architecture\"\u003eNetwork architecture\u003c/h3\u003e\n\u003cimg src=\"/images/2013/09/beez_arch.png\" alt=\"\" class=\"thumbnail-left\" /\u003e\n**Every bee are connected to the hive with a bi-directionnal Peer-to-Peer connection thanks to [WebRTC][webrtc].**\n\n\u003cp\u003eEverytime a (bee) user moves an effect controller with his phone, a position event is sent to the hive.\u003c/p\u003e\n\u003cp\u003eBasically:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-javascript\"\u003exyAxis.on(\u0026quot;change:x change:y\u0026quot;, function () {\n  hive.send([\u0026quot;tabxy\u0026quot;, this.get(\u0026quot;tab\u0026quot;), this.get(\u0026quot;x\u0026quot;), this.get(\u0026quot;y\u0026quot;)]);\n});\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cem\u003eAs you can see, we used Backbone.js models for events.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eHowever, A lot of Touch events per second can be triggered by an Android device, and it may depends on the device speed. We shouldn\u0026#39;t send to the network all of these events because it can saturate it and cause some lags. To avoid that we need to \u003cstrong\u003ethrottle the touch events before sending the event to the network\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eThis is done transparently with the \u003ca href=\"http://underscorejs.org/#throttle\"\u003e\u003ccode\u003e_.throttle\u003c/code\u003e\u003c/a\u003e function and we choose to \u003cstrong\u003ethrottle by 50 milliseconds\u003c/strong\u003e which is \u003cstrong\u003eabout 20 events per second\u003c/strong\u003e which is ok for human eye.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-javascript\"\u003exyAxis.on(\u0026quot;change:x change:y\u0026quot;, _.throttle(function () {\n  hive.send([\u0026quot;tabxy\u0026quot;, this.get(\u0026quot;tab\u0026quot;), this.get(\u0026quot;x\u0026quot;), this.get(\u0026quot;y\u0026quot;)]);\n}, 50));\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe use different other events:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eA bee can send \u003ccode\u003e\u0026quot;tabxychanging\u0026quot;\u003c/code\u003e and \u003ccode\u003e\u0026quot;tabopen\u0026quot;\u003c/code\u003e respectively to informs the cursor has been pressed/released and to inform a new tab has been opened.\u003c/li\u003e\n\u003cli\u003eWhen a Hive receive a \u003ccode\u003e\u0026quot;tabopen\u0026quot;\u003c/code\u003e, it will send back to the bee a \u003ccode\u003e\u0026quot;tabxy\u0026quot;\u003c/code\u003e event in order to inform what is the current value of that tab so we can init the cursor to the current xy axis position on the bee interface.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"websockets-vs-webrtc\"\u003eWebSockets vs WebRTC\u003c/h2\u003e\n\u003cimg src=\"/images/2013/09/websocket.png\" class=\"thumbnail-left\" /\u003e\n\n\u003cp\u003eMost \u0026quot;real-time\u0026quot; web experiments you see on the Internet today use \u003ca href=\"http://www.w3.org/TR/websockets/\"\u003eWebSockets\u003c/a\u003e.\nWebSockets are good, it\u0026#39;s a significant evolution from the Ajax years.\u003c/p\u003e\n\u003cp\u003eWebSocket is a protocol on top of TCP, which \u003cstrong\u003elinks a browser with a server in a bidirectional text communication\u003c/strong\u003e.\nGetting 2 clients to communicate generally consists in broadcasting messages from the server to all clients \n(see the schema).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eThis architecture has some advantages:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSimple to understand, Easy to use.\u003c/li\u003e\n\u003cli\u003eWe can easily implement some server validation.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eBut also has some drawbacks:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eNot always easy to traverse \u003cstrong\u003eproxies\u003c/strong\u003e. \u003cem\u003e(e.g. through an nginx front server)\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003eOnly text communication.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ebandwith\u003c/strong\u003e intensive. \u003cem\u003e(all the bandwidth goes back and forth with your server)\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCPU\u003c/strong\u003e intensive. \u003cem\u003e(e.g. receiving 20 messages per second from 10 clients can be a lot for a small server, especially if you are doing some message processing)\u003c/em\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e(The last two \u0026quot;cons\u0026quot; are scalability issues)\u003c/p\u003e\n\u003chr style=\"clear:both\" /\u003e\n\n\u003cimg src=\"https://upload.wikimedia.org/wikipedia/commons/a/ac/Logo-webrtc.png\" class=\"thumbnail-left\" /\u003e\n\n\u003cp\u003e\u003ca href=\"http://www.webrtc.org/\"\u003eWebRTC\u003c/a\u003e (\u003cem\u003eWeb Real Time Communication\u003c/em\u003e), \nis a new web technology which helps to connect browsers in a \u003cstrong\u003ePeer to Peer\u003c/strong\u003e way.\u003c/p\u003e\n\u003cp\u003eWebRTC has been designed for transfering binary data like files, audio, video (e.g. a webcam stream).\nOf-course, we can still use it for text.\u003c/p\u003e\n\u003cbr style=\"clear:both\" /\u003e\n\n\u003cimg src=\"/images/2013/09/webrtc.png\" class=\"thumbnail-right\" /\u003e\n\n\u003cp\u003eUnlike WebSockets, multiple steps are required to \u003cstrong\u003eestablish a P2P connection between two web clients\u003c/strong\u003e.\nIt is due to the fact that the two clients must resolve the closest network path to communicate with each other.\nThat resolving phase requires a communication between the clients, and for that we can use WebSockets as a \u003cem\u003e\u0026quot;Control Channel\u0026quot;\u003c/em\u003e.\u003c/p\u003e\n\u003cp\u003eBut once the two web clients are connected, they basically don\u0026#39;t need the web server anymore and can \u003cstrong\u003ecommunicate directly together\u003c/strong\u003e.\nIf the two clients are in the same local network, they should directly communicate through that local network.\nThat \u003cstrong\u003ereduces the server load\u003c/strong\u003e and should significantly \u003cstrong\u003edecrease the latency\u003c/strong\u003e.\u003c/p\u003e\n\u003ch3 id=\"playframework--akka-actors\"\u003ePlayframework / Akka Actors\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"http://playframework.com/\"\u003ePlayframework\u003c/a\u003e has been used on the server side for making that WebSocket Control Channel,\nand akka was convenient for handling peer communication and rooms management.\u003c/p\u003e\n\u003ch2 id=\"about-the-melody\"\u003eAbout the melody\u003c/h2\u003e\n\u003cp\u003eYou still didn\u0026#39;t guess where does the melody came from?\u003c/p\u003e\n\u003cp\u003eWell, here is the answer:\u003c/p\u003e\n\u003ciframe width=\"640\" height=\"480\" src=\"//www.youtube.com/embed/3rU_ei_x0Ag\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\u003ch2 id=\"awesome-team\"\u003eAwesome team!\u003c/h2\u003e\n\u003cp\u003eFinally I want to thanks my team-mates: \u003ca href=\"http://twitter.com/mrspeaker\"\u003e@mrspeaker\u003c/a\u003e, \u003ca href=\"http://twitter.com/etaty\"\u003e@etaty\u003c/a\u003e, \u003ca href=\"http://twitter.com/NicuPrinFum\"\u003e@NicuPrinFum\u003c/a\u003e, \u003ca href=\"http://twitter.com/drfars\"\u003e@drfars\u003c/a\u003e, \u003ca href=\"http://twitter.com/srenaultcontact\"\u003e@srenaultcontact\u003c/a\u003e\nwith who we were able to make that demo from scratch in one day!\u003c/p\u003e\n","data":{"title":"Beez, WebRTC + Audio API","description":"Here is Beez, a web real-time audio experiment using smartphones as synthesizer effect controllers. This is our second Web Audio API experiment made in one Hackday at Zenexity.","thumbnail":"/images/2013/09/beez.png","author":"Gaetan","layout":"post","tags":["WebRTC","audio","hackday"]}},"__N_SSG":true},"page":"/[year]/[month]/[slug]","query":{"year":"2013","month":"09","slug":"beez"},"buildId":"3zj-Zlh8XI2kEDVp25Cd3","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>