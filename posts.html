<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>greweb.me</title><link rel="icon" href="/favicon.ico"/><meta name="next-head-count" content="4"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a40ef1678bae11e696dba45124eadd70.js"></script><script src="/_next/static/chunks/webpack-c914a1e31212c00c738a.js" defer=""></script><script src="/_next/static/chunks/framework-bdc1b4e5e48979e16d36.js" defer=""></script><script src="/_next/static/chunks/main-3b0c517193ca5150f81f.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c981e0e3ce59f13eb8d0.js" defer=""></script><script src="/_next/static/chunks/5988-738c1ea5f97353b6463e.js" defer=""></script><script src="/_next/static/chunks/pages/posts-943a26fe6d20b6459806.js" defer=""></script><script src="/_next/static/1XWF0K4yTNe9SzMffEm6x/_buildManifest.js" defer=""></script><script src="/_next/static/1XWF0K4yTNe9SzMffEm6x/_ssgManifest.js" defer=""></script><style id="__jsx-2295160083">.title.jsx-2295160083{text-align:center;margin:0.5em 0;font-size:1.5rem;}</style><style id="__jsx-2180230042">header.jsx-2180230042{margin:1rem 0;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;font-size:16px;}</style><style id="__jsx-1508801263">main.jsx-1508801263{-webkit-flex:1;-ms-flex:1;flex:1;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><style id="__jsx-3621368397">.container.jsx-3621368397{min-height:100vh;padding:0 0.5rem;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><style id="__jsx-3469673304">html,body{padding:0;margin:0;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto, Oxygen,Ubuntu,Cantarell,Fira Sans,Droid Sans,Helvetica Neue, sans-serif;}*{box-sizing:border-box;}a{color:inherit;-webkit-text-decoration:none;text-decoration:none;}a:hover,a:active{-webkit-text-decoration:underline;text-decoration:underline;}</style></head><body><div id="__next"><div class="jsx-3621368397 container"><main class="jsx-1508801263"><header class="jsx-2180230042"><img width="200" src="/profile.jpg"/><h1 class="jsx-2295160083 title">greweb.me</h1><p class="social"><a href="https://twitter.com/greweb">Twitter</a> ‚Äì <a href="https://github.com/gre">Github</a></p></header><h2>Blog Posts</h2><ul><li><a href="/2022/02/shattered">Shattered, a plottable generator on plottables.io</a></li><li><a href="/2022/02/gnsp-swivel">GNSP ‚Äì the swivel rendering</a></li><li><a href="/2022/02/gnsp-nanoscreen">GNSP ‚Äì the Nano screen rendering</a></li><li><a href="/2021/12/gnsp-raymarching">GNSP ‚Äì the 3D distance to a Nano S Plus</a></li><li><a href="/2021/12/gnsp">GNSP ‚Äì the concept</a></li><li><a href="/2021/12/plottable-mountain-moons">‚ÄúPlottable Mountain Moons‚Äù and an approach to query generative art</a></li><li><a href="/2021/11/plottable-storm">‚ÄúPlottable Storm‚Äù, a generator of plottable SVGs</a></li><li><a href="/2021/11/duality">fxhash: duality</a></li><li><a href="/2021/08/the-day-after-pattern-03">The day after Pattern 03 (1/2)</a></li><li><a href="/2021/08/plotting-pattern-03">The day after Pattern 03 (2/2)</a></li><li><a href="/2021/08/pattern-03">Pattern 03, the first plottable generator on ethblock.art</a></li><li><a href="/2021/05/relics">Relics NFT collection</a></li><li><a href="/2021/05/plot-loops">&#x27;Plot loops&#x27; concept</a></li><li><a href="/2021/04/cryptoaliens-tech">CryptoAliens: Genesis, a technical look</a></li><li><a href="/2021/04/cryptoaliens">CryptoAliens: Genesis [ethblock.art]</a></li><li><a href="/2016/12/gl-react-v3">gl-react v3</a></li><li><a href="/2016/09/relay-scrolling-connections">Relay, scrolling connections</a></li><li><a href="/2016/07/projectseptember-opengl">üéâ There are some OpenGL in the Project September fashion app!</a></li><li><a href="/2016/06/glreactconf">Universal GL Effects for Web and Native</a></li><li><a href="/2015/10/introducing-gl-react">Introducing gl-react</a></li><li><a href="/2015/08/making-performant-react-applications">Making performant React applications</a></li><li><a href="/2014/10/webglparis">[FR] webglparis talk: GLSL.io initiative and WebGL Transitions</a></li><li><a href="/2014/09/ibex-cellular-automata">Cellular Automata in IBEX</a></li><li><a href="/2014/09/ibex">IBEX, my js13k game</a></li><li><a href="/2014/05/ld29">48 hours to prototype an Ant Sim Game</a></li><li><a href="/2014/03/panzer-dragoon-1k">Panzer Dragoon 1k</a></li><li><a href="/2014/01/promisify-your-games">Promisify your games</a></li><li><a href="/2013/11/functional-rendering">Functional Rendering</a></li><li><a href="/2013/09/webaudioapi">Slides: Web Audio API, Overview</a></li><li><a href="/2013/09/timelapse">Making a rhythm game with bleeding-edge web</a></li><li><a href="/2013/09/beez">Beez, WebRTC + Audio API</a></li><li><a href="/2013/08/FM-audio-api">Frequency Modulation (FM) with Web Audio API</a></li><li><a href="/2013/08/zound-wip-v1">ZOUND live v1 in development</a></li><li><a href="/2013/08/zanimo">Qep4.: Zanimo.js, a promise-based animation library</a></li><li><a href="/2013/07/zound-live">ZOUND live project initiated</a></li><li><a href="/2013/07/QanimationFrame">Qep3.: QanimationFrame</a></li><li><a href="/2013/07/deferred">Qep2.: Deferred objects, Qimage</a></li><li><a href="/2013/07/q-a-promise-library">Qep1.: Q, a Promise library</a></li><li><a href="/2013/05/playframework-simple-deployment-scripts">Playframework simple deployment scripts</a></li><li><a href="/2013/02/glsl-js-a-javascript-glsl-library-dry-efficient">glsl.js, a Javascript + GLSL library = DRY &amp; efficient</a></li><li><a href="/2013/01/playcli-play-iteratees-unix-pipe">PlayCLI: Play Iteratees + UNIX pipe</a></li><li><a href="/2013/01/be-careful-with-js-numbers">Be careful with JS numbers!</a></li><li><a href="/2012/11/play-framework-enumerator-outputstream">Play Framework ‚Äì Enumerator.outputStream</a></li><li><a href="/2012/08/zound-a-playframework-2-audio-streaming-experiment-using-iteratees">Zound, a PlayFramework 2 audio streaming experiment using Iteratees</a></li><li><a href="/2012/07/how-i-learned-backbone-js-three-js-glsl-in-one-week">How I learned Backbone.js, Three.js, GLSL in one week</a></li><li><a href="/2012/05/minimize-your-javascript-files-with-curl">Minimize your Javascript files with cURL</a></li><li><a href="/2012/05/illuminated-js-2d-lights-and-shadows-rendering-engine-for-html5-applications">Illuminated.js ‚Äì 2D lights and shadows rendering engine for HTML5 applications</a></li><li><a href="/2012/05/html5-canvas-as-a-color-converter">HTML5 Canvas as a color converter</a></li><li><a href="/2012/04/work-in-progress">Work in &lt;progress /&gt;</a></li><li><a href="/2012/04/blender-as-a-2d-game-map-editor-proof-of-concept">Blender as a 2D game level editor ‚Äì Proof Of Concept</a></li><li><a href="/2012/03/play-painter-how-ive-improved-the-30-minutes-prototyped-version">Play Painter ‚Äì how i&#x27;ve improved the 30 minutes prototyped version</a></li><li><a href="/2012/03/30-minutes-to-make-a-multi-user-real-time-paint-with-play-2-framework-canvas-and-websocket">30 minutes to make a multi user real time paint with Play 2 framework, Canvas and WebSocket.</a></li><li><a href="/2012/03/chart-libraries-headaches-finding-the-best-grid-step">Chart libraries headaches ‚Äì finding the best grid step</a></li><li><a href="/2012/02/bezier-curve-based-easing-functions-from-concept-to-implementation">Bezier Curve based easing functions ‚Äì from concept to implementation</a></li><li><a href="/2012/02/css-selector-based-templating-example-with-javascript">CSS-selector-based templating system for scalable JavaScript applications</a></li><li><a href="/2011/10/how-to-deploy-your-play-applications-on-archlinux-with-daemons">How to deploy your play applications on ArchLinux with daemons</a></li><li><a href="/2011/07/improve-your-web-navigation-experience-flexible-nav-jquery-library">Improve your web navigation experience ‚Äì Flexible Nav jQuery library</a></li><li><a href="/2011/07/same-game-gravity-technical-notes">Same Game Gravity: 6 platforms, 1 codebase</a></li><li><a href="/2011/07/same-game-gravity-for-ipad-iphone-android-facebook-chrome-and-web">Same Game Gravity for iPad, iPhone, Android, Facebook, Chrome, and Web!</a></li><li><a href="/2011/06/automating-web-app-development-for-multiple-platforms">Automating Web App development for multiple platforms</a></li><li><a href="/2011/04/releasing-same-game-gravity-android">Releasing Same Game Gravity (Android)</a></li><li><a href="/2011/03/html-canvas-pour-les-neophytes">HTML Canvas pour les n√©ophytes</a></li><li><a href="/2011/01/how-to-make-dlink-dwa-140-perfectly-work-on-linux">How to make DLink DWA-140 B2 perfectly work on Linux</a></li><li><a href="/2010/05/css3-transitions-available-on-firefox-3-7">CSS3 Transitions</a></li><li><a href="/2010/04/automatiser-lexportation-dun-site-statique-avec-wget">Automatiser l&#x27;exportation d&#x27;un site statique avec wget</a></li><li><a href="/2010/04/realiser-une-maquette-web-avec-le-css-3">R√©aliser une maquette web avec le CSS 3</a></li><li><a href="/2010/03/sass-levolution-du-css">SASS : l&#x27;√©volution du CSS pour Play, Rails ou autres</a></li><li><a href="/2010/02/the-same-game-in-html5-canvas">The same game in HTML5 canvas</a></li><li><a href="/2010/02/tutoriel-canvas-realiser-une-banniere-animee-en-quelques-lignes-de-code">Tutoriel Canvas : R√©aliser une banni√®re anim√©e en quelques lignes de code</a></li></ul></main></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"posts":[{"id":"2022-02-23-shattered","year":"2022","month":"02","day":"23","slug":"shattered","content":"\n\u003cimg width=\"50%\" src=\"/images/2022/shattered/thumbnail.jpg\" style=\"float: right\"\u003e\n\n\"Shattered\" is a new generative and plottable art that will be released the February 23rd, 2022 (6PM Paris time) on the emerging platform https://plottables.io/.\n\n\u003e Shattered [adj] ‚Äì broken into many small pieces.\n\u003e\n\u003e This plottable NFT generator visualizes the explosion of polygons (e.g. squares) into pieces then rendered using different filling techniques and suggested inks. There are various rare cases to discovery, like the shape, filling method, alignments,...\n\u003e\n\u003e The plots are designed to be plotted with fountain pens and on A4 paper, but the plotter artist is free to do any interpretation they want, chose different colors, different paper techniques, the possibilities are endless! Depending on your configuration, the plot can take between 15 minutes to a few hours to execute.\n\u003e\n\u003e @greweb can ship plotted editions to NFT owners with a small extra cost via https://greweb.me/plots/nft\n\n\u003c!--\nMore information can be found on the release article https://greweb.me/2022/02/shattered\n--\u003e\n\n## Physically plottable?\n\n\u003cvideo loop autoplay controls src=\"/images/2022/shattered/video.mp4\" width=\"100%\"\u003e\u003c/video\u003e\n\nLike in [my previous article](https://greweb.me/2021/11/plottable-storm), the generator produces \"Plottable NFTs\".\n\nA Plottable NFT is, like regular art NFT, a token to a digital artwork that can be sold and resold on a second market ‚Äì blockchain guarantees its unicity.\n\nThe **Plottable** NFT also gives a privilege to its owner: making or requesting a physical plot (with an artistic fee to cover costs, [see /plots/nft](https://greweb.me/plots/nft)). As a physical work can be executed multiple times, every physical piece remain unique: plotting is an analog process that will inevitably yield different results.\n\n\u003cimg width=\"100%\" src=\"/images/2021/11/plottablestorm/schema.png\"\u003e\n\n[https://greweb.me/plots/nft](https://greweb.me/plots/nft) sets my own \"plotting services\" terms but any other plotter artist is welcome to collaborate and contribute to offering to the community their own services under their terms and interpretation of the art ‚Äì I would just ask to verify that the requester owns the NFT.\n\n\u003cimg width=\"100%\" src=\"/images/plots-promo/letters.jpeg\"\u003e\n\n\u003e You can also be your own artist, buy a plotter, try it out! This is all designed to be open-source friendly and you can download the SVG yourself!\n\n### Prototype, prototype, prototype\n\nMany prototypes have been physically plotted in order to guarantee the viability of results: plottable often is a challenge when it comes to pen and paper. The density is very challenging to adjust: not enough lines and you miss the opportunity for great ink effects and saturation. Try with too much lines and your paper is melting.\n\n\u003cimg width=\"100%\" src=\"/images/2022/shattered/prototypes.jpg\"\u003e\n\nThese prototypes helped to adjust the density and tweak the algorithms over the weeks of development.\n\n\u003cimg width=\"100%\" src=\"/images/2022/shattered/prototype1.jpg\"\u003e\n\u003cimg width=\"100%\" src=\"/images/2022/shattered/prototype3.jpg\"\u003e\n\u003cimg width=\"50%\" src=\"/images/2022/shattered/prototype2.jpg\"\u003e\u003cimg width=\"50%\" src=\"/images/2022/shattered/prototype5.jpg\"\u003e\n\u003cimg width=\"100%\" src=\"/images/2022/shattered/prototype4.jpg\"\u003e\n\n## How does the generator work\n\nThe generator works in a few steps:\n\n- Step 1 implements **the shattering** and outputs polygons.\n- Step 2 implements **the filling**: the polygons are filled with various techniques and colors.\n- Step 3 implements **post-processing effects**. Not only a SVG is generated, but it is also used in a WebGL shaders texture to simulate ink and paper effects.\n\n## Step 1: the shattering\n\nThe \"shattering\" is implemented with a recursive function that, given a polygon, randomly cut it in half and apply the same recursion on each part. The cut happens in many different ways. For instance, there is a chance for the \"cut\" to be aligned with the polygon. The recursion can also randomly stop, which can sometimes keep great parts intact. There is even a very small chance (~0.3%) to not obtain a cut at all and have the original polygon preserved. Parts are usually split near their center and the 2 parts that result from the cut can diverge from each other ‚Äì this simulates some kind of \"pushback\" resulting of the shattering.\n\n### 'Shape' feature\n\nThe main polygon used before the shattering occurs can vary from a square to a circle, with rare cases of triangles, hexagons,...\n\nSquare remains the most common shape to have.\n\n\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/shape-square.jpg\"\u003e\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/shape-circle.jpg\"\u003e\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/shape-hexagon.jpg\"\u003e\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/shape-pentagon.jpg\"\u003e\n\n### 'Parts' feature\n\nThis feature categorizes the number of polygon parts that result from the shattering. It can vary from one to thousands.\n\n\u003cimg width=\"100%\" src=\"/images/2022/shattered/examples/variety3.jpg\"\u003e\n\n### 'Pushback' feature\n\nThis feature expresses how intense the Pushback is for a given piece ‚Äì effect that shifts parts away from each other.\n\n\u003cimg width=\"100%\" src=\"/images/2022/shattered/examples/pushback-high.jpg\"\u003e\n\nIt can sometimes not exist at all which make the polygon preserving its general shape and only showing subdivisions:\n\n\u003cimg width=\"100%\" src=\"/images/2022/shattered/examples/pushback-none.jpg\"\u003e\n\n### 'Alignment' feature\n\nThe alignment represents how perpendicular the parts are organized altogether.\n\nThis has no alignment:\n\n\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/alignment-none.jpg\"\u003e\n\nThese have the max alignment:\n\n\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/alignment-max.jpg\"\u003e\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/alignment-max-2.jpg\"\u003e\n\n### 'Destructed' feature\n\n'Destructed' is a rare feature that expresses that part of the polygons has been destructed. This sometimes happens when the shattered parts are pushbacked out of boundaries.\n\n\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/destructed-1.jpg\"\u003e\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/destructed-2.jpg\"\u003e\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/destructed-3.jpg\"\u003e\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/destructed-4.jpg\"\u003e\n\n### 'Distribution' feature\n\n'Distribution' feature expresses the area balance between parts. Here are some examples:\n\n**\"One Major\"** (2 examples side by side)\n\n\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/distribution-onemajor-2.jpg\"\u003e\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/distribution-onemajor.jpg\"\u003e\n\n**\"Two Main\"** (2 examples side by side)\n\n\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/distribution-twomainparts-2.jpg\"\u003e\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/distribution-twomainparts.jpg\"\u003e\n\n**\"Small Parts\"** (2 examples side by side)\n\n\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/distribution-small-parts-2.jpg\"\u003e\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/distribution-small-parts-3.jpg\"\u003e\n\n## Step 2: the filling\n\neach polygon resulting from the shattering is individually filled with various techniques. That said, the shattering algorithm has already decided how each polygon is colored and filled. There is a higher chance that related parts have same fill and color, but it can still happen that one has many different filling techniques on the same piece:\n\n\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/variety.jpg\"\u003e\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/variety2.jpg\"\u003e\n\nThese are statistically the TOP 10 filling technique combination to most likely occur:\n\n```\nStatistics of feature 'Fill':\n                   Web, Spiral: 6.0%\n                           Web: 5.7%\n                Web, Ping Pong: 3.3%\n                   Spiral, Web: 3.3%\n                     Ping Pong: 3.1%\n                     Scratches: 2.8%\n                         Hatch: 2.5%\n          Ping Pong, Scratches: 2.3%\n                     Stippling: 2.3%\n            Web, Spiral, Empty: 2.2%\n```\n\nThe 66% remaining is going to many different combinations of one of the 8 different filling technique that can occur: **Empty, Spiral, Web, Ping Pong, Scratches, Hatch, Stippling, Zigzag** which I'm going to detail now:\n\n### Fill = Empty\n\nThere is a first, rare filling technique that consists of NOT filling! but simply stroking the polygon. It is mostly used on small polygons and used in combination with another filling technique.\n\n\u003cimg width=\"100%\" src=\"/images/2022/shattered/examples/fill-empty.jpg\"\u003e\n\n### Fill = Spiral\n\nSpiral is the most complex algorithm of the generator and can take up to a few seconds to run in JavaScript. It works by connecting randomly sampled dots by rotating the lines, each iteration scans for a point making the smallest angle turn, this creates a spiral that statistically converge to the center.\n\n\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/fill-spiral.jpg\"\u003e\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/fill-spiral-3.jpg\"\u003e\n\nDue to the complexity of the algorithm, I have chosen to bail out some cases to fallback to \"Web\" filling technique which is why you most likely never get a \"Spiral only\" filling (it can still happen ~0.4%).\n\n### Fill = Web\n\nThe web is, after spiral, the second most popular filling technique to happen but is the most likely technique to happen as \"standalone\" (without combination with another filling technique)\n\n\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/fill-web.jpg\"\u003e\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/fill-web-3.jpg\"\u003e\n\nOne of the easter eggs of this Fill technique is that it will sometimes also have circles hole that the algorithm will try to avoid:\n\n\u003cimg width=\"100%\" src=\"/images/2022/shattered/examples/fill-web-2.jpg\"\u003e\n\nThe algorithm samples a bunch of points along the polygon edge and connects dots randomly. The dots are connected with lines that maximize avoiding the circles and \"traversing\" the polygon rather than connecting lines that would be along an edge. Points are also samples randomly away from the polygon edge in order to avoid too much paper density problem ‚Äì it creates an interesting border width effect.\n\n### Fill = Ping Pong\n\nPing Pong filling is the idea to throw a ray that bounces on the polygon edge.\n\n\u003cimg width=\"100%\" src=\"/images/2022/shattered/examples/fill-pingpong.jpg\"\u003e\n\nThe basic idea is to sample points along the polygon perimeter and then to do a permutation of indexes that can for instance looks like this: **0, 100, 200, 1, 101, 201, 2, 102, 202,...\\***.\n\nMany optimizations were done on this to make the plot viable. Each line is traced by counting how much lines traversed at a given position and will stop as soon as it's too \"crowded\".\n\nWe can notice the algorithm at work on the previous example as we see some \"gaps\" near the intersection of some lines, but in practice, a plot will be generally fine as the ink will \"bleed\" a bit and fill these gaps.\n\n### Fill = Hatch\n\nthe hatching technique traces parallel lines with a varying distance to produces a fabric pattern. The lines are chosen in a way that they are aligned to the longest segment that can exists inside the polygon.\n\n\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/fill-hatch.jpg\"\u003e\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/fill-hatching-3.jpg\"\u003e\n\n### Fill = Stippling\n\nthe stippling technique is implemented with many samples points from which a small line stroke is done. The line stroke segment, which is one millimeter long, is oriented following a simple field determined with the center of the polygon: `atan2(cx - p[0], cy - p[1])`\n\n\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/fill-stippling.jpg\"\u003e\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/fill-stippling-2.jpg\"\u003e\n\n### Fill = Scratches\n\n\u003cimg width=\"100%\" src=\"/images/2022/shattered/examples/fill-scratches.jpg\"\u003e\n\nScratches are sorting randomly samples points on two perpendicular axis that follow the general polygon direction.\n\n### Fill = Zigzag\n\n\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/fill-zigzag.jpg\"\u003e\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/fill-zigzag-2.jpg\"\u003e\n\nZigzag has a similar idea as in Scratches except it only traces line on one axis and will only sample points on the edge.\n\n## Step 3: post-processed effects\n\nTo give more life to the digital shape, I have added some paper grain effects as well as a lighting effect that grows from the center.\n\nIt is important to note that I actually generate 2 SVGs:\n\n- one dedicated for the WebGL shaders where I use #0FF, #F0F, #FF0 colors and addition mode to be able to tell what even ink there is at a position. It allows me to implement some sort of ink simulation (those which remains minimal, but using opacity to express the saturation of an ink and each color defines 2 colors on each it derives too ‚Äì for instance the yellow of Diamine Amber is diverging to orange tones)\n- a second SVG is the downloadable SVG that tries to approach as much as possible the ink color and have layers ready for the plotter. Note that you can directly drag\u0026drop (or right-click) from the digital canvas itself (thanks to a hidden `\u003cimg\u003e` in the DOM)\n\n## Palette\n\nThe art pieces are picking colors among a palette of 9 curated inks from Diamine that looks like this:\n\n\u003cimg width=\"100%\" src=\"/images/2022/shattered/inks.jpg\"\u003e\n\nThey are sorted from common to relatively rarer cases.\n\n- Black\n- Poppy Red\n- Turquoise\n- Amazing Amethyst\n- Indigo\n- Hope Pink\n- Amber\n- Pumpkin\n- Aurora Borealis\n\nThe most common case is to obtain a plot that will only have one ink chosen, but the plot can go up to having 3 different colors. They are split into different layers.\n\n\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/colors-2.jpg\"\u003e\u003cimg width=\"50%\" src=\"/images/2022/shattered/examples/colors-3.jpg\"\u003e\n\n## Artistic challenges and choices\n\nTo wrap up, it took me quite a lot of iterations to reach decision on many aspects and tweak what would be my final pieces. In generative art and especially when you reach a high variety of results it is really hard to guarantee one given output is satisfying. This is especially true in my case that involve many level of recursion which makes it very hard to control the outcome. The wider your generation is, more likely you will get non interesting results or edge cases you do not prefer to happen. The boundaries of the generation have to be well controlled in order to avoid the \"bad cases\".\n\n**Total rewrite of my tech stack**. Technically speaking, this generator was a total rework of my stack, which is in Rust, back into pure JavaScript. As the code is on-chain, it needs to be very well optimized in term of bytes and I used many techniques from back of my JS1K / JS13K days! I compressed my GLSL shader code as well as the JavaScript code, I use only tuples (arrays) to represent my data so they do not have the verbosity of JS objects, etc...\n\n**curating away the bad cases**. The way I generally approached the curation in my generator is to curate away the cases I didn't want rather than explicitly curating the cases I want! Typically, one algorithm will control the quality of the output and will be retrying the result until it satisfies the criterias. One example is I have to exclude any \"empty result\" to happen. I also explicitely wanted to bail out a result containing only one cut (2 polygons).\n\n**Brace for impact**. embracing collisions and allowing some shapes to intersect was part of the \"glitch\" I wanted to keep. It can be interesting to have a mix of inks that occur on these intersections. This is a rare effect.\n\n**recursion depth**. It took me quite a lot of shattering iteration to decide on the maximum number of recursion I would do as well as it's rarity. At the end, I've chose to make the \"low number of parts\" rarer than having a very exploded shattered case, because it emphasis on what the piece is about. Pieces that are intacted or only cut in a few parts are still very interesting to have and should actually be quite scarced.\n\n**shapes rarity \u0026 filling consistency**. The way the shape are filled are not always decided randomly and many logic bail out from cases I didn't want.\n\n\u003cimg width=\"100%\" src=\"/images/2022/shattered/thumbnail.jpg\"\u003e\n","data":{"title":"Shattered, a plottable generator on plottables.io","thumbnail":"/images/2022/shattered/thumbnail.jpg","description":"Shattered is a NFT plottable generator that visualizes the explosion of polygons (usually squares) into parts that are rendered with different filling techniques and suggested inks. There are various rarity elements, like the shape, filling method, inks...","tags":["NFT","plottable"]}},{"id":"2022-02-20-gnsp-swivel","year":"2022","month":"02","day":"20","slug":"gnsp-swivel","content":"\nThis fourth article (in a series of 7 articles) reveals the technique used to render the Nano swivel.\n\n\u003cvideo muted loop autoplay controls src=\"/images/2022/gnsp/361.mp4\" width=\"50%\" style=\"float:left; margin-right: 40px; margin-bottom:20px\"\u003e\u003c/video\u003e\n\n**Timeline:**\n\n- [article 1: GNSP ‚Äì the concept](/2021/12/gnsp)\n- [article 2: the 3D distance to a Nano S+](/2021/12/gnsp-raymarching)\n- [article 3: the nano screen](/2022/02/gnsp-nanoscreen)\n- [**article 4: the swivel**](/2022/02/gnsp-swivel)\n- article 5: the background\n- article 6: the video generation\n- article 7: the final drop\n- (?March) public mint\n\n**The collection is browsable on https://greweb.me/gnsp**\n\n**OpenSea: https://opensea.io/collection/gnsp**\n\n\u003cbr style=\"clear:left\"/\u003e\n\nThis is very similar to the \"nano screen\" in that we will use a Canvas 2D as a texture to the GLSL shader and it will be projected on the swivel surface.\n\n## The Canvas 2D generative swivel code\n\nA canvas 2D of 1200 x 400 pixels is used to draw the swivel engraved texture. We use it to possibly display a text and draw some \"plotted lines\".\n\n```js\nasync function metal(word, swivelPlotted) {\n  const w = 1200;\n  const h = 400;\n  const canvas = document.createElement(\"canvas\");\n  canvas.width = w;\n  canvas.height = h;\n  const ctx = canvas.getContext(\"2d\");\n  ctx.fillStyle = \"#000\";\n  ctx.fillRect(0, 0, w, h);\n  ctx.fillStyle = \"#fff\";\n  let font = \"Arial\";\n  // ...plot things...\n  // ...draw text...\n  return canvas;\n}\n```\n\nHere is what the texture of #472 looks like:\n\n\u003cimg src=\"/images/2022/gnsp/472-texture.png\" width=\"100%\" /\u003e\n\nand here is the final result in the raymarched object:\n\n\u003cvideo muted loop autoplay controls src=\"/images/2022/gnsp/472.mp4\" width=\"100%\"\u003e\u003c/video\u003e\n\n\n### \"plotted\" lines\n\n\u003cvideo muted loop autoplay controls src=\"/images/2022/gnsp/461.mp4\" width=\"100%\"\u003e\u003c/video\u003e\n\nThe swivel can sometimes be \"laser plotted\" which is a reference to the \"plotting\" work I've been doing last year. This \"plotting\" is a simple stacking of lines which create some mountains effect.\n\nIn the following code, `swivelPlotted` is defined when plotting is active and is an array of random values.\n\n```js\nif (swivelPlotted) {\n  ctx.strokeStyle = \"#fff\";\n  ctx.lineWidth = 3;\n  const octaveCount = Math.floor(3 + 6 * swivelPlotted[4]);\n  const perlin = generatePerlinNoise(w, h, {\n    octaveCount,\n    amplitude: swivelPlotted[3],\n    persistence: 0.2,\n  });\n  let pad = [50, 20];\n  let amp = 120 * mix(swivelPlotted[0], swivelPlotted[1], swivelPlotted[2]);\n  let incr = Math.floor(3 + 50 * swivelPlotted[1]);\n  if (incr \u003c 15 || (octaveCount \u003c 4 \u0026\u0026 amp \u003e 40)) {\n    ctx.fillStyle = \"#000\";\n    font = \"Arial Black\";\n  }\n  let heights = Array(w).fill(h);\n  for (let y = h - pad[1]; y \u003e pad[1] + amp; y -= incr) {\n    ctx.beginPath();\n    let up = true;\n    for (let x = pad[0]; x \u003c w - pad[0]; x++) {\n      let dy = amp * perlin[y * w + x];\n      let yy = y - dy;\n      let m = heights[x];\n      if (yy \u003e m) {\n        up = true;\n        continue;\n      }\n      heights[x] = yy;\n      if (up) {\n        ctx.moveTo(x, yy);\n        up = false;\n      } else {\n        ctx.lineTo(x, yy);\n      }\n    }\n    ctx.stroke();\n  }\n}\n```\n\n\n### text\n\nAs shown at the beginning, we can also simply have a text on the swivel. Sometimes it is combined with the plotting effect.\n\n\u003cvideo muted loop autoplay controls src=\"/images/2022/gnsp/322.mp4\" width=\"100%\"\u003e\u003c/video\u003e\n\n```js\nif (word) {\n  ctx.textAlign = \"center\";\n  ctx.textBaseline = \"middle\";\n  const lines = word.split(\"\\n\");\n  const sz = Math.floor(\n    20 + 1200 / (3 + Math.max(...lines.map((l) =\u003e l.length)))\n  );\n  ctx.font = sz + \"px \" + font;\n  lines.forEach((line, i) =\u003e // multi line\n    ctx.fillText(\n      line,\n      w / 2,\n      Math.round(h / 2 + 1.2 * sz * (i + 0.5 - lines.length / 2))\n    )\n  );\n}\n```\n\n\n### sticker text\n\nWe can also use emoji instead of text.\n\n\u003cvideo muted loop autoplay controls src=\"/images/2022/gnsp/365.mp4\" width=\"100%\"\u003e\u003c/video\u003e\n\n## Raymarching shader integration\n\nThe swivel metal texture is rendered using a brownian noise. (like explained in https://thebookofshaders.com/13/)\n\nContextually to the raymarching, we have the local coordinate of the swivel (including its rotation), which allows us to do a texture lookup on the swivel text texture. All of this information (noise and texture) is encoded in the material value (a float). It is possible to encode it on one value like this because it's a grayscale value.\n\n```glsl\nnoiseMetal = fbm(vec2(40.0, 1000.) * p.xy);\nvec2 coord = fract(vec2(1.0, -3.0) * p.xy + vec2(0.5));\nvec4 mt = texture2D(metalText, coord);\nfloat t = mix(0., grayscale(mt.rgb),mt.a * step(p.z, 0.) * step(p.x, -0.5) * step(abs(p.y), 0.16));\nfloat swivelM = 2.2 + t;\ns = opU(s, HIT(swivel, swivelM));\n```\n\n## sticker emoji\n\n\u003cvideo muted loop autoplay controls src=\"/images/2022/gnsp/321.mp4\" width=\"100%\"\u003e\u003c/video\u003e\n\nFor the sticker that appear on the swivel, we add to the distance a disc and we use a side effect `vec3 sticker_color` variable that is later used in the `shade` function. The position of the sticker is randomly placed.\n\n```glsl\n${\n  !opts.sticker\n    ? \"\"\n    : `\nvec2 q = p.xy + vec2(${opts.stickerPosX.toFixed(2)}, ${opts.stickerPosY.toFixed(2)});\nfloat sticker_size = 0.15;\nfloat sticker_border = 0.01;\ncoord = fract(vec2(.5,-.5) * q / sticker_size - 0.5);\nvec4 v = texture2D(stickerText, coord);\nsticker_color = mix(vec3(1.), v.rgb, v.a);\nfloat l = length(q.xy)-sticker_size;\ns = opU(s, HIT(max(\n  abs(p.z + 0.13)-0.005,\n  l-sticker_border\n), 4.2 - step(0.0, l)));\n`\n}\n```","data":{"title":"GNSP ‚Äì the swivel rendering","thumbnail":"/images/2022/gnsp/swivel-thumbnail.png","description":"This fourth article reveals the technique used to render the Nano swivel.","tags":["NFT"]}},{"id":"2022-02-14-gnsp-nanoscreen","year":"2022","month":"02","day":"14","slug":"gnsp-nanoscreen","content":"\n\nThis third article (in a series of 7 articles) reveals the technique used to render the screen display itself.\n\n\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/419.mp4\" width=\"50%\" style=\"float:left; margin-right: 40px; margin-bottom:20px\"\u003e\u003c/video\u003e\n\n**Timeline:**\n\n- [article 1: GNSP ‚Äì the concept](/2021/12/gnsp)\n- [article 2: the 3D distance to a Nano S+](/2021/12/gnsp-raymarching)\n- [**article 3: the nano screen**](/2022/02/gnsp-nanoscreen)\n- [article 4: the swivel](/2022/02/gnsp-swivel)\n- article 5: the background\n- article 6: the video generation\n- article 7: the final drop\n- (?March) public mint\n\n**The collection is browsable on https://greweb.me/gnsp**\n\n**OpenSea: https://opensea.io/collection/gnsp**\n\n\u003cbr style=\"clear:left\"/\u003e\n\nThe screen, displays the unique BIP39 word and can sometimes have an effect or an animation. In the NFT metadata, they are expressed on the \"Screen\" feature, and here is the distribution:\n\n```\n                 \u003cnot defined\u003e: 1527 = 74.6%\n                     scrolling: 251 = 12.3%\n                      blinking: 136 = 6.6%\n                      negative: 81 = 4.0%\n                       complex: 25 = 1.2%\n        negative and scrolling: 11 = 0.5%\n                 half-negative: 8 = 0.4%\n         negative and blinking: 7 = 0.3%\n    half-negative and blinking: 2 = 0.1%\n```\n\nThis means 75% of the time you will only get the text displayed statically but in other case you have various effects implemented.\n\n## Step 1: a Canvas2D texture is used for the word text\n\na 128 by 64 Canvas 2D texture is generated ‚Äì this is the actual resolution in pixels on the actual device.\n\n```js\nfunction screen(word) {\n  const w = 128; const h = 64;\n  const canvas = document.createElement(\"canvas\");\n  canvas.width = w; canvas.height = h;\n  const ctx = canvas.getContext(\"2d\");\n  ctx.fillStyle = \"#fff\";\n  ctx.fillRect(0, 0, w, h);\n  ctx.textAlign = \"center\";\n  ctx.textBaseline = \"middle\";\n  ctx.font =\n    (navigator.userAgent.includes(\"Mac OS\")\n      ? \"\" : \"bold \") + \"22px Arial\";\n  ctx.fillStyle = \"#000\";\n  ctx.fillText(word, w / 2, h / 2);\n  return canvas;\n}\n```\n\n\u003e Ok, there is a funny trick here on an annoying fact: depending on your OS you will have a different font weight, as Mac devices tend to have bolder font, I only used non bold in other cases. \n\n\n\u003c!--\nI will explain in Article 6 why I actually needed to support different OSs during the video generation phase. I have also externalized the `document.createElement(\"canvas\")` and `ctx.fillText` implementations into a function in order to be able to run this in Node.js (against node-canvas library).\n--\u003e\n\nApart this trick, there are nothing fancy here: we are just writing the word in the Canvas. So there are actually no animation logic at all here: the animations (text motion / balls motion in negative) are all implemented in the GLSL shader.\n\n\n## Step 2: the text texture is processed in GLSL\n\nI'm using `regl` library helper and I need to inject the text canvas as a `uniform sampler2D text` parameter:\n\n```js\nuniforms: {\n  text: regl.texture({ data: screenCanvas, flipY: true }),\n```\n\nAfter this, the main trick is to project the 2D Texture of the word onto the 3D raymarched object, and in my case, I simply project it along the Z-axis, globally. Indeed it would need to be applied \"locally\" if the Nano was actually moving or rotating but I didn't need that so we can simply stick to a global mapping.\n\nSo basically:\n\n```glsl\nvec2 coord = someOffset + someMultiplier * p.xy;\nfloat m = step(texture2D(text, coord).x, 0.5);\n```\n\nmakes `m` being a value of either 0.0 or 1.0 based on if the pixel is on or off.\n\nNow, it also need to be pixelated, so we need to round the coordinate:\n\n```glsl\nvec2 coord = someOffset + someMultiplier * p.xy;\nvec2 a = coord * vec2(128.,64.);\ncoord = floor(a) / vec2(128.,64.);\nfloat m = step(texture2D(text, coord).x, 0.5);\n```\n\nThen we add a `edge` effect. This `edge` represents the distance to the edge of a pixel.\n\n```glsl\nvec2 coord = someOffset + someMultiplier * p.xy;\nvec2 a = coord * vec2(128.,64.);\nfloat edge = min(fract(a.x), fract(a.y));\ncoord = floor(a) / vec2(128.,64.);\nfloat m = step(texture2D(text, coord).x, 0.5)\n  * (1.0 - 0.5 * step(edge, 0.25)); // changes the pixel color\n```\n\nThis will accentuate even more the pixel effect as we can see in this zoom:\n\n\u003cimg src=\"/images/2022/gnsp/pixel.png\" width=\"100%\"/\u003e\n\nOk, to precise exactly what `coord` is, here is the actual code:\n\n```glsl\nvec2 coord = fract(fract(vec2(-0.2, 0.5) + vec2(3.6) * p.xy / vec2(-2.25, 1.0)) + ${\n  opts.scrollingScreen ? \"vec2(0.5+floor(time*15.0)/15.0, 0.)\" : \"0.0\"\n});\n```\n\nYou can note that:\n- multiply by `vec2(-2.25, 1.0)` to stretch a bit the font.\n- in case of `scrollingScreen` an offset by `time` is applied on x coordinate, and using some floor function so it does it by \"increments\" (pixel scroll)\n- we apply a whole `fract` function (which is a `% 1.0`) to keep the coord in a 0.0 - 1.0 range and actually make it repeat.\n\nNow, to create the negative effect, what we simply need to do is to either chose `m` or `1.0 - m` as a pixel value. \n\n\nThis is implementing the simple idea to have half of the screen cut into two negative parts:\n\n```glsl\n${opts.halfnegativeScreen ? \"m=mix(m,1.-m,step(coord.y, 0.5));\" : \"\"}\n```\n\nAnd this implements the possibly blinking effect:\n\n```glsl\n${opts.blinkingScreen ? \"m*=step(fract(2.*time),0.5);\" : \"\"}\n```\n\nNow for the more complex animation, the effect varies at a given position on x,y, so we will give it to a function to determine if we need to swap the color. The animation we can see in the video above is a reference to one of my last year creation: [/shaderday/65](/shaderday/65).\n\n```glsl\n${\n  !opts.screenAnimation\n    ? \"\"\n    : `\n      float sz = ${(\n        1 -\n        opts.screenAnimation[3] * opts.screenAnimation[3]\n      ).toFixed(2)};\n      coord -= 0.5;\n      coord *= vec2(2.,1.) * ${(\n        1 -\n        opts.screenAnimation[3] * opts.screenAnimation[3]\n      ).toFixed(2)};\n      coord += 0.5;\n      ${\n        opts.screenAnimation[1] \u003c 0.2\n          ? `coord.y${opts.screenAnimation[1] \u003c 0.1 ? \"+\" : \"-\"}=time;`\n          : \"\"\n      }\n      ${opts.screenAnimation[2] \u003c 0.2 ? `coord.x-=time;` : \"\"}\n      coord=fract(coord);\n      m=mix(m,1.-m,step(shape(coord,2.*PI*time), 0.5));\n    `\n}\n```\n\n\n`screenAnimation` is a array of random values and with that, we can yield variation of the initial `shape` animation which is implemented relatively like in my [/shaderday/65](/shaderday/65):\n\n```glsl\nfloat shape (vec2 p, float t) {\n  float smoothing = 0.15;\n  p -= 0.5;\n  vec2 q = p;\n  pR(p, t + cos(${Math.round(5 * opts.screenAnimation[0] - 2)}. * t));\n  vec2 dist = vec2(0.0);\n  float crop = 99.0;\n  float s = 99.0;;\n  s = fOpUnionRound(q.y, s, smoothing);\ndist = vec2(0.31, 0.0);\nfloat radius = 0.11;\ns = fOpUnionRound(s, length(p + dist) - radius, smoothing);\ncrop = fOpUnionRound(crop, length(p - dist) - radius, smoothing);\n  s = fOpDifferenceRound(s, crop, smoothing);\n  return smoothstep(0.0, 1.0 / min(resolution.x, resolution.y), s);\n}\n```\n\nFinally, we map the \"m\" value to actual colors, and in our case it's basically black and white. Note the usage of `negativeScreen` flag:\n\n```glsl\nmix(\n  vec3(0.01),\n  vec3(1.0),\n  ${opts.negativeScreen ? \"1.-\" : \"\"}m\n)\n```\n\n\u003e This GLSL code is templated in JavaScript as you may notice, it's a trick to make the GLSL compile even faster to avoid having runtime ifs.\n\n**That's it folks! There are nothing more to say about the screen rendering of GNSP.**\n\n\n\u003c!--\n// TODO explain multi platform in VIDEO article\n\n```js\nuniforms: { text: regl.texture(createImageTexture(screenCanvas))\n```\n\nwhere `createImageTexture` in context of web is:\n\n```js\nlet createImageTexture = canvas =\u003e ({ data: canvas, flipY: true })\n```\n\nbut for instance, in context of Node.js implementation is:\n\n```js\nlet createImageTexture = (canvas) =\u003e {\n  const ctx = canvas.getContext(\"2d\");\n  const width = canvas.width;\n  const height = canvas.height;\n  const imageData = ctx.getImageData(0, 0, width, height);\n  return { data: imageData.data, width, height };\n};\n```\n---\u003e","data":{"title":"GNSP ‚Äì the Nano screen rendering","thumbnail":"/images/2022/gnsp/screen-thumbnail.png","description":"This third article reveals the technique used to render the screen display itself.","tags":["NFT"]}},{"id":"2021-12-29-gnsp-raymarching","year":"2021","month":"12","day":"29","slug":"gnsp-raymarching","content":"\n\nThis second article (in a series of 7 articles) reveals the technique used to 3D render the Generative Nano S Plus collection: using a GLSL fragment shader, without any 3D model, raymarching a calculated distance to a Nano S Plus.\n\n\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/509model.mp4\" width=\"50%\" style=\"float:left; margin-right: 40px; margin-bottom:20px\"\u003e\u003c/video\u003e\n\n**Timeline:**\n\n- [article 1: GNSP ‚Äì the concept](/2021/12/gnsp)\n- [**article 2: the 3D distance to a Nano S Plus**](/2021/12/gnsp-raymarching)\n- [article 3: the nano screen](/2022/02/gnsp-nanoscreen)\n- [article 4: the swivel](/2022/02/gnsp-swivel)\n- article 5: the background\n- article 6: the video generation\n- article 7: the final drop\n- (?March) public mint\n\n**The collection is browsable on https://greweb.me/gnsp**\n\n**OpenSea: https://opensea.io/collection/gnsp**\n\n\u003cbr style=\"clear:left\"/\u003e\n\n## The rendering is implemented in a GLSL Fragment Shader\n\nThe generative art is rendered entirely into one \"fragment shader\", which is essentially a GPU program that takes a bunch of inputs and efficiently calculates the pixel colors with your graphic card.\n\nThis is a paradigm I like to call \"Functional Rendering paradigm\": see article https://observablehq.com/@gre/introduction-to-functional-rendering-paradigm\n\nOn top of this paradigm, I have used a technique called Raymarching distance function, see article https://observablehq.com/@gre/introduction-to-raymarching-distance-functions\n\nHere is the main algorithm that implement the raymarching: (where `map` is the distance function)\n\n```glsl\nHIT marcher (inout vec3 p, vec3 dir) {\n  HIT hit = HIT(0.);\n  float t = 0.;\n  for (int i=0; i\u003c120; i++) {\n    HIT h = map(p + t * dir);\n    t += h.x;\n    if (abs(h.x) \u003c .0001) {\n      hit = h;\n      break;\n    }\n  }\n  p += t * dir; \n  return hit;\n}\n```\n\nA fun proof of this is to play with the number of raymarching iteration (modulating iterations from 0 to 120):\n\n\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/glitch.mp4\" width=\"50%\"\u003e\u003c/video\u003e\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/glitch2.mp4\" width=\"50%\"\u003e\u003c/video\u003e\n\nThe main scene is described in this \"map\" distance function:\n\n```glsl\nHIT map (vec3 position) {\n  HIT s = HIT(10. - length(position), 0.);\n  float t = 3. * fract(time);\n  float swivelAngle = PI * ( 1. +\n    cubicInOut(min(1.0, t)) +\n    cubicInOut(min(1.0, max(t - 1.8, 0.0))) );\n  s = opU(s, sdLedgerNanoSPlus(position, swivelAngle));\n  return s;\n}\n```\n\n## The 3D distance to a Nano S Plus\n\nEssentially, a Nano S Plus can be rendered with a bunch of union, difference and intersection math operations. Here are the basic utilities I used:\n\n```glsl\n// SHAPE PRIMITIVES:\nfloat sdCylinder( vec3 p, vec3 c ) {\n  return length(p.xz-c.xy)-c.z;\n}\nfloat sdCappedCylinder( vec3 p, float h, float r ) {\n  vec2 d = abs(vec2(length(p.xz),p.y)) - vec2(h,r);\n  return min(max(d.x,d.y),0.0) + length(max(d,0.0));\n}\nfloat sdBox( vec3 p, vec3 b ) {\n  vec3 q = abs(p) - b;\n  return length(max(q,0.0)) + min(max(q.x,max(q.y,q.z)),0.0);\n}\nfloat sdBox2(in vec2 p, in vec2 b) {\n  vec2 d = abs(p) - b;\n  return length(max(d, vec2(0))) + min(max(d.x, d.y), 0.0);\n}\nfloat sdBoxRoundZ(vec3 p, vec3 b, float r) {\n  return max(sdBox2(p.xy, b.xy-r)-r, abs(p.z)-b.z);\n}\n// SHAPE OPERATIONS:\nfloat fOpUnionRound(float a, float b, float r) {\n  vec2 u = max(vec2(r - a,r - b), vec2(0));\n  return max(r, min (a, b)) - length(u);\n}\nfloat fOpIntersectionRound(float a, float b, float r) {\n  vec2 u = max(vec2(r + a,r + b), vec2(0));\n  return min(-r, max (a, b)) + length(u);\n}\nfloat fOpDifferenceRound (float a, float b, float r) {\n  return fOpIntersectionRound(a, -b, r);\n}\nfloat opSmoothSubtraction( float d1, float d2, float k ) {\n  float h = clamp( 0.5 - 0.5*(d2+d1)/k, 0.0, 1.0 );\n  return mix( d2, -d1, h ) + k*h*(1.0-h);\n}\nvoid pR(inout vec2 p, float a) {\n  p = cos(a)*p + sin(a)*vec2(p.y, -p.x);\n}\n```\n\nThis code might seems complex, but it's relatively simple primitives, some are from this great article: https://iquilezles.org/www/articles/distfunctions/distfunctions.htm\n\n\nWith these utilities, I have developed that function called `sdLedgerNanoSPlus`, that implements the **distance to a Nano S Plus**:\n\n`HIT sdLedgerNanoSPlus (vec3 p, float rot)`\n\nKnowing the \"space distance to an object\" allows to use a raymarching algorithm to render it in 3D.\n\nThe function takes two parameters `p` and `rot`:\n- `p` is the 3D point from which to evaluate the distance. If the Nano S Plus is at 1 meter away from the Nano S Plus, it must return a value of 1 meter. as simple as this.\n- `rot` allows for me to control the rotation of the swivel, so it can be animated from the caller.\n\nThe function returns one `HIT` value. `HIT` is a simple alias to `vec2`, which actually allows me to return a tuple of two values: `(distance, material)`. On top of the distance, I need to track what is the \"closest material\". Basically answering the question: from the point `p` what is the part of the Nano S Plus that is the closest? For instance, the swivel, the plastic part, the screen,...\n\n\u003e I was able to take precise measurements from some wireframes of the actual device and tried to make it as close as possible, transposed into code.\n\n\nThis is probably a lot of code to digest, but here is its implementation:\n\n```glsl\nHIT sdLedgerNanoSPlus (vec3 p, float rot) {\n  float btn = sdBoxRoundZ(\n    vec3(abs(p.x - 0.18) - 0.22, p.z, p.y - 0.155),\n    vec3(0.06, 0.03, 0.04), 0.03);\n  float case2d = sdBox2(p.xy, vec2(0.624, 0.174)-0.08)-0.08;\n  float swivel_hook = sdCylinder(p.xzy, vec3(-0.44, 0.0, 0.074));\n  HIT s = HIT(max(\n    min(\n      opSmoothSubtraction(\n\t\t    min(\n          max(case2d+0.015, abs(p.z+0.12)-0.015), // main casing carving\n          btn-0.004 // btns carving\n        ),\n        max(case2d, abs(p.z)-0.101) - 0.01, // main casing\n        0.008\n      ),\n      min(\n        btn,\n        max(swivel_hook-0.015, abs(p.z)-0.12) // plastic in the casing for the swivel\n      )\n    ),\n    -swivel_hook // carve the swivel hook out\n  ), 2.05);\n  // screen\n  float screen2 = sdBox2(p.xy - vec2(0.18, 0.), vec2(0.27, 0.12));\n  s = opU(s, HIT(max(s.x, screen2), 2.1));\n  // swivel\n  p.x += 0.04;\n  p.x += 0.4;\n  pR(p.xy, rot);\n  p.x -= 0.4;\n  float w = 0.54;\n  float x = p.x + 0.8;\n  float z = abs(p.z) - 0.12;\n  float swivel_radius = 0.192;\n  float swivel_metal_width = 0.006;\n  float rounding = 0.003;\n  float swivel = opSmoothSubtraction(\n    sdCylinder(p.xzy, vec3(-0.4, 0.0, 0.08)), // carved\n    min(\n      sdCappedCylinder(vec3(p.y, z, x - 0.4), swivel_radius, swivel_metal_width),\n      sdBox(vec3(x - 0.41 + w, p.y, z), vec3(w, swivel_radius, swivel_metal_width))\n    )-rounding,\n    0.04\n  );\n  // metal to close the swivel end\n  swivel = fOpUnionRound(swivel,\n    sdBox(vec3(x + 0.135 + w, p.y, p.z), vec3(swivel_metal_width, swivel_radius, 0.123))\n  ,0.01);\n  noiseMetal = fbm(vec2(40.0, 1000.) * p.xy);\n  vec2 coord = fract(vec2(1.0, -3.0) * p.xy + vec2(0.5));\n  vec4 mt = texture2D(metalText, coord);\n  float t = mix(0., grayscale(mt.rgb),mt.a * step(p.z, 0.) * step(p.x, -0.5) * step(abs(p.y), 0.16));\n  float swivelM = 2.2 + t;\n  s = opU(s, HIT(swivel, swivelM));\n  return s;\n}\n```\n\nTo visualize it more, I've animated the code to make it show the different steps union and difference operations:\n\n\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/model.mp4\" width=\"100%\"\u003e\u003c/video\u003e\n\nBut let's try to dive into more details and tricks.\n\n### The main casing\n\nThe first part of the function is making the plastic casing:\n\n```glsl\nfloat btn = sdBoxRoundZ(\n  vec3(abs(p.x - 0.18) - 0.22, p.z, p.y - 0.155),\n  vec3(0.06, 0.03, 0.04), 0.03);\nfloat case2d = sdBox2(p.xy, vec2(0.624, 0.174)-0.08)-0.08;\nfloat swivel_hook = sdCylinder(p.xzy, vec3(-0.44, 0.0, 0.074));\nHIT s = HIT(max(\n  min(\n    opSmoothSubtraction(\n      min(\n        max(case2d+0.015, abs(p.z+0.12)-0.015), // main casing carving\n        btn-0.004 // btns carving\n      ),\n      max(case2d, abs(p.z)-0.101) - 0.01, // main casing\n      0.008\n    ),\n    min(\n      btn, // buttons themself\n      max(swivel_hook-0.015, abs(p.z)-0.12) // plastic in the casing for the swivel\n    )\n  ),\n  -swivel_hook // carve the swivel hook out\n), 2.05);\n```\n\nWe can see that I use a lot of `min()` to combine the primitives. when you apply `min(A,B)` between the distance of objects A and B it is essentially the distance to the union of A and B, because min returns the closest distance. so `min` is an union.\n\nWe can also see the use of many `max()` like for instance `max(case2d, abs(p.z)-0.101) - 0.01` which renders a rounded box but with sharp edge on the Z axis:\n\n![](/images/2021/12/gnsp/casing.png)\n\n\u003e `max(A, B)` is basically the intersection of A and B. Sadly, it's not an \"exact\" distance that limits the ability to bevel a tiny bit the edge. If someone have a trick to implement this shape with the exact distance, I would love to know.\n\n`max(case2d, abs(p.z)-0.101) - 0.01` is therefore made of the intersection of:\n- `case2d` is a 2D box in xy space. defined by `sdBox2(p.xy, vec2(0.624, 0.174)-0.08)-0.08`. The value 0.8 is removed from the box dimension but also then removed from the distance. That value is actually the border radius of the box.\n- `abs(p.z)` is the distance to the Z plan. `abs(p.z)-0.101` is distance to a volume of the plan enlarged with a `0.101` padding.\n\n### Carving out\n\n![](/images/2021/12/gnsp/casingdig.png)\n\nThe first operation done on the casing is to carve the part of the buttons and the inside. This is essentially done with the operation `max(A, -B)` (remove B from A), except here we are using a smooth difference operation to not make too \"sharp\" cuts.\n\nThe same technique is used on the swivel hole:\n\n![](/images/2021/12/gnsp/casingdig2.png)\n\nAnd again done on the model swivel shape:\n\n![](/images/2021/12/gnsp/swiveldig.png)\n\n**but for the swivel, it is done with a \"smoothing difference\"**, which creates the nice rounded effect in the hole:\n\n![](/images/2021/12/gnsp/swiveldigresult.png)\n\nHere is the code that do this:\n\n```glsl\nopSmoothSubtraction(\n  sdCylinder(p.xzy, vec3(-0.4, 0.0, 0.08)), // the carving shape is a cylinder\n  min(\n    sdCappedCylinder(vec3(p.y, z, x - 0.4), swivel_radius, swivel_metal_width), // circle of the swivel\n    sdBox(vec3(x - 0.41 + w, p.y, z), vec3(w, swivel_radius, swivel_metal_width)) // long rectangle metal part\n  )-rounding, // add a bit of rounding (bevel)\n  0.04\n)\n```\n\nand if we tweak that 0.04 value between 0.0 and 0.1, we obtain this interesting range of smoothing subtraction:\n\n\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/smoothdiff.mp4\" width=\"50%\"\u003e\u003c/video\u003e\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/smoothdiff2.mp4\" width=\"50%\"\u003e\u003c/video\u003e\n\nThere would be so much to improve in the modelization. For instance, I couldn't figure out a simple way to make the smooth corner of the swivel metal, so I end up with this simplification which isn't perfect:\n\n![](/images/2021/12/gnsp/modelsimpl.png)\n\n```glsl\n// metal to close the swivel end\nswivel = fOpUnionRound(swivel,\n  sdBox(vec3(x + 0.135 + w, p.y, p.z), vec3(swivel_metal_width, swivel_radius, 0.123))\n,0.01);\n```\n\nDistance function can always be optimized and details can always be added, possibilities are infinite it's only a matter of how much hours do you want to spend on. And on my case, it was basically a weekend.\n\n\n## Lighting and material\n\nThere would be a lot to write about the accumulation of techniques used for the lightning. Basically there are 2-3 lights in the scene and some are casting some shadows using raymarching as well.\n\n\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/material.mp4\" width=\"100%\"\u003e\u003c/video\u003e\n\n*That video looks surprisingly so professional, I love it!* The scene is just this code:\n\n```glsl\nHIT map (vec3 p) {\n  HIT s = HIT(10. - length(p), 0.);\n  s = opU(s, HIT(length(p)-0.5, 2.05)); // sphere distance is just length(p)-radius\n  return s;\n}\n```\n\nIt shows the rendering of a sphere on the plastic material. Before I talk about the materials, let's focus on the lighting.\n\n### The lighting\n\nThe general code that renders the scene is pretty straightforward:\n\n```glsl\nvec3 scene(vec2 uv) {\n  vec3 c = vec3(0.); // color of the pixel to set\n  vec3 p = cameraP;\n  vec3 dir = normalize(vec3(uv - .5, 1.)); // perspective camera\n  dir = lookAt(cameraP, focusP) * dir; // camera focus on a point\n  HIT hit = marcher(p, dir); // this throw camera ray and tells what points it hits (material and distance)\n  vec3 n = normal(p); // this calculates the NORMAL VECTOR on the surface of the hit object\n  c += lighting(hit, p, n, dir); // \u003c= THIS IS WHERE LIGHTING IS CALCULATED\n  c = mix(c, sceneBgColor, pow(smoothstep(4., 10., length(p-cameraP)), .5)); // mist on far away objects\n  return c;\n}\n```\n\nOk, so as you can see on the previous video, we have essentially 2 lights in the scene, one blueish and one redish. They spread differently in the material to simulate a bit their different size. We however can see the user of a THIRD light, which is more in the back and will be useful to simulate the fact the background is emitting its color. This was very useful for strong background colors like orange:\n\n\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/100.mp4\" width=\"100%\"\u003e\u003c/video\u003e\n\nThis is what lighting is implementing:\n\n```glsl\nvec3 lighting (HIT hit, vec3 p, vec3 n, vec3 dir) { // (code is a bit simplified from original)\n  vec3 l, ldir;\n  vec3 c = vec3(0.);\n  l = vec3(lightPos, 1.5, -3.4);\n  vec3 obj = shade(hit, p);\n  ldir = normalize(l - p);\n  c +=\n  0.92 * vec3(0.9, 0.7, 0.6) * (\n    // ambient\n    0.1\n    // diffuse\n    + obj\n      * (.5 + .5 * diffuse(p, n, l)) // half lambert\n      * (0.5 + 0.5 * softshadow(p, ldir, 0.05, 5., 8.))\n    + specular(n, p, hit.y, ldir, dir, 10.)\n  );\n  l = vec3(-lightPos, 5., -2.);\n  ldir = normalize(l - p);\n  c +=\n  0.92 * vec3(0.3, 0.5, 0.6) * (\n  // ambient\n  0.1\n  // diffuse\n  + obj\n    * (.5 + .5 * diffuse(p, n, l)) // half lambert\n  + specular(n, p, hit.y, ldir, dir, 20.)\n  );\n\n  l = vec3(0., 2., 8.);\n  ldir = normalize(l - p);\n  c += bgLightColor * (\n    obj\n    * diffuse(p, n, l)\n    + specular(n, p, hit.y, ldir, dir, 20.)\n  );\n  return c;\n}\n```\n\nYou can see the first thing done here is:\n\n```\n  vec3 obj = shade(hit, p);\n```\n\nIt is a very important step that ask the hit object material \"give me your color\". I will cover it in the Material section.\n\nThen for each light I want to apply, you can see the same pattern:\n\nFirst of all, we will put the light at a specific position in the space. For instance:\n\n```glsl\nl = vec3(lightPos, 1.5, -3.4);\n```\n\nwe calculate the actual light direction with `p` which is the point of interest to color.\n\n```glsl\nldir = normalize(l - p);\n```\n\nAnd then we can finally add to the object its emitted color:\n\n```glsl\n  c +=\n  0.92 * vec3(0.9, 0.7, 0.6) * (\n    // ambient\n    0.1\n    // diffuse\n    + obj\n      * (.5 + .5 * diffuse(p, n, l)) // half lambert\n      * (0.5 + 0.5 * softshadow(p, ldir, 0.05, 5., 8.))\n    + specular(n, p, hit.y, ldir, dir, 10.)\n  );\n```\n\nand there are a few components that is very inspired by the classical \"ambient diffuse specular\" paradigm: https://learnopengl.com/Lighting/Basic-Lighting which I recommend you to read if you want to dive more into this part.\n\nIn my case, I use half lambert technique which I find really interesting for a cartoon-ish effect https://developer.valvesoftware.com/wiki/Half_Lambert and because i don't use expensive ambient occlusion.\n\nI also, in this light case, will use `softshadow` that cast a shadow toward the light direction.\n\n**specular** function is an important utility that have this implementation:\n\n```glsl\nfloat specular (vec3 n, vec3 pos, float m, vec3 ldir, vec3 dir, float p) {\n  return specularStrength(m, n, pos) * pow(max(dot(dir, reflect(ldir, n)), 0.0), p);\n}\n```\n\nnote that we can have different \"specular strength\" for each material or even depending on the position.\n\n### The material `specularStrength()`\n\n\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/material.mp4\" width=\"100%\"\u003e\u003c/video\u003e\n\nAs we can see in this material, the plastic has been added some texturing.\n\nThis is implemented with a fbm noise and it is contained in the material value.\n\nThen I had two possible ways to make it visible: either I change the color of the material OR I change the way it reflect lights in `specularStrength`. I've used the second option for the plastic case:\n\n```glsl\nfloat noiseMetal;\nfloat specularStrength (float m, vec3 n, vec3 p) {\n  if (m \u003c 2.1) {\n  \tfloat v =\n  \t  n.z * fbm(600. * p.xy) +\n  \t  n.x * fbm(600. * p.yz) +\n  \t  n.y * fbm(600. * p.xz);\n  \treturn 0.4 + 0.3 * v;\n  }\n  if (m \u003c 2.2) {\n    return 2.0;\n  }\n  if (m \u003c 4.) {\n    return 0.6 - 0.5 * noiseMetal + 1. * ceil(m-2.21);\n  }\n  return 0.4;\n}\n```\n\nSo pretty basic stuff, I use `m` material value, which is a one dimension value to express all materials in. in plastic case, we are in the `m \u003c 2.1` case which have a strong noise frequency.\n\nOn the same principle, we can see it is also done for the swivel metal.\n\nI used some trickery here because, as the swivel is moving, I had to use a global variable to set the noise value because I don't have the \"local position\", only the global position.\n\nThe `noiseMetal` is set directly from the `sdLedgerNanoSPlus` function with this:\n\n```glsl\nnoiseMetal = fbm(vec2(40.0, 1000.) * p.xy);\n```\n\nwhere `p` is a local position of the swivel, contextual to the rotation applied to it.\n\nThe 40/1000 implements a stretched fbm noise which nicely recreates the effect:\n\n![](/images/2021/12/gnsp/metaltexture.jpg)\n\n\n### The material `shade()`\n\n`shade(hit,p)` is essentially the material coloring function. It tells what color does the object reflect at a given hit position.\n\nHere is its implementation for the whole scene (I have omitted some part that will be covered in other articles):\n\n```glsl\nvec3 shade (HIT hit, vec3 p) {\n  if (hit.y \u003c 2.0) return sceneBgColor;\n  if (hit.y \u003c 4.0) {\n    if (hit.y \u003c 2.1) {\n    \treturn plasticColor;\n    }\n    if (hit.y \u003c 2.2) {\n      return ...SCREEN RENDERING HERE...;\n    }\n    // swivel metal\n    return vec3(0.7 - 0.1 * noiseMetal - 0.2 * (hit.y - 2.2));\n  }\n  if (hit.y \u003c 5.0) {\n    return stickerColor;\n  }\n  return vec3(0.0);\n}\n```\n\nSo it's a simple, \"give me the color of the material id number\". The only trick for the noise metal was to make it a bit darker on some part to accentuate the metal effect.\n\n**The next article is going to dive into what this `...SCREEN RENDERING HERE...` part is doing, and more generally how I managed to also make the screen display text from inside a shader!**","data":{"title":"GNSP ‚Äì the 3D distance to a Nano S Plus","thumbnail":"/images/2021/12/gnsp/glitch.png","description":"This second article (in a series of 7 articles) reveals the technique used to 3D render the Generative Nano S Plus collection: using a GLSL fragment shader, without any 3D model, but with raymarching distance function technique.","tags":["NFT"]}},{"id":"2021-12-28-gnsp","year":"2021","month":"12","day":"28","slug":"gnsp","content":"\n\nThis article is the first of a series of 7 articles that will explain the development and release of GNSP ‚Äì a special NFT collection I've been secretly working on for a Christmas gift offered to my colleagues at Ledger. It has been a wonderful year between my crypto-developer work and my crypto-artist work and this collection is here to celebrate it.\n\n**Timeline:**\n\n- [**article 1: GNSP ‚Äì the concept**](/2021/12/gnsp)\n- [article 2: the 3D distance to a Nano S Plus](/2021/12/gnsp-raymarching)\n- [article 3: the nano screen](/2022/02/gnsp-nanoscreen)\n- [article 4: the swivel](/2022/02/gnsp-swivel)\n- article 5: the background\n- article 6: the video generation\n- article 7: the final drop\n- (?March) public mint\n\n\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/1558.mp4\" width=\"50%\" style=\"float:left; margin-right: 20px; margin-bottom:20px\"\u003e\u003c/video\u003e\n\nThe NFT collection is called \"GNSP\" which stands for \"Generative Nano S Plus\". The Nano S Plus is the new incoming device that Ledger will release next year, it was [announced at Ledger Op3n a few weeks ago](https://www.youtube.com/watch?v=qUVkfPwSdyg\u0026list=PL6VM0N695IhkMNEvBAOoLeKLU05qDNfd5).\n\n\n\nThe NFT collection is pushed on Polygon (Ethereum Layer-2) and will have in total **2048 items** ‚Äì one is unique for each word in [the standard BIP39 wordlist](https://github.com/bitcoin/bips/blob/master/bip-0039/english.txt). A dedicated contract was developed that allowed to do an internal airdrop but will later allow to open for a public mint phase. Right now, there are 233 mints done and owned by many of us but later, there will be 2048 items fully available! There are many diversity in this **generative art work**. Nothing has been developed \"statically\" everything is random and generative.\n\n\u003cbr style=\"clear:left\"/\u003e\n\n**The collection is browsable on https://greweb.me/gnsp**\n\nThe collection is also browsable on https://opensea.io/collection/gnsp but again, is only partial as not all the items are yet minted in the [contract](https://polygonscan.com/address/0x3c11b1975c17fcf8cbb315d4430233ed1e87cf05). The current plan is to release the public mintability in a near future ‚Äì please stay tuned in case this plan changes.\n\n## Why this collection?\n\nFirst of all, I would like to say that I've been working on this secretly on my own artistic time to drop this as a great surprise for all my colleagues. These are not \"NFTs made by Ledger\" but I had great feedback and help from some colleagues in the team. I did this because i'm passionate but also because there is something valuable and important:\n\nAs an engineer working at Ledger for the past 4 years, it's sad to see how undervalued and forgotten are the security aspects of crypto-currencies in the NFT world. I have seen so many tutorials \"how to get into NFT\" that don't even mention the use of hardware wallets, and sadly we already have seen many hacks due to the use of software wallets not backed by a hardware wallets. The collection is an interesting way to convey this message ‚Äì making it an NFT collection actively make everyone at Ledger learning more about owning NFTs and how it works under the hood as this is still a very bleeding edge technology! Your Ethereum address is your identify, your NFT collection is your digital persona, you better want it secured.\n\n## real-time, model-less 3D raymarching\n\nThe 3D rendering is rendered in real time, in a GLSL fragment shader, without any 3D model, using raymarching distance function technique. Even tho a high quality video is attached to each of the NFTs and stored on IPFS, the source code is fully public [here](https://github.com/gre/gre/tree/master/doodles/generative-nano-s-plus) and it only needs an index to entirely render with just code, it is self hosted on https://greweb.me/gnsp/0. The 2048 series is finalized, it's not a secret and there are no \"reveal\" because its code is immutable.\n\nMore explanation of the rendering technique used will be explained in further articles.\n\n## rarity, randomless and likelyhood\n\nIt's important to understand I didn't design the outcome of the actual collection, I have only designed the likelyhood of things to happen. For instance, I have designed that there is a 2 out of 2048 of chance that one of the item have a \"Snoop Dogg\" on the Swivel metal engraved part but when the dice was thrown, only one item happened to have the trait: https://greweb.me/gnsp/547.\n\n\u003cimg width=\"50%\" src=\"/images/2021/12/gnsp/547.png\" /\u003e\n\nThe designed likelyhood of probability of things to occured have driven the final outcome of the rarity of all the traits. They will be fully available when everything has been minted. Note that the contract distribute the items randomly on-chain on each mint action so even I can't control who gets what. In other words, the collection is deterministically finished and fully discoverable as of today but it is getting distributed pseudo-randomly.\n\nThere are a lot of traits to discover, here is a few highlights. There are reference to some of my past work. There are many different kind of animations of the camera, swivel and screen.\n\n\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/419.mp4\" width=\"50%\"\u003e\u003c/video\u003e\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/100.mp4\" width=\"50%\"\u003e\u003c/video\u003e\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/710.mp4\" width=\"50%\"\u003e\u003c/video\u003e\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/1300.mp4\" width=\"50%\"\u003e\u003c/video\u003e\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/1353.mp4\" width=\"50%\"\u003e\u003c/video\u003e\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/494.mp4\" width=\"50%\"\u003e\u003c/video\u003e\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/1170.mp4\" width=\"50%\"\u003e\u003c/video\u003e\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/1047.mp4\" width=\"50%\"\u003e\u003c/video\u003e\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/1231.mp4\" width=\"50%\"\u003e\u003c/video\u003e\u003cvideo muted loop autoplay controls src=\"/images/2021/12/gnsp/260.mp4\" width=\"50%\"\u003e\u003c/video\u003e\n\nSome articles will cover more of these.\n\n## Development eXperience set up and code architecture\n\nThe first PoC of this artwork was developed in one day, directly inside Google Chrome using the integrated IDE that lives under the Dev Tools: Did you know you can edit and save code from there? I have just used this, I did not use any compiler / webpack / babel but instead, I have directly used web modules and the `.mjs` extension. Unfortunately, I still needed an http server (that just hosts a folder) because `file://` doesn't allow web modules.\n\n![](/images/2021/12/gnsp/dx.jpg)\n\nThe code was decoupled in modules to allow to split the logic, rendering, and execution code:\n\n- **the logic part**: [`features.mjs`](https://github.com/gre/gre/blob/master/doodles/generative-nano-s-plus/features.mjs) have all the likelihood logic that generates all the parameters and \"features\" for the art.\n  - also some utilities: [`rng.mjs`](https://github.com/gre/gre/blob/master/doodles/generative-nano-s-plus/rng.mjs) and [`perlin.mjs`](https://github.com/gre/gre/blob/master/doodles/generative-nano-s-plus/perlin.mjs) that implement all the noise utilities.\n  - it allowed me to run some statistics script to tweak my likelihood.\n- **the render part**: [`art.mjs`](https://github.com/gre/gre/blob/master/doodles/generative-nano-s-plus/art.mjs) takes in input the features/parameters to render the actual art.\n- **the execution part**:\n  - [`index.html`](https://github.com/gre/gre/blob/master/doodles/generative-nano-s-plus/index.html) file that put everything together for web. It's using module import, also also import the only dependency I have: [regl](https://github.com/regl-project/regl/).\n  - [`node.mjs`](https://github.com/gre/gre/blob/master/doodles/generative-nano-s-plus/node.mjs) implements headless Node rendering of the art, using headless-gl and node-canvas. It worked smoothly but I will explain this part more in *article 6* especially on why this ended up not being a so good idea for performance and what other alternatives I used.\n","data":{"title":"GNSP ‚Äì the concept","thumbnail":"/images/2021/12/gnsp/security.png","description":"This article is the first of a series of 7 articles that will explain the development and release of GNSP ‚Äì a special NFT collection I've been secretly working on for a Christmas gift offered to my colleagues at Ledger. It has been a wonderful year between my crypto-developer work and my crypto-artist work and this collection is here to celebrates it.","tags":["NFT"]}},{"id":"2021-12-01-plottable-mountain-moons","year":"2021","month":"12","day":"01","slug":"plottable-mountain-moons","content":"[plotnft]: https://greweb.me/plots/nft\n[fxhash]: https://www.fxhash.xyz/u/greweb\n\n**=\u003e https://www.fxhash.xyz/u/greweb \u003c= is where you can find my plottable generators.**\n\nI plan to release a plottable generator every Wednesday ‚Äì this article covers today's release \u0026 share some thoughts on how to approach generative art variety.\n\n\n**Last week highlight:**\n\n\u003cimg width=\"40%\" src=\"/images/2021/11/plottablestorm/digital.jpg\" style=\"float: left\"\u003e\n\n[See article to explain this concept](/2021/11/plottable-storm) released last week with \"Plottable Storm\" from which I learned and grew some ideas for the next iterations!\nPlottable Storm may still be available on https://www.fxhash.xyz/generative/1050 ‚Äì it was released via a mint of 10tez, which was pretty expensive for the platform's current price average ‚Äì but made sense as the first \"plottable NFT\" of the platform. Note that, getting a physical plot is only an option, on which I will ask (1) to own the NFT and (2) to pay extra fees for me to plot it on demand.\n\n\u003cbr style=\"clear:both\"\u003e\n\n## üéâ Release: Plottable Mountain Moons\n\nOk, this may be the best plot generator I have ever created so far. I have put so much effort. This is months of iterations and experiments concentrated into one generator. I had goosebumps exploring the results and plotting them. This is just an illegal amount of fun.\n\nI have also improved the ink simulating shader and added animation to make it visually interesting on its digital rendered version. Here is a quick preview:\n\n\u003cvideo loop autoPlay muted src=\"/images/2021/12/plottable-mountain-moons/promo1.mp4\"  width=\"100%\"\u003e\u003c/video\u003e\n\n**Find it on =\u003e https://www.fxhash.xyz/u/greweb \u003c=**\n\n## Tips: Prototype, prototype, prototype\n\nI've developed my generator in the mind that it would be realistic to plot with fountain pens, e.g. without destroying the paper. This means I had to test a bunch of prototypes to check that:\n- Paper can handle it (think of it like \"load testing\" but with ink)\n- It's good looking (ink need to react well, some ink mix can be bad looking for instance)\n- that the digital preview and the ink simulation is accurate (I've added many new inks and I had to tweak colors)\n\nHere are a few prototypes I did this weekend... Actually 18 plots! Beware I use that as an iteration loop, so some helped to tweak things and do not necessarily represent a possible outcome anymore, but it's mostly the case.\n\n\u003cimg width=\"100%\" src=\"/images/2021/12/plottable-mountain-moons/all.jpg\"\u003e\n\nEach plot is done on a 21 by 21 centimeters watercolor paper.\n\nHere are a few zooms:\n\n\u003cimg width=\"100%\" src=\"/images/2021/12/plottable-mountain-moons/zoom1.jpg\"\u003e\n\u003cimg width=\"100%\" src=\"/images/2021/12/plottable-mountain-moons/zoom2.jpg\"\u003e\n\u003cimg width=\"100%\" src=\"/images/2021/12/plottable-mountain-moons/zoom3.jpg\"\u003e\n\u003cimg width=\"100%\" src=\"/images/2021/12/plottable-mountain-moons/zoom4.jpg\"\u003e\n\u003cimg width=\"100%\" src=\"/images/2021/12/plottable-mountain-moons/zoom5.jpg\"\u003e\n\u003cimg width=\"100%\" src=\"/images/2021/12/plottable-mountain-moons/zoom6.jpg\"\u003e\n\u003cimg width=\"100%\" src=\"/images/2021/12/plottable-mountain-moons/zoom7.jpg\"\u003e\n\nAnd finally, the OG one that is the generator cover:\n\n\u003cimg width=\"100%\" src=\"/images/2021/12/plottable-mountain-moons/plot.jpg\"\u003e\n\n\n## Economy: Supply and Price\n\nDetermining the supply and price to put an NFT generator has never been easy. In fxhash, there is a great average of price of 0.5tez to 2tez which is very cheap if you think about what is happening in Ethereum. In a way, it needs to fit in this platform range, but I also need to think a lot about the supply: to me, the supply is proportional to the variety of your generator, how different results it can produce. Low Supply creates scarcity while High Supply gives more chance to collectors and reduce the FOMO effect. The price is also a way to limit the number someone can take, but is not very fair for low budget,...\n\nFor my case, and it may only justify for my own plans. I fallback-ed a bit on the idea (at least this time) to make is a Low Supply.\n\nI decided this time to make it high in supply and a relatively accessible price for these reasons:\n\n- This generator covers a great amount of my plotting work this year. A lot of primitives and shape combination has been converged into one big generator. And I find it fair and ethical that I can't anymore in the future release new generators that overlap too much with these. So in a way, this is a sacrifice to make. That means, I better need the supply to be high enough to make the generator fully express its variety.\n- The idea of this NFT, first of all which are pretty cool as digital art, is also to use them as a power to have a physical action: claiming a physical plot to be performed. That physical plot request costs a fee. (currently set at 15tez but may evolve depending on demand and my capacity to respond to it) That means I want the price of the NFT itself to be relatively accessible which means low, which means supply need to have a better offer to those who will want to find the rare gems.\n- I want my collectors to fully embrace collecting for the art and lower the \"collecting for the speculation\". Increasing the supply will decrease the scarcity effect that exist on second market. That said, it's not that high supply (smolskull was 2000!) and there will be room for second market to occur for those who are looking for \"the perfect plot to perform\".\n\n## Tech: In pursuit of the perfect rarity equilibrium\n\nMy plot generator are develop with Rust. I have developed myself my own set of tools and I will basically have a watch loop that recompile and regenerate a .svg each time I change code. My Visual Studio Code set up will have a SVG display within the IDE while I can edit the generative code. This makes a nice ~5s live reloading development loop. Not bad.\n\nThe problem of this is that it's a generator, it yields a lot of different results. So I usually would just have a \"seed\" parameter exposed on the CLI and generate a folder with tons of images. Not very practical neither.\n\nThis time was still a bit of this for the initial development. But at some point I have to really think about the rarity of the results and trying to make everything interesting and unique. on my JavaScript side, I have a `generateVariables(random)` function that takes a RNG and yield a bunch of parameters to give to my generator as well as all the \"features\" to put in the NFT metadata.\n\n### rarity analysis\n\nThis was really useful to split this in a dedicated file, because I could then develop a small node script that would do an analysis on 100 000 tries and give me stats about rarity. For instance:\n\n```\nInk Sherwood Green\n                     undefined: 95.2%\n                      Mountain: 2.1%\n                         Stars: 2.1%\n                          Both: 0.6%\nInks Count\n                             2: 71.7%\n                             1: 28.3%\nMountains 2-Colors\n                            No: 80.6%\n                           Yes: 19.4%\nMountains Visibility\n                           Low: 46.7%\n                          High: 34.8%\n                          Full: 10.8%\n                         Empty: 7.7%\n```\n\nWhich is super useful to try to classify and adjust how much you want a given case to occur. Indeed it forces you to really think about your classification first.\n\n### as a query language\n\nOk, now what's super cool about this is I have a way to query my generator to give me a result of a given query!! So I made it work. Here are few examples:\n\n```\nnode script.mjs 'p[\"Primitives\"] \u003e 4 \u0026\u0026 p[\"Inks Count\"]==2'\nooa7qfERP3s73oycm7hvvtexuidFMvHtcr6dcLRHSpM9Uz33gVt\n...\nnode script.mjs '(p[\"Ink Aurora Borealis\"] \u0026\u0026 p[\"Ink Poppy Red\"]) \u0026\u0026 p[\"Primitives\"]\u003e4 \u0026\u0026 o.a1 \u003e 0.5 \u0026\u0026 o.f1 \u003e 0.02 \u0026\u0026 p[\"Elements Count\"]==\"Normal\" \u0026\u0026 p[\"Mountains Visibility\"]==\"Low\"'\nooAzT5R8UgCy7eFYbvtryykBt8AVVBf25MQG7eknVfzHstahomV\n```\n\nthe query is literally JavaScript code. And here is the simple script to run it:\n\n```js\nconst predicate = new Function(\"p,o\", \"return (\" + (argv2 || \"true\") + \")\");\nlet r, hash;\ndo {\n  const { fxhash, random } = newRandom();\n  r = generateVariables(random);\n  hash = fxhash;\n} while(!predicate(r.props, r.opts));\nconsole.log(hash);\n```\n\n### generating the .svg files in a folder\n\nI didn't plan to explain in depth my tech stack because it would deserve a whole article, but to summarize: **generator is developed in Rust language, compiled in WASM which takes props and outputs SVG, which is rendered with WebGL shaders (sorry for your headaches)**\n\nWASM can run in a web browser and is pretty efficient. (I'm not sure you can say the same of p5js for instance)\n\nWhat's cool is WASM can also run in Node.js. So putting together that `generateVariables(random)` and even the query system, I ended up with a very powerful tool.\n\nFor instance, here I could generate a lot of results that have Blue Mountain and either Yellow or Pink moons:\n\n```\nnode script.mjs '(p[\"Ink Amber\"] || p[\"Ink Pink\"]) \u0026\u0026 p[\"Ink Bloody Brexit\"]==\"Mountain\"'\n```\n\n\u003cimg width=\"100%\" src=\"/images/2021/12/plottable-mountain-moons/folder.jpg\"\u003e\n\nPretty cool hey!\n\nand the JavaScript is faily simple evolution of the previous code:\n\n```js\nconst wasmBuffer = fs.readFileSync(path.join(__dirname, \"./rust/pkg/main_bg.wasm\"));\nconst wasmLoad = import(\"./rust/pkg/main.mjs\");\nwasmLoad.then(async (wasmModule) =\u003e {\n  await wasmModule.default(wasmBuffer);\n  const predicate = new Function(\"p,o\", \"return (\" + (argv2 || \"true\") + \")\");\n  let r, hash;\n  while (true) {\n    const { fxhash, random } = newRandom();\n    r = generateVariables(random);\n    hash = fxhash;\n    if (predicate(r.props, r.opts)) {\n      const svg = wasmModule.render(r.opts);\n      fs.writeFileSync(\"dist/\" + hash + \".svg\", svg, \"utf-8\");\n    }\n  }\n})\n```\n\nI will definitely reuse this idea in the future. **With powerful tools comes great responsibilities!**\n","data":{"title":"‚ÄúPlottable Mountain Moons‚Äù and an approach to query generative art","thumbnail":"/images/2021/12/plottable-mountain-moons/plot.jpg","description":"I plan to release a plottable generator every Wednesday ‚Äì this article covers today's release \u0026 share some thoughts on how to approach generative art variety.","tags":["NFT","plot"]}},{"id":"2021-11-24-plottable-storm","year":"2021","month":"11","day":"24","slug":"plottable-storm","content":"[plotnft]: https://greweb.me/plots/nft\n[fxhash]: https://www.fxhash.xyz/generative/1050\n\n\n## The \"Plottable NFT\" paradigm\n\nI've finalized what will be my paradigm for the next weeks and months:\n\n- I will release a digital version of my work using NFTs and notably generative art NFT platforms.\n- I will allow the NFT owners to request a physical plot on demand. ([https://greweb.me/plots/nft][plotnft])\n\n### A collaborative and open ecosystem\n\nThe NFT, like any regular NFT, points to a digital artwork that can be sold and resold on second market ‚Äì blockchain guarantees its unicity.\n\nIn the \"Plottable NFT\", such **NFT gives a privilege to its owner: requesting a physical plot** (with an artistic fee to cover costs, [see /plots/nft][plotnft]). This obviously can be executed multiple times, yet every physical piece is unique: plotting is an analog process that will inevitably yield different results.\n\n[https://greweb.me/plots/nft][plotnft] sets my own \"plotting services\" terms but any other plotter artist is welcome to collaborate and contribute to offering to the community their own services under their own terms and interpretation of the art ‚Äì I would just ask to verify that the requester owns the NFT.\n\n\u003cimg width=\"100%\" src=\"/images/2021/11/plottablestorm/schema.png\"\u003e\n\nThe NFT vs physical art decoupling allows a business model that allows both the digital creator and the physical creator to be rewarded and makes the NFT truly interesting as a token that has a real-life action: to unlock physical pieces.\n\n\u003e You can be your own artist too, buy a plotter, try it out! This is all designed to be open-source friendly and you can download the SVG yourself!\n\n## @greweb's generative art, plotting and NFTs\n\n### RELEASING \"Plottable Storm\" on fxhash.xyz\n\n[**=\u003e Check my profile on fxhash.xyz \u003c=**][fxhash]\n\n**Here is one digital ink simulated version: (this is not a photo!)**\n\n\u003cimg width=\"100%\" src=\"/images/2021/11/plottablestorm/digital.jpg\"\u003e\n\n**Here is the physically plotted version: (photo of the 3 hours long plot on A4 paper)**\n\n\u003cimg width=\"100%\" src=\"/images/2021/11/plottablestorm/physical.jpg\"\u003e\n\n\nThis work is based on one of my original exploration: [https://greweb.me/plots/312](https://greweb.me/plots/312).\n\n\u003e Plottable Storm is a flow field simulating fountain pen ink drawing on paper on its digital form. 10 inks, many rarity features varying noise, size and color positionning. Having only one color is rare.\n\u003e The digital NFTs can be used to perform a physical action: @greweb plotting on demand a fountain pen plot for those who also want physical originals.\n\u003e\n\u003e Digital and Physical art, hybrid and decoupled:\n\u003e - The art is made as a regular digital NFT on Tezos blockchain ‚Äì its digital form rendered with WebGL shaders.\n\u003e - Token to the physical world: owning each NFT confer the power to request the related physical plot.\n\u003e \n\u003e A collaborative and open ecosystem: an SVG file can be downloaded (Drag\u0026Drop or right-click save) and plotted with fountain pens physically by plotter artists who can interpret it the way they want in their own conditions. @greweb offers his service on https://greweb.me/plots/nft\n\u003e \n\u003e Designed for A4 size. Estimated time of 3 hours of plotting time (25% speed)\n\n\n**Here are a few other physical plots made during the exploration**, especially important to tweak the generator to make a plot that would be feasible without damaging the paper too much!\n\n\u003cimg width=\"100%\" src=\"/images/2021/11/plottablestorm/2.jpg\"\u003e\n\u003cimg width=\"100%\" src=\"/images/2021/11/plottablestorm/3.jpg\"\u003e\n\u003cimg width=\"100%\" src=\"/images/2021/11/plottablestorm/4.jpg\"\u003e\n\n### Coming from past explorations\n\nThis latest work is a continuation of a lot of previous explorations. In the past months, I've been continuously exploring different ways to combine physical art with digital NFT and one of the recent way I've been trying this is [\"Pattern 03\" (article)](/2021/08/pattern-03).\n\nI've been polishing my ink simulator lately and based on the same principle I will be able to generate more plots and have easy ways to iterate on. The tech stack is a bit ambitious, experimental and unique:\n\n- The SVG generator is implemented in Rust language and then compiled and executed in WASM on the browser.\n- It is rendered in real time and enhanced with WebGL shaders.\n\nIn context of [Ethereum absured fees](https://etherscan.io/gastracker), I've decided to focus my NFT effort on other blockchains like Tezos. I'm also focusing even more on my \"plottable art\" lately because, even if it's maybe not where I find the most sucess, it is definitely what I love the most experimenting with. There are so much challenge to tackle with the physical ink and paper constraints.\n\n### Future plans\n\nThis year, I've been releasing a plot a day as shown on [https://greweb.me/plots](https://greweb.me/plots) and I intend to continue to do so, but in a more organized way:\n\n**On the next weeks, and on a weekly basis,** I intend to release a new plottable NFT generator on fxhash. \"Plottable Storm\" is hopefully the series of many! I will adapt some of my best plotting explorations of this year but I also plan to dedicate time on exploring new ideas.\n\n**Starting next year, on a monthly basis,** I intend to have a \"subscribe to my plots\" series where people would receive surprise plots in their mailbox from me and using a NFT membership system. The idea is still a work in progress ‚Äì **Would you be interested in this idea? Don't hesitate to keep in touch with me if you like this concept.**\n","data":{"title":"‚ÄúPlottable Storm‚Äù, a generator of plottable SVGs","thumbnail":"/images/2021/11/plottablestorm/digital.jpg","description":"I will release a digital version of my work using NFTs and notably generative art NFT platforms. I will allow the NFT owners to request a physical plot on demand.","tags":["NFT","plot"]}},{"id":"2021-11-17-duality","year":"2021","month":"11","day":"17","slug":"duality","content":"\nThis weekend, a new platform called fxhash emerged on https://fxhash.xyz/, which opens to everyone the release of NFT art generators ‚Äì similar to artblocks.io but on the Tezos blockchain.\n\nThis platform have a lot of hicetnunc's vibe when it was released back in March: it is very experimental yet a huge community is jumping into it ‚Äì this was long awaited on Tezos. It is trustless and fully opened like hicetnunc was. Despite being in a beta state, there is already a decent UX and DX (developer experience): The developer onboarding might be one of the best among all the generative art platform I've been able to test. Without contacting anyone, I was able to publish my first generator. You get guided interactively and end to end from testing your generator, snapshotting the preview and signing transactions to publish it. It is completely trustless and autonomous.\n\n**Beware the platform is still barely working due to heavy load from the community trying to jump into it.**\n\nThis weekend, I released a variant of one of my \"shaderday\" to make \"Wave\":\n\n\u003e Wave by @greweb ‚Äì https://www.fxhash.xyz/generative/67\n\u003e\n\u003e \u003cvideo src=\"/images/2021/11/duality/wave.mp4\" width=\"60%\" controls autoplay muted loop\u003e\u003c/video\u003e\n\nI will now release a new generator called 'Duality' ‚Äì heavily inspired from https://greweb.me/shaderday/65 but with a lot of variety of results and loop possible.\n\n## Duality\n\nwill appear soon on https://www.fxhash.xyz/u/greweb\n\n\u003e Duality is a minimalistic loop of circles and squares jumping between dark and light sides. wide variety of effects using repetitions, polar modulus, disc and squares. There are very rare cases implemented that may be revealed over mints! A generalization of my \"Jumping Blob\" creation ‚Äì develop in 2021 under water by @greweb.\n\n\u003cvideo controls autoplay muted loop width=\"100%\" src=\"/images/2021/11/duality/a.mp4\"\u003e\u003c/video\u003e\n\u003cvideo controls autoplay muted loop width=\"50%\" src=\"/images/2021/11/duality/b.mp4\"\u003e\u003c/video\u003e\u003cvideo controls autoplay muted loop width=\"50%\" src=\"/images/2021/11/duality/c.mp4\"\u003e\u003c/video\u003e\n\u003cvideo controls autoplay muted loop width=\"50%\" src=\"/images/2021/11/duality/d.mp4\"\u003e\u003c/video\u003e\u003cvideo controls autoplay muted loop width=\"50%\" src=\"/images/2021/11/duality/e.mp4\"\u003e\u003c/video\u003e\n\u003cvideo controls autoplay muted loop width=\"50%\" src=\"/images/2021/11/duality/f.mp4\"\u003e\u003c/video\u003e\u003cvideo controls autoplay muted loop width=\"50%\" src=\"/images/2021/11/duality/g.mp4\"\u003e\u003c/video\u003e\n\u003cvideo controls autoplay muted loop width=\"50%\" src=\"/images/2021/11/duality/h.mp4\"\u003e\u003c/video\u003e\u003cvideo controls autoplay muted loop width=\"50%\" src=\"/images/2021/11/duality/i.mp4\"\u003e\u003c/video\u003e\n","data":{"title":"fxhash: duality","thumbnail":"/images/2021/11/duality/a.gif","description":"Releasing a generative art on fxhash","tags":["NFT"]}},{"id":"2021-08-29-the-day-after-pattern-03","year":"2021","month":"08","day":"29","slug":"the-day-after-pattern-03","content":"\n- [Pre-release article](/2021/08/pattern-03)\n- [Post article: Part 1: covering the rarity aspects](/2021/08/the-day-after-pattern-03)\n- [Post article: Part 2: covering the future plots](/2021/08/plotting-pattern-03)\n\n\u003e ü§ó **The ethblock.art BlockStyle ['Pattern 03'](https://ethblock.art/style/32) sold out yesterday in only 2 hours with 888 pieces.** I still can't believe it. There are no words to describe how thankful I am for all your support. THANK YOU!\n\nYou can browse through the mints on [https://ethblock.art/style/32](https://ethblock.art/style/32).\n\nAs a first disclaimer, I will have to say it's nearly impossible to cover in this article all of the 888 or even to classify them, there are amazing discoveries. I will highlight some of them.\n\n## Rarity in ethblock.art\n\nWith ethblock.art there is always a search for the rare piece. This happens because everyone wants to find the pearl that no one will have, the one that will be unique among them all. Nevertheless, the rarity often appears where no one expected. There is also a bit of game theory at play here: if everyone mints the same rarity, it's not reeeaaally rare anymore in the final collection. What makes a great piece ultimately is the general composition and the (relative) beauty of the piece. I would always recommend not necessarily to search for the \"rarity by design\" (by the BlockStyle creator rules) but to find something that you like with all these components at play.\n\nIn the case of 'Pattern 03', some very cool patterns emerged. Symmetry plays an important role to reveal it, like in Nature.\n\nThe metadata of OpenSea are still loading as we speak but I'm going to highlight a small analysis of what happened with some of the picks to explain it.\n\n## Designed rarity of Pattern 03\n\nThere were a few elements that were covered yesterday (check the [Pre-release article](/2021/08/pattern-03)) and that took an important part yesterday.\n\n### Crosshatch shape\n\nThis shape has been used relatively popularly as it's quite rare.\n\nHere are many examples:\n\n\n[*#6824 BY BITSIKKA.ETH* ![](/images/2021/08/picks/6824.png)](https://ethblock.art/view/6824)\n[*#6739 BY 0X8A53...63CF* ![](/images/2021/08/picks/6739.png)](https://ethblock.art/view/6739)\n[*#6813 BY 0XE20A...866E* ![](/images/2021/08/picks/6813.png)](https://ethblock.art/view/6813)\n[*#6807 BY J0RDAN.ETH* ![](/images/2021/08/picks/6807.png)](https://ethblock.art/view/6807)\n[*#6931 BY SETANIMALS.ETH* ![](/images/2021/08/picks/6931.png)](https://ethblock.art/view/6931)\n[*#6611 BY KLEINTONNO.ETH* ![](/images/2021/08/picks/6611.png)](https://ethblock.art/view/6611)\n\n### The symmetry\n\nSymmetry helped a lot on revealing some human / animals patterns and let me highlight some.\n\n[*#6991 BY 0X5A17...4A17* ![](/images/2021/08/picks/6991.png)](https://ethblock.art/view/6991)\n[*#6958 BY 0XB84D...04D3* ![](/images/2021/08/picks/6958.png)](https://ethblock.art/view/6958)\n[*#6766 BY BITSIKKA.ETH* ![](/images/2021/08/picks/6766.png)](https://ethblock.art/view/6766)\n[*#6776 BY 0XB8BA...F556* ![](/images/2021/08/picks/6776.png)](https://ethblock.art/view/6776)\n[*#6778 BY 0X27F8...269B* ![](/images/2021/08/picks/6778.png)](https://ethblock.art/view/6778)\n[*#6766 BY BITSIKKA.ETH* ![](/images/2021/08/picks/6766.png)](https://ethblock.art/view/6766)\n[*#7154 BY 0X2D55...8034* ![](/images/2021/08/picks/7154.png)](https://ethblock.art/view/7154)\n[*#6808 BY 0X3037...CF6E* ![](/images/2021/08/picks/6808.png)](https://ethblock.art/view/6808)\n[*#6817 BY 0XE63E...1D85* ![](/images/2021/08/picks/6817.png)](https://ethblock.art/view/6817)\n\n### inks used\n\nAs I expected, black was the most common ink used. It's pretty good to get nice contrasts, especially on important patterns.\n\nBlack is 25% of plots, then comes \"Bloody Brexit\" ink which is a dark blue (about 10%). then the great \"Red Dragon\" which is a strong red (about 8%). Then indigo and turquoise take about 5% each.\n\nI don't have precise stats because OpenSea is still indexing.\n\n### colors-duality\n\nAbout 20% of the picks are using two colors so it was an undeniable search from minters.\n\nI was very interested by the combination of colors that were chosen, some I had never considered in the past and it truely was a revelation to me on some ink coupling to try more in future.\n\n[*#6824 BY BITSIKKA.ETH* ![](/images/2021/08/picks/6815.png)](https://ethblock.art/view/6815)\n\n[*#7000 BY PLAYPAUSESTOP.ETH* ![](/images/2021/08/picks/7000.png)](https://ethblock.art/view/7000)\n\n[*#7013 BY 0X81E5...5D63* ![](/images/2021/08/picks/7013.png)](https://ethblock.art/view/7013)\n\n[*#6790 BY 0XB8BA...F556* ![](/images/2021/08/picks/6790.png)](https://ethblock.art/view/6790)\n\n[*#6832 BY 0X42AD...B84E* ![](/images/2021/08/picks/6832.png)](https://ethblock.art/view/6832)\n\n[*#7030 BY 0X6E82...918E* ![](/images/2021/08/picks/7030.png)](https://ethblock.art/view/7030)\n\n[*#6637 BY 0XB8BA...F556* ![](/images/2021/08/picks/6637.png)](https://ethblock.art/view/6637)\n\n## Undocumented rarity of Pattern 03\n\nThere were a few surprises that I didn't document but that some of you found out!\n\n### \"partial\" lines\n\nAnother dimension of the \"colors duality\" is actually to have the second color not plotted at all, it makes it back to one color but with part of the pattern that is simply disabled.\n\nIt's about as rare as having a dual color. And it was used by a few pieces.\n\n**it allowed to keep only one line here:**\n\n[*#6777 BY 0X44DC...F3BD* ![](/images/2021/08/picks/6777.png)](https://ethblock.art/view/6777)\n[*#6736 BY 0X04C4...A644* ![](/images/2021/08/picks/6736.png)](https://ethblock.art/view/6736)\n\nthere are many other examples of \"partial\" lines.\n\n### The \"strikethrough\" lines.\n\nI've made it possible that on top of borders, there would be some strikethrough lines. It was both a really rare feature and also very hard to use properly in a way that it's really estetic. It allows to \"divide\" the space of the canvas.\n\nHere are technically all the possible strikethrough:\n\n\u003cimg src=\"/images/2021/08/strikethrough.jpg\" width=\"400\"/\u003e\n\nHere is a prototype plot that shows how lines blends:\n\n\u003cimg src=\"/images/2021/08/strikethrough-pic.jpg\" width=\"400\"/\u003e\n\nIt's not easy to \"simulate\" in the digital version and be 1:1 with the physical result because all the parameters at stake with the fountain pen, ink and paper.\n\nTechnically, the SVG have a lot of lines that goes back and forth to achieve this:\n\n\u003cimg src=\"/images/2021/08/strikethrough-spec.png\" width=\"800\"/\u003e\n\nOne of the mint:\n\n[*#6979 BY 0XBA29...0345* ![](/images/2021/08/picks/6979.png)](https://ethblock.art/view/6979)\n\n### Ink saturation\n\nOne of the important feature I developed was to try to simulate the effect of important amount of ink. This is something you can see at stake in https://mountainofink.com/blog/diamine-pink which is one of the ink I use.\n\nHere are 3 examples:\n\n[*#6802 BY 0X04C4...A644* ![](/images/2021/08/picks/6802.png)](https://ethblock.art/view/6802)\n[*#6804 BY 0XAE68...FBE3* ![](/images/2021/08/picks/6804.png)](https://ethblock.art/view/6804)\n[*#6800 BY 0XE20A...866E* ![](/images/2021/08/picks/6800.png)](https://ethblock.art/view/6800)\n\nthat last one will be particularly interesting to plot because of the \"red-ish\" effect of that ink.\n\n### Curly loops: a rarity that wasn't found! (correct me if i'm wrong)\n\nThere were a rare possibility that the displacement would have been so high that it would have make lines curly.\n\nThis was in the prototype:\n\n\u003cimg src=\"/images/2021/08/picks/curly.png\" width=\"400\"/\u003e\n\nI have to admit this property is pushing the plotting to a certain extreme because small curls often will be too hard for the paper and the pen will start digging through it. So i'm relief 50% of the plots are not like this. üòÖ\n\n## Organic rarity\n\nThere are rarity aspects that happened organically and that I didn't planned or anticipate at all. That is the beauty of generative art in general. It goes beyond its creator.\n\n### low/stable `f` noise values\n\n(refer to [first article](/2021/08/pattern-03) for explanation of `f`)\n\nWhile there were indeed a possibility for the `f` value to go low or having a very similar value, I didn't anticipate it would be used as a feature and having other factors (e.g. `displacement`)\n\nSo these art appear to not have `f` noises at all:\n\n(fun fact, this block was minted [3 times!](https://opensea.io/collection/blockart?search[stringTraits][0][name]=BlockNumber\u0026search[stringTraits][0][values][0]=13115508))\n[*#7025 BY 0X91D7...28B3* ![](/images/2021/08/picks/7025.png)](https://ethblock.art/view/7025)\n\nas well as this one which nearly have no `f` but have an heavy cos displacement and a pattern I never expected:\n\n[*#7182 BY 0X7DB2...E04B* ![](/images/2021/08/picks/7182.png)](https://ethblock.art/view/7182)\n\nThere are a few similar pieces having these wavy displacement\n\n[*#7193 BY 0X9422...17DF* ![](/images/2021/08/picks/7193.png)](https://ethblock.art/view/7193)\n[*#6938 BY ZEROLIGHTING.ETH* ![](/images/2021/08/picks/6938.png)](https://ethblock.art/view/6938)\n\n### Great usage of the \"radial gradient\"\n\nOne uncommon feature of the generator was to create concentric circles. It can happen either by \"removing\" or by \"adding\" into the `f` factor. It was curated in various interesting ways.\n\n[*#7049 BY PSYCHOKILLER.ETH* ![](/images/2021/08/picks/7049.png)](https://ethblock.art/view/7049)\n[*#7042 BY GABRIELLEMIC.ETH* ![](/images/2021/08/picks/7042.png)](https://ethblock.art/view/7042)\n[*#7178 BY PBOCK.ETH* ![](/images/2021/08/picks/7178.png)](https://ethblock.art/view/7178)\n[*#6714 BY 0XE20A...866E* ![](/images/2021/08/picks/6714.png)](https://ethblock.art/view/6714)\n\n### Pushing the plots to their extreme\n\nI'm not astonished that some mints pushed the generator to its extreme boundaries but some of the picks still surprised me.\n\n**very low line resolution (mod5) with white holes:**\n\n[*#7056 BY 0XBA29...0345* ![](/images/2021/08/picks/7056.png)](https://ethblock.art/view/7056)\n\n**extreme displacement and f going beyond 1.0 (so it overlaps the next line)**\n\n[*#7040 BY 0XD21A...100C* ![](/images/2021/08/picks/7040.png)](https://ethblock.art/view/7040)\n\n**one liner**\n\n[*#7151 BY 0X5AB4...BAD5* ![](/images/2021/08/picks/7151.png)](https://ethblock.art/view/7151)\n\n**invisible art** (which funnily I tried to prevent but you still found one!)\n\n[*#6900 BY 0X81E5...5D63* ![](/images/2021/08/picks/6900.png)](https://ethblock.art/view/6900)\n\n**extreme amount of lines**\n\n[*#6640 BY 0X8A53...63CF* ![](/images/2021/08/picks/6640.png)](https://ethblock.art/view/6640)\n\n\n**A LOT OF OTHER COOL THINGS! impossible to cover everything.**\n\n[Next article on the future plots](/2021/08/plotting-pattern-03)\n\n","data":{"title":"The day after Pattern 03 (1/2)","thumbnail":"/images/blockstyles/pattern-03/05.png","description":"The ethblock.art BlockStyle 'Pattern 03' sold out yesterday in only 2 hours with 888 pieces. The day after Pattern 03, covering the rarity aspects.","tags":["NFT"]}},{"id":"2021-08-29-plotting-pattern-03","year":"2021","month":"08","day":"29","slug":"plotting-pattern-03","content":"\n- [Pre-release article](/2021/08/pattern-03)\n- [Post article: Part 1: covering the rarity aspects](/2021/08/the-day-after-pattern-03)\n- [Post article: Part 2: covering the future plots](/2021/08/plotting-pattern-03)\n\n\n\u003e ü§ó **The claiming phase has ended. Please now refer to https://greweb.me/plots/nft if you want a physical plot.**\n\n## What about the physical art?\n\nQuick recap of the philosophy announced before the BlockStyle was released:\n\n**Decoupling physical art from its digital form:** Not all digital NFT collectors also are physical collectors. It is therefore important to allow both to co-exist without having a limitation on any side.\n\n**Openness and scalability of physical plotting**. The digital NFT have a \"SVG\" button on hover which allow to download the SVG file (right click, download). The SVG format is a vectorial format that is used to define all the \"paths\" of drawing. It is the recipe to plots. That way, this is truly scalable and decentralized. Like blockchain is. I am not necessarily the only one that can plot the work physically. It could be you. It could be another artist. This model allows it.\n\n**An economy around the digital NFTs?**  In this decoupled model, 'Pattern 03' digital NFT are the main pivot NFTs at stake. an economy can be built around them. Physical artists can plot them, create \u0026 sell on side collection that allows the second market to grow. There are many ways this can be traded: for instance swapping the digital NFT as a currency is a possibility that would be recommended, but so is doing a more regular \"selling print as a service\"... Ultimately, the physical artist value their own skills, own \"touch\" ‚Äì a bit like a baker can sell baguettes with a shared recipe designed by ancestors.\n\n## *\"@greweb 'Pattern 03' plots\"* collection\n\nA collection [*\"@greweb 'Pattern 03' plots\"*](https://opensea.io/collection/greweb-pattern-03-plots) has been created on OpenSea.\nIt will curate plots that I will start drawing in the coming days.\n\nIt's going to be a time-consuming process as a plot takes about 1 hour to perform (depending on settings and complexity of the lines). The physical art is designed to be plotted with fountain pens on A4 water-colour 300g paper and then cut into a 21 by 21 centimeters piece with a 5mm padding (so your art is a 20x20cm square). The planned inks to use are those described by the \"Ink\" trait (Diamine inks). The digital NFT has simulated a rendering that will be relatively well respected. There is going to be unpredictability when plotting it for real.\n\n\u003e Disclaimer: Beware I will likely not be able to plot them all. There are 888 items and it takes an average 1 hour to plot. However, I'll start plotting following a bit this curation decision process:\n\u003e - personal favorite picks\n\u003e - I will look at Opensea most starred NFTs\n\u003e - I will look at Opensea most sold NFTs\n\u003e - also if you really want the physical piece. e.g. if you ask me on Discord / Twitter. I can sure take request and add it in the wished list But I can't guarantee it as it would probably takes me 800 hours =)\n\n## [EXPIRED] First phase: üéâ claim a physical plot *before November 1st, 2021*\n\nThis first phase will ends on November 1st, 2021 for all physical NFTs. The presented offer will only be available during that time. The NFT physical I own after this time will still be owned by me and a second phase will be documented later.\n\n**How did it work?**\n\nYou can claim a physical NFT (created on the collection shared above) **as long as you own the digital NFT**. \n\nTo claim it, you make a small Offer on the NFT physical (pay what you want, e.g. 0.03 WETH ‚Äì ideally \u003e0.02 to cover the mint cost) so I can verify you are the NFT digital owner \u0026 proceed to the NFT transfer of that 'physical NFT'.\n\nYou will then get a code to obtain it physically.\n\n**How shipping works?**\n- The 'physical NFT', once acquires, will have unlockable content with instructions on how to contact me for proceeding to shipping.\n- **IMPORTANT: Please use a PO Box or other postal service ( e.g. https://en.wikipedia.org/wiki/Poste_restante ). I do not want to know your home address, thanks.**\n- On my side, I will use a simple (possibly tracked) letter to ship the 20x20cm art internationally.\n\n## second phase\n\n**The second phase is done via https://greweb.me/plots/nft**\n\n\n\u003cimg width=\"50%\" src=\"/images/2021/08/artist-1.jpg\" /\u003e\u003cimg width=\"50%\" src=\"/images/2021/08/artist-2.jpg\" /\u003e\n","data":{"title":"The day after Pattern 03 (2/2)","thumbnail":"/images/2021/08/timelapse.gif","description":"The ethblock.art BlockStyle 'Pattern 03' sold out yesterday in only 2 hours with 888 pieces. Let's cover what's next with future plots.","tags":["NFT"]}},{"id":"2021-08-28-pattern-03","year":"2021","month":"08","day":"28","slug":"pattern-03","content":"\nSee follow-up articles:\n\n- [Post article: Part 1: covering the rarity aspects](/2021/08/the-day-after-pattern-03)\n- [Post article: Part 2: covering the future plots](/2021/08/plotting-pattern-03)\n\nü§ó **The ethblock.art BlockStyle ['Pattern 03'](https://ethblock.art/style/32) was released on August 28th, 2021 and sold out in only 2 hours with 888 pieces.**\n\nPattern 03 is a tribute to fountain pen plotting, generating strokes lines to yield unique patterns. It recreates the condition of ink on paper in a digital art form. The second market will be exciting as these are also physically plottable. A SVG file can be downloaded from each NFT that is the recipe for a physical artist to perform the art physically (20cm square), with their own tools.\n\nThis article goes through many features of the generator and includes plotting tricks, technical notes, and useful information for collectors. You will see images attached, both digital and physical prototypes, at the end the rendering is very similar!\n\n\u003cimg width=\"50%\" src=\"/images/2021/08/pattern-03-post.jpg\" /\u003e\u003cimg width=\"50%\" src=\"/images/blockstyles/pattern-03/01.png\" /\u003e\n\nFor many months, I've been exploring plotting art with fountain pens while exploring many different creative coding techniques. Every day, I've been releasing generators and plotting art that you can find on https://greweb.me/plots. This has been very interesting both technically and artistically. I expanded more and more my algorithm knowledge and creative coding techniques. These various experiences now contribute to this next level: I've made a \"super generator\" that generalizes many of them into one bundled *ethblock.art* BlockStyle generator.\n\nPattern 03 took a lot of patience and iterations to achieve its final result. I put my own expectation pretty high, I wanted great variety (and tweaking parameters is always hard). I also wanted a very realistic rendering technique that would almost look like the physical piece. The digital version had to be as good, if not better, than the possible physical piece that it could produce. I believe a lot in digital NFT by itself and the goal is to embrace digital-first.\n\nIn the past months, I've been thinking a lot about NFTs and the possibilities of physical VS digital art. I see many artists bridging the two worlds as well, there are many great experiments out there, I collected a bunch of physical art myself via NFTs. You already saw me already trying this [\"plot loops\"](/2021/05/plot-loops) concept.\n\n**By minting on the generator, you are generating digital art.**\n\nA lot of effort has been spent on the digital piece itself. It renders in a very realistic way on your browser, almost like if it was plotted, simulating many aspects of plotting. There is even a slight animation that I let you discover! The digital art is a unique 1:1 piece that will be an NFT that includes the SVG recipe:\n\n **By minting on the generator, you are curating one recipe for a possible plot.**\n\nI've been plotting many prototypes to tweak parameters in a way that everything should technically be drawn with fountain pens! By minting, you are curating a plottable art that an artist from the plotting community could use. I've intentionally made it public and open for anyone to download the SVG related to a mint piece: ultimately anyone can plot it. This only unlocks the possibility and I will personally be one of this \"physical artist\" for the coming weeks. **(see *NFT and physical pieces?* section for more information)**\n\n**By minting on the generator, you are a curator of the future plot collection.**\n\nGiving the possibility for curators to select plotted art out of a generator with fine controls to tweak it is beyond anything I've seen done today in this field. In fine, minters have the responsibility to chose something really interesting that worth plotting, to express this responsibility, the fees to mint has been a bit higher than my previous BlockStyle.\n\n \n What I now want to achieve is to make the NFT as good \"digitally\" as it can be \"physically\". I've been plotting many prototypes to tweak parameters in a way that everything should technically be drawn with fountain pens! I want this art to be both physical and digital and in a loosely coupled way that gives full freedom of preferring one or the other or both.\n\n I intend later to offer a way for collectors to obtain the physical creation from me, but I believe this idea is beyond me and anyone in the plotting community could pick these to plot them. I believe a lot in free art, opensource and hacking mindset. I enjoy a lot free initiative like [plotterfiles.com](https://plotterfiles.com/) and this was a great inspiration here.\n\n## What is EthBlock.art?\n\nBefore talking about 'Pattern 03', a quick recap of the platform I'm releasing on:\n\nEthBlock.art aims to create a virtuous ecosystem of \"deterministic art\", code visualization of Ethereum blocks. Everything is data: from the ethereum block of transactions, to the code that visualizes it, and to the NFTs minted/traded using Ethereum transactions (that themselves are into Ethereum blocks).\n\nThis is a virtuous ecosystem, similar to [Supply Chain Transformation concepts](https://en.wikipedia.org/wiki/Value_chain): each actor in this ecosystem add value and get retributed for it, as I tried to explain in this schema:\n\n![](/images/posts/cryptoaliens/ethblockart.png)\n\n## The controls\n\nAs a minter, you get 6 \"mods\" controls.\n\n- **MOD1:** allows you to pick your color. which recreates the conditions of some inks. Black is very powerful but also other inks are very vivid.\n\n- **MOD2:** is controlling two factors, the border width (0.5 to 1.0) and the intensity of the noise (cycles twice).\n\n- **MOD3:** is a general padding control and also tweaks border placement and sometimes lines distance.\n\n- **MOD4:** control the alignment of waves and the amplitude of the \"displacement\" effect.\n\n- **MOD5:** controls the resolution of each line. (beware high values slows down other mods)\n\n- **MOD6:** adds a variable blur effect.\n\n\u003e MOD2 to MOD5 are controlling at least two properties of the art. This is something I like to call the \"duality of controls\". When you make one slider controlling two properties, you are giving a lot of variety while \"constraining\" the boundaries of the possible tweaks, forcing the minter to look for other blocks. This also is very important for the UX of the controls. There is always a tradeoff between being fully in control and having to search another block.\n\n## Rarity features, block data and what drives the generation\n\nIn JavaScript is implemented the logic to extract out the \"parameters\" of the generator from the block data. While some of the work relies on the block hash and use an RNG (fun fact: there are 70 calls to `random()` to make it very diverse in results and rarity), some of the work also are big data analysis on the block data.\n\nI'm going to document here how some of the main features work, You can do a general Ethereum blocks analysis to find the shape you prefer!\n\nThere are some other rarity features that I intentionally do not fully document. You do not need to \"hunt\" for the rarity that is \"designed by the author\", but instead search for the organic rarity \u0026 enjoy patterns and unexpected shapes.\n\n\u003e I crawled a lot of data from coingecko and etherscan to snapshot the prices of all ERC20 tokens in a way that I can value exactly how much a block have in $. I can then compare the Ethereum value and the ERC20 tokens value and do decisions based on it. (evaluated with Aug 2021 static prices)\n\n### Main shapes\n\nThis most important feature split is the main shapes:\n\n- If there is at least one transaction and mostly only Ethereum is transferred on that block (which is the case for very old blocks btw), you're going to get a **\"rectangle spiral\"**.\n- If more $ is transferred in ETH than in ERC20 tokens, it's going to be **vertical lines**, **horizontal otherwise**.\n- And finally, if there are actually exceptional amounts of ERC20 transfers (mostly only ERC20 transfers) it's going to Cross-Hatch.\n\n\n### lines and \"sub-lines\"\n\n**First: The plots are all composed of N main lines.**\n\nThe number of lines are driven by the number of transactions. More there are transactions in the minted art, more likely the number of lines will be high. This goes between 1 and 90 and these extremes are rare.\n\n**Second: each line subdivide into M \"sub-lines\".**\n\nIt is most visible when there are not a lot of \"lines\" as it will diverge so much that sublines are no longer touching:\n\n![](/images/2021/08/turquoise-crosshatch.jpg)\n\nbut it can also be subtle and not separate at all:\n\n![](/images/2021/08/orange_lines.jpg)\n\n### `f` main driver of curves\n\nWhat I call `f` is a function that drives the sublines positioning and effectively makes them expanding or collapsing into one same line.\n\nIt is implemented using various noises and combined with different techniques. The following plot is covering most of them:\n\n![](/images/2021/08/pink-osc.jpg)\n\n- the general noise effect is the most common feature here and is available 99% of the time (not having this noise is rare). It is built with Simplex Noise and domain warping.\n- the circling is an added feature that is not very common. It can have various properties.\n\nThere are also other cases like the fact lines would sometimes stop when the `f` values goes below 0.0.\n\n### `displacement` second driver of curves\n\nDisplacement is a similar yet different technique. It allows displacing the whole lines independently of the `f` values. \n\n- The waving effect seen in the pink image above is produced with simple trigonometry.\n- It can aggressively make lines moving a lot, even \"looping\" sometimes.\n\nDisplacement is overall a relatively rare feature. It is also one possible source of trouble for plotting (can dig the paper if you draw too much at the same place!) so it was pretty well contained.\n\nHere is an example of displacement with aggressive settings:\n\n![](/images/2021/08/aggressive-displacement.jpeg)\n\n\u003e It looks like a photo but this is actually the digital version!\n\n### Inks and colors\n\nThe curator-artist chose one color among 11 inks. Each ink has various properties and some are very vivid. I would overall recommend black which produces very good contrast plots, but it's great to have the general possibility.\n\n![](/images/2021/08/inks.jpg)\n\nThere are some plots that have color duality and that can also be plotted:\n\n![](/images/2021/08/color-duality.jpg)\n\n\u003e This is a prototype and I was not careful as the corner and the final line scratch. Plotting is a challenge!\n\n## Plot optimizations\n\nA plotter is a machine that follows your drawing paths, here using SVG. When you do a SVG `\u003cpath\u003e` every `move` (M) will be an actual move so you got to avoid useless moves. As it's easy to \"move to the start\" in algorithms, something that should not happen here.\n\nSometimes, the pen also was going up where I planned to \"continue\", this was the case of the initial algorithm of the \"rectangle spiral\", I avoided doing moves and preferred to connect lines.\n\nTo optimize the traveling of plotting lines, I've made it go back and forth: The first line is drawn from left to right, the second line gets drawn from right to left, and so on... This allows to avoid too much undesired ink accumulation or other pen/paper issues by alternating the direction.\n\nAnother problem I've optimized is the order of plotting lines and sublines. Instead of plotting all sublines in once, I preferred to plot the first subline of each line, then the second subline of each line, and so on... Despite the small extra move it requires, it's very important to balance the ink evenly as some inks tend to \"dry\" a bit over time which makes the drawing \"fade away\" as it flows. What's interesting is you can see the fading happening on each line:\n\n![](/images/2021/08/strokes-zoom.jpg)\n\nwhich is something I've simulated on the digital piece too! The effect varies from one ink to another.\n\n\u003e See also how the ink behaves on this photo? accumulation of this ink produces red-ish colors. Something that can be observed on the digital generator as well!\n\n## Technical stack\n\nTechnically speaking, I believe this is the best achievement I did in a few years and profoundly refreshing. I managed to connect my creative coding pipeline: Rust generating SVG files, used through WASM and post-processed in WebGL to add extra effects and simulate the ink drawing. This was a blast to put these all together and I feel I've found something I'll keep doing in the next weeks and months.\n\nThe choice to use Rust was initially a personal exploration, using a language that is meant to be performant while being relatively high level and functional. It's something I did initially for plots and it helped on time-consuming algorithm. I've used it here for two reasons: first of all, most of my tooling for plotting is written in Rust, I'm very fluent it in now and I have many helpers I could reuse, secondly, there is a gain of performance. I have initially written my code in JavaScript and had some bad performance on the simple templating of the SVG code, I've estimated a gain of about x2 on the performance of using Rust. There are no super complex algorithms here, but it's still 2-3 loops with big iterations on them. The performance overall can still feel a bit slow, but you have to imagine each mod changes regenerate the SVG that can be up to 4 MB of XML.\n\nI didn't try to enable asynchronicity or parallelism but it's something I will dig in the future when I'll do more complex work like shape packing (that can require parallelization).\n\nThis is a melting pot of technologies as I had to work on 3 aspects:\n- In JavaScript is managed the logic that handles mods and block data and bakes the \"parameters\" of the generator.\n- In Rust is implemented the generator itself, taking parameters in input and resulting SVG file.\n- In WebGL/GLSL and gl-react library is implemented the simulation of ink on paper rendering.\n\n## NFT and physical pieces?\n\nThe release of \"Pattern 03\" on EthBlock.art is primarily \"digital\" NFTs. Every BlockArt is going to be a digital NFT that includes the SVG recipe to make it physical.\n\n\n**The physical collection is not meant to be 1 physical for 1 digital**. The idea is that the digital NFT remains unique but the physical art can be explored in different ways or ordered multiple times, it will not be guaranteed unique. Actually I can't prevent the plot to be done X times, like anyone can draw X times a La Joconde. However Nature have randomness and each physical plot will be unique.\n\n**This is by design decoupled.**\n\n- There are curators that prefer digital art, this is the BlockArt. There are curators that may be interest by the physical version.\n\n- The \"physical artist\" do not necessarily need to be the same as the \"digital artist\". The idea here is the art goes beyond me, the digital pieces are recipes and a community of plotters could be using these digital pieces in different ways under different terms and with their own tools. This is the openness of blockchain and NFT and this is secured by the contracts and the ownership of NFTs.\n\n\n**What I plan personally to do \"as a physical artist\"**. Now that this decoupling is explain, let me take the physical artist hat.\nAs physical plot have some cost, there are many ways to cover this. Indeed the simplest way is just to sell physical goods like a regular marchant.\n\nAs I said before *Nature have randomness and each physical plot will be unique.*. As a physical plot is still unique, I will emit an NFT each time I plot something.\n\nIn my case, there will be two ways of buying the physical piece:\n\n- Either you will be able to SWAP your digital NFT against the physical NFT. And physical art will be shipped physically to where you want.\n- Or you will be able to BUY it, but the price will be intentionally HIGHER than the digital NFT / will be following a bit the \"floor price\" of the digital NFTs of the collection.\n\nI will also be the one to chose what I plot, I will not necessarily \"plot them all\" (but I can take requests) and I will not necessarily keep a 1:1 relationship as shared before. Actually, if you chose the SWAP option, I will have back the NFT digital, this is the way I get \"paid\" as I can then put it back on market.\n\nI believe this ecosystem will be amazing and will allow a lot of options and freedom. I am so thrilled by the possibilities of NFT.\n\n\u003cimg width=\"50%\" src=\"/images/2021/08/artist-1.jpg\" /\u003e\u003cimg width=\"50%\" src=\"/images/2021/08/artist-2.jpg\" /\u003e\n","data":{"title":"Pattern 03, the first plottable generator on ethblock.art","thumbnail":"/images/2021/08/pattern-03-post.jpg","description":"Pattern 03 is a tribute to fountain pen plotting, generating strokes lines to yield unique patterns. It recreates the condition of ink on paper in a digital art form. The second market will be exciting as these are also physically plottable. A SVG file can be downloaded from each NFT that is the recipe for a physical artist to perform the art physically (20cm square), with their own tools.","tags":["NFT"]}},{"id":"2021-05-03-relics","year":"2021","month":"05","day":"03","slug":"relics","content":"\n\u003cimg width=\"400\" src=\"/images/2021/05/relics.gif\" /\u003e\n\n# Releasing **Relics**, my first curated NFT collectible series, on Tezos blockchain and platform [_hicetnunc.xyz_](https://hic.link/greweb).\n\n**Collection:** There are 28 curated visual loops that will be shared on the 7 days of this week. 4 NFT a day. I will use French day names as well as French seasons to name them all.\n\n**Technical:** Relics collection are unique animation made with Perlin noise, cellular automaton and GLSL. The generator is fully open source and available on my website here: https://greweb.me/shaderday/63 as I strongly believe in open sourcing literally everything I do even as an artist. That said, I curated carefully the 28 unique NFTs myself and it's the only official collection of that generator.\n\n\u003e Contact me if you are the buyer to obtain the parameters of your NFT. Unfortunately it's not possible yet to \"unlock\" content as part of buying a NFT but I would be glad to give you the parameters of your NFT! I can also generate high resolution versions on demand (but also, the open sourced code allows it, once you have these parameters!).\n\n_Lundi d'hiver, Lundi de printemps, Lundi d'√©t√©, Lundi d'automne._\n\n\u003ca href=\"https://www.hicetnunc.xyz/objkt/61391\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmSbZ7iK1S2aDKtTM4HLkzjZrMCwgyDhNbqum8kQTfF44N\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/61403\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmbGrg2mckEbz5u6ShuQi5SbRcUKkEiY9S8irExUiJLizS\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/61410\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmZSQduPnXvmWvFo8JdC3WrxjzU9MbaKGD2JtywtWRq6C8\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/61413\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/Qmb2qS5Vp7YPfW5KqqgEm4aFqeTaGv2RXb81j1mG9dcLqL\" width=\"25%\"/\u003e\u003c/a\u003e\n\n_Mardi d'hiver, Mardi de printemps, Mardi d'√©t√©, Mardi d'automne._\n\n\u003ca href=\"https://www.hicetnunc.xyz/objkt/63131\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmQHcBZ18wYDMq2B2i7AfQFC1e5CJytgsYxYzuanJ8Zxuy\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/63126\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmZRfP9fNKWRizNzHEiB4yHFPTSAuij53XJzXdeqz2qqNu\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/62848\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmT4xPg39PyVDEYGSi1BRce62jJnt2w8SmCDxEYJaZ47cQ\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/62839\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmXrWE4csSkPXYRg3BqhFmKVCzbXGDtNLRkku6Gc2PCY2Y\" width=\"25%\"/\u003e\u003c/a\u003e\n\n_Mercredi d'hiver, Mercredi de printemps, Mercredi d'√©t√©, Mercredi d'automne._\n\n\u003ca href=\"https://www.hicetnunc.xyz/objkt/64885\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmcwpMrbg3n3iNo1TgQUS5LarKwFwz6kK9wexkc38ybRgT\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/64874\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmeDzVu5G1wLJxEsDXJXtdiJpMWAiRDfJnRgx16VmGH96X\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/64863\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmZuJcvNG2T7sCn959CWEuHZxi1C2Nr5H5JzbV6Sp5Pz3p\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/64852\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/Qmcomh1sD8L6wewENSWnqLT4akpRwrDpgABy8viu68b9Ua\" width=\"25%\"/\u003e\u003c/a\u003e\n\n_Jeudi d'hiver, Jeudi de printemps, Jeudi d'√©t√©, Jeudi d'automne._\n\n\u003ca href=\"https://www.hicetnunc.xyz/objkt/66703\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmRe8RwPjLhatKbSMV5YGf3TDBKdB8W7xVnM8zQzNqe3dM\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/66686\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmNpPhofb4eJJq7YCDpjba8t3fNFQkY8W9ryM2iSreWkwB\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/66683\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmRN9L8PhKmfb4uM2LsFb2C2Gxa3YHZ7KQFZsSbSAUAMZ6\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/66674\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmXh2RAdgP841oJWdyMtBSQ78XnYQwHzEkTF8qze6hnbCn\" width=\"25%\"/\u003e\u003c/a\u003e\n\n_Vendredi d'hiver, Vendredi de printemps, Vendredi d'√©t√©, Vendredi d'automne._\n\n\u003ca href=\"https://www.hicetnunc.xyz/objkt/68578\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmTXdtycdrp4v6UWymCeDJCamxHqUdHdy99XfK7vmVqqtx\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/68575\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmVyxcykCS8f1vpriReyedAfdP1M9wHq8pwyKtWtoUgXRb\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/68572\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmY1HcgrX7S5wxaYxRoFuxUWEdG6W6gAApDpzc1oYHSUDt\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/68522\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmSc8Q5ozk8nhLvzF6jry613WDhvFX6PjbqnKbyb97smhy\" width=\"25%\"/\u003e\u003c/a\u003e\n\n_Samedi d'hiver, Samedi de printemps, Samedi d'√©t√©, Samedi d'automne._\n\n\u003ca href=\"https://www.hicetnunc.xyz/objkt/69282\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmasrfTskgzSj48gG62xFyWvXrJXWznyPakMB7GtYwgcY3\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/69280\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmdvFEWzkbK2yPNSNURPNNJTkGf55MoDfboXboCvYwVUob\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/69276\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmZhD4pwmRB43CN7hXKouGHMgCdwuFfnBweJfNBeSJeTBt\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/69275\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmdgVW9CnMY1P6o41EEaiwsJMDQdgxhPzo4xmrV8YLLJ66\" width=\"25%\"/\u003e\u003c/a\u003e\n\n_Dimanche d'hiver, Dimanche de printemps, Dimanche d'√©t√©, Dimanche d'automne._\n\n\u003ca href=\"https://www.hicetnunc.xyz/objkt/70994\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmbTE4QGptyHGYsyemttEXSDoydapMuBiT94kooD3zLHc9\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/70991\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmXMUCZmptV4MyXJ8zUMM9ZoAfmWMLCTr5toPH99YLo6LY\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/70973\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmfVNN6bfzhsEK5fx45DwQTQ6hMrbASxgGzQnSgTwJEbJd\" width=\"25%\"/\u003e\u003c/a\u003e\u003ca href=\"https://www.hicetnunc.xyz/objkt/70967\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmU1VEYV7zyyN3LhES4mgAm8qdgjgvDERDRqu7GJnKtVQL\" width=\"25%\"/\u003e\u003c/a\u003e\n\n_published on [hic.link/greweb](https://hic.link/greweb)_\n","data":{"title":"Relics NFT collection","thumbnail":"/images/2021/05/relics.gif","description":"Relics collection are unique animation made with Perlin noise, cellular automaton and GLSL.","tags":["NFT"]}},{"id":"2021-05-01-plot-loops","year":"2021","month":"05","day":"01","slug":"plot-loops","content":"\nI am thrilled to release my first **\"plot loop\"** and plan to release one every week. Let me explain simply the idea:\n\n- an animation is generated with code, (SVG plottable files)\n- Each frame of this animation is plotted physically,\n- photographed back into a digital picture,\n- and post-processed back into a video that makes it an artistic creation by itself as well as each physical plot being one \"edition\" of that plot loop series.\n\n\u003e **You can see all my plot loops made so far at [https://greweb.me/plots/tags/plotloop](https://greweb.me/plots/tags/plotloop)**\n\n**So there are two creations:**\n\n- the animation is itself digital art =\u003e it is released as an NFT, with as many editions as there are frames.\n- each individual frame is physical art =\u003e it can be shipped to first buyer.\n\n### Plots and NFT ?!\n\n**Plot loop animations will be released as [Non Fungible Token (NFT)](https://www.youtube.com/watch?v=Y-i9Jsm95ro)** on [hicetnunc](https://www.hicetnunc.xyz/). It allows anyone with Tezos crypto currency to acquire this video on Tezos blockchain. Digital collectors are collecting one edition of this NFT which adds to their collection: **The NFT video is an art in itself**. On top of this, physical collectors can also claim the shipping of the physical art of the frame corresponding to the collected NFT edition (there is as many editions as there are frames plotted).\n\n**This is a hybrid concept between physical and digital art.** By buying a frame (by order of buy), the first buyer has the possibility to chose what happens to the physical art: either you allow me to keep it OR you can claim it (contact me on Twitter) and have me sending it anywhere in the world!\n\n\u003e I'm writing a bigger article about my recent NFT exploration and my art in general. I've been exploring so many ideas recently. I already sold some plots via NFTs, I am very honored to have found some buyers üòç. They can still be found at [hic.link/greweb](https://hic.link/greweb). I'm currently burning them one after the other to expire the possibility to buy them as I will focus on these new projects.\n\n## First release, the triplanet loop\n\n**hicetnunc NFT: [OBJKT#57902](https://www.hicetnunc.xyz/objkt/57902)**\n\n\u003ca href=\"/plots/121\"\u003e\u003cimg width=\"50% \" src=\"/images/plots/121.gif\"/\u003e\u003c/a\u003e\n\nMy first plot loop is called \"Triplanet\". It's a transition of two parametric functions with many perlin noise displacements. This is a continuation of [\"Planet Holes\"](/plots/100) series as well as recent [\"Parametric stack\"](/plots/111) explorations.\nEvery frame is generated with a Rust script I wrote.\n\n**Triplanet was released in [plots#121](/plots/121).** 16 frames have been plotted with fountain pen on bristol paper (ink: Red Dragon by Diamine).\n\n\u003cimg width=\"100%\" src=\"/images/plots/121_walled.jpg\"/\u003e\n\u003cimg width=\"50%\" src=\"/images/plots/121_zoom.jpg\"/\u003e\u003cimg width=\"50%\" src=\"/images/plots/121_plot.jpg\"/\u003e\n\n**Plotting with fountain pen is challenging** as it requires many search on the ink, paper and many fail and retry. Plotting all these frames took an afternoon.\n\n## What's next?\n\nI plan to release a _plot loop_ every week.\n\nIt's taking a lot of time to plot all frames so there are great challenges: first of all is to reduce the number of frames, secondly is to make them worth it, to make them really good: I want every frame to be unique, to include some specific elements, easter eggs, that make it recognizable out of the other frames.\n\n---\n\n# Please also check out other great artists\n\nI believe a lot of this hybrid idea of mixing plot, animation, and NFTs. It was very funny for me to see all co√Øncidence and convergence of ideas on this topic. It's one of these moments when we have the same ideas at the same time. I'm not the only one exploring this and I want to do a big kudos to these great artists.\n\n### Marion [@atelier_marion](https://twitter.com/atelier_marion/status/1388071908575490048)\n\nwho recently released a very similar idea! \u003cdel\u003eShe has 2 more frames yet to be taken!\u003c/del\u003e Not anymore!\n\n\u003ca href=\"https://www.hicetnunc.xyz/objkt/42546\"\u003e\u003cimg src=\"https://cloudflare-ipfs.com/ipfs/QmXcAWQTaGHQp4BGy1krER284PoaPCNhKYzDrxp86MaM7b\" style=\"max-width: 300px\"\u003e\u003c/a\u003e\n\nShe is also creating impressive drawing work as seen on her website: https://www.marionguy.com/shop\n\n### Maks Surguy [@msurguy](https://twitter.com/msurguy)\n\nwho is behind [plotterfiles.com](https://plotterfiles.com/) platform. He also does a lot of great plotting art and thought about similar concept recently! [Checkout this article about his work.](https://medium.com/poptwig/artist-in-focus-maks-surguy-26277c95b197)\n\n### Thomas Lin Pedersen [@thomasp85](https://twitter.com/thomasp85)\n\nwho is doing a [stuning work](https://www.hicetnunc.xyz/tz/tz2Pkj2xWJovKKCsABjnr3NbyMVJTMBkpTvb) and very impressive landscapes. Also selling NFT for shipping physical print.\n\nHe recently did a very cool NFT in collaboration with **Lionel Radisson** [@MAKIO135](https://twitter.com/MAKIO135):\n\n\u003ca href=\"https://www.hicetnunc.xyz/objkt/53662\"\u003e\u003cvideo style=\"max-width: 300px\" controls autoplay loop src=\"https://ipfs.io/ipfs/QmaGfhSgWX1UTq6jPX11mfHDxccK5eXGYqi6u71zNLaBhY\"/\u003e\u003c/a\u003e\n\n### ...so many other artists!\n\nLet's chat! You can join us on [hicetnunc's discord](https://discord.gg/jKNy6PynPK) on channel #phygital-objkts which explore very similar idea and where other artists will be active!\n\nI'm also available for collaboration.\n","data":{"title":"'Plot loops' concept","thumbnail":"/images/plots/121.gif","description":"Releasing my first 'plot loop', generative animation plotted and published as an NFT.","tags":["plot","plotloop","NFT"]}},{"id":"2021-04-09-cryptoaliens-tech","year":"2021","month":"04","day":"09","slug":"cryptoaliens-tech","content":"\n[main]: /2021/04/cryptoaliens\n\n\u003e See also [CryptoAliens: Genesis][main] main article.\n\nThis article explains how [CryptoAliens: Genesis (ethblock.art)][main] works in technical depth.\n\n\u003cimg src=\"/images/posts/cryptoaliens/032_px.png\" width=\"50%\" /\u003e\u003cimg src=\"/images/posts/cryptoaliens/036_px.png\" width=\"50%\" /\u003e\n\nFirst of all, I would like to point out the [source code is available here on Github](https://github.com/gre/gre/tree/master/blockarts/CryptoAliens).\n\nThis whole idea was kicked off on [Twitch](https://twitch.tv/greweb). A recording is [available on Youtube](https://www.youtube.com/watch?v=WUzOlLq0IAo). Apart from the many glitches this 3 hours session had remained to be solved, the main part of this was implemented that night. Indeed I had to work countlessly on polishing the shaders, lighting and post-processing. I also spent a lot of time using the block data in a meaningful way because it's what [EthBlock.art](https://ethblock.art) really is about.\n\n## EthBlock.art revolutionary idea\n\nBefore going further into the technical details of CryptoAliens, I would like to point out how revolutionary this EthBlock.art idea is.\n\nThe project aims to create a virtuous ecosystem of \"deterministic art\", code visualization of Ethereum blocks. Everything is data: from the ethereum block of transactions, to the code that visualize it, and to the NFTs minted/traded using Ethereum transactions (that themselves are into Ethereum blocks).\n\n**This is a virtuous ecosystem, similar to [Supply Chain Transformation concepts](https://en.wikipedia.org/wiki/Value_chain): each actor in this ecosystem add value and get retributed for it, as I tried to explain in this schema:**\n\n![](/images/posts/cryptoaliens/ethblockart.png)\n\n`CryptoAliens: Genesis` is one possible BlockStyle that I've designed, as a creative coder. It tries to visualize what happened in the Ethereum Block and will take mods into account to try to be as good as possible to deliver interesting possibilities to BlockArt minters.\n\n## Ok, so how is it implemented technically?\n\nIndeed WebGL.\n\nMore precisely, it is implemented with [`gl-react`](https://github.com/gre/gl-react) which is convenient to write and compose [_GLSL Fragment Shaders_](https://www.khronos.org/opengl/wiki/Fragment_Shader).\n\n**here is the big picture of the pipeline:**\n\n\u003cvideo src=\"/images/posts/cryptoaliens/graph.mp4\" width=\"100%\" controls autoplay muted loop\u003e\u003c/video\u003e\n\nThere are 2 main shaders: Mandelglitch (for skin texturing) and Scene (the main raymarching shader). Each of them take a bunch of parameters. `mod1..4` are values from the creator. The rest are inferred from the Block information, they are split into multiple parameters for convenience.\n\nThe parameters `s1..9` are coming directly from `mersenne-twister` library, a [PRNG](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) used to get a wide and deterministic variety of shapes, initialized with the block hash. That said, as pointed in the previous section, the main features of the shape are determined by Ethereum block information itself (number of transactions, timestamp, transfers, gas used,...).\n\nOn top of these, some other parameters are controling key elements they come from more block information (heavy, head, bonesK, arms info,...).\n\nThe technique implemented on the main Scene shader is [raymarching distance functions](https://www.iquilezles.org/www/articles/raymarchingdf/raymarchingdf.htm). The shapes at stake are mostly segments that are merged with a smooth union. There are many loops involved which made it challenging to optimize.\nThere may be issues on some mobile phone even tho it works on mine thanks to a \"pixelated\" version. (downscaling the pixels helped)\n\nBecause of an heavy usage of this technique, the main scene was really challenging to optimize, it actually runs something like 5 FPS. My choice was to not animate it directly (you can see on the rendering graph that actually the buffer don't need to refresh, except when a \"mod\" is changed). However, thanks to intermediary framebuffers we can do 60 FPS animation at the postprocessing level. This is what the final user will get as the mods are static.\n\n### Canvas 2D to draw texts\n\nA Canvas 2D element is used to draw the text that will appear on top of everything.\n\n```js\n/// canvas used to draw in postprocessing ///\nfunction FrameText({ blockNumber, dateText, width, height, kg, bones }) {\n  const onCanvasRef = (canvas) =\u003e {\n    if (!canvas) return;\n    const ctx = canvas.getContext(\"2d\");\n    const w = ctx.canvas.width;\n    const h = ctx.canvas.height;\n    const pad = Math.round(w * 0.01);\n    const padX = Math.round(w * 0.02);\n    const fontS = Math.floor(0.19 * w) / 10;\n    const fontSize = `${fontS}px`;\n    ctx.save();\n    ctx.fillStyle = \"#000\";\n    ctx.fillRect(0, 0, w, h);\n    ctx.font = \"bold \" + fontSize + \" monospace\";\n    ctx.textBaseline = \"top\";\n    ctx.fillStyle = \"#fff\";\n    ctx.fillText(`CryptoAliens specimen #${blockNumber}`, padX, pad);\n    ctx.textBaseline = \"bottom\";\n    ctx.textAlign = \"right\";\n    ctx.font = fontSize + \" monospace\";\n    ctx.fillText(\n      `born ${dateText}, ${kg} kg, ${bones} bones`,\n      w - padX,\n      h - pad\n    );\n    ctx.restore();\n  };\n  return (\n    \u003ccanvas\n      ref={onCanvasRef}\n      width={String(width * 2)}\n      height={String(height * 2)}\n    /\u003e\n  );\n}\nconst FrameTextCached = React.memo(FrameText);\n```\n\n### How is Mandelglitch used?\n\nAs said, [Mandelglitch BlockStyle](https://ethblock.art/create/17) is re-used in this CryptoAliens BlockStyle. This really is the power of gl-react: it makes such composability really easy to do, the same way you can compose React components.\n\nYou can see in the [Youtube recording](https://www.youtube.com/watch?v=WUzOlLq0IAo) the way I have implemented it initially: it is just a simple import of Mandelglitch.js (literally the BlockStyle as-is) that I can just send as a uniform sampler2D.\n\n```\n\u003cNode\n  shader={sceneShaders.scene}\n  uniforms={{\n    t: \u003cMandelglitch block={block} mod1={mod1} mod2={mod2} mod3={mod3} /\u003e,\n  ...\n```\n\nafter that, it was simpler to embed Mandelglitch in the BlockStyle.\n\nThe way Mandelglitch texturing is used however is that I will only use the \"red\" component and remap it to CryptoAliens' own palette, in order to have a better control of the coloring.\n\n### Code organisation\n\nReact and Gl-React allows to organize the code relatively easily. First of all each pass in the rendering scene is a component, then shaders are organized in the `Shaders.create` usage. I've tried to collocate them (still in same one big file to simplify the upload to EthBlock.art).\n\nI find it pretty convenient to externalize piece of the logic into \"hooks\" function. Example:\n\n```js\nconst CustomStyle = (props) =\u003e {\n  // prettier-ignore\n  const { block, attributesRef, mod1, mod2, mod3, mod4, highQuality, width, height } = props;\n  // prettier-ignore\n  const { kg, bones, theme, background, s1, s2, s3, s4, s5, s6, s7, s8, heavy, head, bonesK, armsLen, armsSpread, armsCenter, armsEndW, dateText, blockNumber } =\n    useBlockDerivedData(block, mod1, mod2, mod3, mod4);\n\n  useAttributesSync(attributesRef, kg, bones, theme);\n\n  return (\n    \u003cLiveTV\n      text={\n        \u003cFrameTextCached ... /\u003e\n      }\n      ...\n    \u003e\n      \u003cNearestCopy width={w} height={h}\u003e\n        \u003cScene\n          t={\u003cMandelglitchCached ... /\u003e}\n          ...\n        /\u003e\n      \u003c/NearestCopy\u003e\n    \u003c/LiveTV\u003e\n  );\n};\n```\n\n`useBlockDerivedData` internally uses `useMemo` in order to cache the computation of block data interpretation.\n\nIn order to make **only** one part of the tree to actively re-render, i've used a local `useTime` that would re-render only that part (the LiveTV final shader). It's implementation is trivial:\n\n```js\nfunction useTime() {\n  const [time, setTime] = useState(0);\n  useEffect(() =\u003e {\n    let startT;\n    let h;\n    function loop(t) {\n      h = requestAnimationFrame(loop);\n      if (!startT) startT = t;\n      setTime((t - startT) / 1000);\n    }\n    h = requestAnimationFrame(loop);\n    return () =\u003e cancelAnimationFrame(h);\n  }, []);\n  return time;\n}\n```\n\n## Arms joints rotation, GLSL random and determinism\n\nOk, this is a hard topic. But it's extremely important that every BlockArt reliably produce the same result with the same block data, regardless of the computer used.\n\nThat last \"regardless of computer used\" part has challenged me at the last minute! JavaScript doesn't have this problem because it's stable between implementations (computers, engines). However, **this is not the case with OpenGL / GLSL**: every computer, every hardware (GPU) or possibly the \"backend\" implementation for WebGL ([ANGLE](https://github.com/google/angle) have different backends) can differ when it comes to float precision and primitive results.\n\nIn my shader, I was using the classical `random` function that is documented at https://thebookofshaders.com/10/\n\n```cpp\nfloat random (vec2 st) {\n  return fract(sin(dot(st.xy, vec2(12.9898,78.233))) * 43758.5453123);\n}\n```\n\nIt works very well when you need a nice 2D distributed noise for basic effects **but it is very bad if you strongly rely on a stable \u0026 consistent noise to generate different shapes**.\n\nEmpirically, I can observe that `sin()` yields different results on different computers.\n\nThis was impacting me badly because I was able to see very various shapes:\n\n\u003cimg src=\"/images/posts/cryptoaliens/rand1.png\" width=\"33%\"/\u003e\u003cimg src=\"/images/posts/cryptoaliens/rand2.png\" width=\"33%\"/\u003e\u003cimg src=\"/images/posts/cryptoaliens/rand3.png\" width=\"33%\"/\u003e\n\nWhat I need to varies a bit here is just the angle at each joint of the arms. This is very important for the uniqueness of the creature. The problem is that if each value changes a tiny bit, the whole thing diverge VERY QUICKLY, as the rotation angle will accumulate.\n\nWorse than that, I had a bad pattern to accumulate randomness like this:\n\n```cpp\nfloat ss1 = s1;\nfor (int i = 0; i \u003c armsLen; i++) {\n  ss1 = random(ss1);\n  ...\n}\n```\n\nActually I don't need that, first of all it's probably not good for performance, secondly I can just afford taking the fractional part of a simple polynomial:\n\n```cpp\nfloat arm (inout vec3 p, float index, float w, float h) {\n  float s = sdSegment(p, h, w);\n  float base1 = 305.53 * s1 + 77.21 * index;\n  float base2 = 403.53 * s2 + 69.71 * index;\n  for (int i = 0; i \u003c armsLen; i++) {\n    float fi = float(i);\n    float ss1 = fract(base1 + 9.412 * fi);\n    float ss2 = fract(base2 + 8.823 * fi);\n    pR(p.xy, 8. * s4 * (ss2-.5));\n    pR(p.xz, 6. * s5 * (ss1-.5));\n    s = fOpUnionSoft(bonesK, s, sdSegment(p, h, w));\n    h *= .9;\n    w *= .9;\n    p.y -= 1.2 * h;\n  }\n  s = fOpUnionSoft(bonesK + 0.2 * s5, s, length(p) - armsEndW);\n  return s;\n}\n```\n\nNote that here it's very arbitrary numbers, the point is to obtain variety and unpredictability in the results. `fract` is a very simple operation (take the fractional part of the number). Indeed i'm still prone to approximation, but the risk is limited by the fact i don't go too high in values here. Worse case scenario is it varies a bit the rotation but it should be so tiny that it won't be visible.\n\nAs said before and as seen in this code, the number will be used to do rotations (that `pR` is transforming `p` with some rotations). I use `s4` and `s5` values to give the magnitude of rotations.\n\n**Let's look at a few cases:**\n\nIf both s4 and s5 are very near 0.0, it will be straight arms (it's a rare case therefore).\n\n\u003cimg src=\"/images/posts/cryptoaliens/042_px.png\" width=\"50%\"/\u003e\u003cimg src=\"/images/posts/cryptoaliens/033_px.png\" width=\"50%\"/\u003e\n\nIf one of the s4 or s5 are 0.0, it will be only happening on one \"plan\", or slightly diverging spirals, which I assume also to be rare cases:\n\n\u003cimg src=\"/images/posts/cryptoaliens/017_px.png\" width=\"50%\"/\u003e\u003cimg src=\"/images/posts/cryptoaliens/020_px.png\" width=\"50%\"/\u003e\n\nOtherwise, most of the times, it will be relatively random:\n\n\u003cimg src=\"/images/posts/cryptoaliens/029_px.png\" width=\"50%\"/\u003e\u003cimg src=\"/images/posts/cryptoaliens/032_px.png\" width=\"50%\"/\u003e\n\u003cimg src=\"/images/posts/cryptoaliens/028_px.png\" width=\"50%\"/\u003e\u003cimg src=\"/images/posts/cryptoaliens/026_px.png\" width=\"50%\"/\u003e\n\n## going 128px. Last minute decision, hard tradeoff\n\nDue to concerns on the \"deterministic rendering\" from Ethblock.art folks, I had to make a choice regarding the fact it was too slow on mobile... I've decided to switch to 128x128 rendering for ALL platforms so it's consistent.\n\nAll the images on that article were done on 1024x1024 which is slow on computer and not even working on my mobile phone. (OnePlus)\n\nIt's hard to have efficient raymarching today when you have many items.\n\n**Ultimately, I like how it finally looks, there were some minimalism / cell shaded styles,.. now it embraces Pixel Art even more!**\n\n\u003cimg src=\"/images/posts/cryptoaliens/r02.png\" width=\"50%\" /\u003e\u003cimg src=\"/images/posts/cryptoaliens/r01.png\" width=\"50%\" /\u003e\n\nIt's also always possible to make higher quality version of these rendering and I'm excited to experiment more of these in future.\n\n---\n\nMy name is Ga√´tan Renaudeau, and I'm a noise explorer. **feel free to ping me on Twitter [@greweb](https://twitter.com/greweb)**\n","data":{"title":"CryptoAliens: Genesis, a technical look","thumbnail":"/images/posts/cryptoaliens/026_px.png","description":"Technical aspects of CryptoAliens digital creatures generated with Ethereum blockchain blocks. They can be minted on ethblock.at by anyone, which establishes a limited set of CryptoAliens species.","tags":["NFT","shaders"]}},{"id":"2021-04-08-cryptoaliens","year":"2021","month":"04","day":"08","slug":"cryptoaliens","content":"\n[create]: https://ethblock.art/create/24\n[opensea]: https://opensea.io\n[tech]: /2021/04/cryptoaliens-tech\n\n\u003e See also [CryptoAliens: Genesis, a technical look][tech].\n\n\u003cimg src=\"/images/posts/cryptoaliens/021_px.png\" width=\"50%\" /\u003e\u003cimg src=\"/images/posts/cryptoaliens/001_px.png\" width=\"50%\" /\u003e\n\n\u003e CryptoAliens: Genesis establishes the first embryonic species. Which CryptoAliens are you going to chose? Each creature gets born on an Ethereum block that nourishes its shape: transactions, ETH value transferred, gas,... their unique skin comes from Mandelglitch's BlockStyle with the same rarity scheme.\n\n## What are CryptoAliens?\n\nCryptoAliens are digital creatures generated from Ethereum blockchain blocks. They can be minted on [ethblock.art][create] by anyone, to establishes a limited set of CryptoAliens species in a decentralized matter. Each block produced on Ethereum have unique elements that can be visualized in creative ways.\n\n\u003cimg src=\"/images/posts/cryptoaliens/014_px.png\" width=\"50%\" /\u003e\u003cimg src=\"/images/posts/cryptoaliens/017_px.png\" width=\"50%\" /\u003e\n\n\u003e **CryptoAliens are born and nourished from transactions, transactions are bones, ETH is flesh,... and many other aspects that this article will explain!**\n\n\u003cimg src=\"/images/posts/cryptoaliens/032_px.png\" width=\"50%\" /\u003e\u003cimg src=\"/images/posts/cryptoaliens/013_px.png\" width=\"50%\" /\u003e\n\n### So I decide which ones are the CryptoAliens?\n\n**Yes! As a NFT minter, you are the creator and you contribute at establishing the first 'Genesis' series of CryptoAliens.** You decide which creature deserve to live. You are the curator and it is your responsible to do a lot of research and find the most adorable (or the creepiest?) creature!\n\n\u003cimg src=\"/images/posts/cryptoaliens/036_px.png\" width=\"50%\" /\u003e\u003cimg src=\"/images/posts/cryptoaliens/004_px.png\" width=\"50%\" /\u003e\n\n### How many CryptoAliens are there?\n\nEvery single CryptoAliens species is unique and there are currently 12 millions because that's as many blocks there are today (April 2021). Every 15 seconds, a new block is minted on Ethereum blockchain (with usually hundreds of transactions in it) making a new CryptoAliens possibility.\nThe block is the DNA, but the creature only starts existing when minted!\n\n**TLDR. CryptoAliens only comes to life when someone mint it as an NFT on the [ethblock.art][create] contract.** They can then be sold and traded on [opensea.io][opensea]. There is **a limited amount of CryptoAliens possible to mint** so be wise at your choice. In this 'Genesis' series, the current supply is set to 100!\n\n\u003cimg src=\"/images/posts/cryptoaliens/002_px.png\" width=\"50%\" /\u003e\u003cimg src=\"/images/posts/cryptoaliens/003_px.png\" width=\"50%\" /\u003e\n\n### What is 'Genesis' series about?\n\nThe idea of 'Genesis' is that we are collectively going to create the initial species of this universe (with the NFT we chose to mint).\n\nMinting a _CryptoAliens: Genesis_ specimen is giving birth to the creature, therefore the current NFT is visualized on [ethblock.art][create] as a video tape recording of that time of birth (with the block number, time, weight and number of bones). These data are included in the NFT itself and could be reused in future!\n\n## What determines how a CryptoAliens specimen looks like?\n\nThere are many information contains in Ethereum blocks that will get used to determine the general shape and gives its rarity.\n\n### Block's timestamp\n\n\u003cimg src=\"/images/posts/cryptoaliens/040_px.png\" width=\"50%\" /\u003e\u003cimg src=\"/images/posts/cryptoaliens/041_px.png\" width=\"50%\" /\u003e\n\nWhen the block happened during UTC night, the visual will be in dark mode.\n\n### Block's transactions amount\n\n\u003cimg src=\"/images/posts/cryptoaliens/007_px.png\" width=\"50%\" /\u003e\u003cimg src=\"/images/posts/cryptoaliens/008_px.png\" width=\"50%\" /\u003e\n\nWhen a block contains a lot of transactions it will impact its general weight. It will be highlighted by this very heavy blobs shapes. That said, the weight can be more or less dense based on amount of bones and also unique for each CryptoAliens specimen.\n\n### Block's heavy transfers in ETH\n\n\u003cimg src=\"/images/posts/cryptoaliens/035_px.png\" width=\"50%\" /\u003e\u003cimg src=\"/images/posts/cryptoaliens/020_px.png\" width=\"50%\" /\u003e\n\nAs said in the introduction, \"ETH is flesh\". Even tho most of the time it will impact the general weight of the creature, when a block contains an expectionally high transfer of Ethereum value, it will be highlighted by a big \"head\" on the creature. (\"head\" in doublequote because none of our scientist really figured what is this)\n\n### Block's exceptionally low amount of ETH transferred\n\n\u003cimg src=\"/images/posts/cryptoaliens/005_px.png\" width=\"50%\" /\u003e\u003cimg src=\"/images/posts/cryptoaliens/006_px.png\" width=\"50%\" /\u003e\n\nOn the contrary, when a block contains almost no ETH transfers, the arms will be very thin. Clearly ETH traders didn't nourish enough this poor creature.\n\n### Block's important ratio of gas used (vs ETH value transfer)\n\n\u003cimg src=\"/images/posts/cryptoaliens/023_px.png\" width=\"50%\" /\u003e\u003cimg src=\"/images/posts/cryptoaliens/016_px.png\" width=\"50%\" /\u003e\n\nIt often appears in combination with the previous criteria, if the ratio `total gas / total eth transfers` is high, meaning that a lot of the ETH is into gas, there will be some blobs at the end of the arms.\n\n### ...and more block rare features\n\nThere are a lot of special cases are rare conditions that can happen. I will not disclose and I will let you discover. Some are really rare and some will be discovered in the future (even the author of blockstyle won't be aware of all cases!).\n\n### Block's hash\n\nFinally, the block hash gives variety in the results. It's necessary in order to have truly unique 12 millions species. But it's only complementary to the various other criteria. There are many features that are getting impacted by it, including the skin texturing (see _Mandelglitch BlockStyle_ section).\n\n\u003cimg src=\"/images/posts/cryptoaliens/043_px.png\" width=\"50%\" /\u003e\u003cimg src=\"/images/posts/cryptoaliens/042_px.png\" width=\"50%\" /\u003e\n\u003cimg src=\"/images/posts/cryptoaliens/012_px.png\" width=\"50%\" /\u003e\u003cimg src=\"/images/posts/cryptoaliens/038_px.png\" width=\"50%\" /\u003e\n\n### What controls does the creator have?\n\nAt creation time, the minter also have the ability to move a bit the specimen:\n\n- `mod1` is a simple rotation around it.\n- `mod2` is a simple climbing and zooming.\n- `mod3` will flex a bit the shape to make it torn \u0026 twist a bit.\n- `mod4` have an impact on the color palette scaling.\n\n**On top of this, mods have the ability to transform the skin texturing** which is actually based on [Mandelglitch BlockStyle](https://ethblock.art/create/17)! That means the rarity elements of Mandelglitch are shared in this new BlockStyle.\n\n### mmh, Mandelglitch BlockStyle?\n\n\u003ca href=\"https://ethblock.art/create/17\"\u003e\u003cimg src=\"/images/posts/cryptoaliens/mandelglitch.png\" width=\"10%\" /\u003e\u003c/a\u003e **[Mandelglitch](https://ethblock.art/create/17) is a BlockStyle on [ethblock.art](https://ethblock.art/create/17), derived from Mandelbrot fractal.**\n\nThe visibility of Mandelglitch on the skin has been intentionally contained, but sometimes it is more visible. Here are two examples:\n\n\u003cimg src=\"/images/posts/cryptoaliens/031_px.png\" width=\"50%\" /\u003e\u003cimg src=\"/images/posts/cryptoaliens/022_px.png\" width=\"50%\" /\u003e\n\n## ...and, What's next?\n\nWho knows what's next! As everything is available on the blockchain, what you mint is saved immutably and forever. Me or other artists could fork the code ([available on Github](https://github.com/gre/gre/tree/master/blockarts/CryptoAliens)) to make animated version of the CryptoAliens that were chosen (as this code is open source). Also we can imagine doing crossover between species or doing \"evolution\" of these species over time. Everything is possible!\n\n---\n\nSee also [CryptoAliens: Genesis, a technical look][tech].\n\nMy name is Ga√´tan Renaudeau, and I'm a noise explorer. **feel free to ping me on Twitter [@greweb](https://twitter.com/greweb)**\n","data":{"title":"CryptoAliens: Genesis [ethblock.art]","thumbnail":"/images/blockstyles/24.png","description":"CryptoAliens are digital creatures generated from Ethereum blockchain blocks. They can be minted on ethblock.at by anyone, which establishes a limited set of CryptoAliens species. Each block produced on Ethereum have unique elements that can be visualized in creative ways.","tags":["NFT","shaders"]}},{"id":"2016-12-03-gl-react-v3","year":"2016","month":"12","day":"03","slug":"gl-react-v3","content":"\n[cookbook]: https://gl-react-cookbook.surge.sh\n[github]: https://github.com/gre/gl-react\n[jest]: https://github.com/facebook/jest\n[headless-gl]: https://github.com/stackgl/headless-gl\n[bus]: https://gl-react-cookbook.surge.sh/api#bus\n\n# Happy to release **[https://gl-react-cookbook.surge.sh][cookbook]** containing 43 unique examples and API documentation!\n\n\u003cvideo src=\"/images/2016/12/gl-react-v3.mp4\" width=\"100%\" controls autoplay muted loop\u003e\u003c/video\u003e\n\n\u003e If you don't want to be \"spoiled\" by this article, go through [the cookbook examples][cookbook]. This article will explore some of them.\n\n\u003c!--more--\u003e\n\n## gl-react has been rewritten from scratch\n\n**gl-react v3 is a complete rewrite of the v2 implementation for better performance and compatibility with React paradigm.**\n\nThis is not yet published on NPM as it's [still in development][github] (the Web version is pretty ready, React Native version is not implemented).\n\nMost features provided by gl-react v2 are preserved (API haven't changed, [see how similar is the HelloGL example](https://gl-react-cookbook.surge.sh/hellogl)), but v3 fixes most Github issues accumulated for a year.\n\n### The biggest mistake of the previous implementation\n\nIf there is one lesson learned from previous gl-react implementation: **\"unfolding\" / consuming the `children` prop by yourself is (probably) wrong, let React solve this job!** Using React, you can benefit [React reconciliation and diff algorithm](https://facebook.github.io/react/docs/reconciliation.html).\nIn other words, always prefer to keep users' VDOM tree rather than consuming it with `React.Children.*` functions.\n\nI feel dumb not having discovered this before, but if you are not actually rendering DOM it's an easy path for a library to just map, traverse, consume the children tree and just render what you needs (like just a `\u003ccanvas/\u003e`). But this is probably a mistake! First, this makes it impossible to use React Devtools and see the original tree, but more importantly, it breaks interoperability with other libraries (e.g. don't forbid someone to use [react-motion](https://github.com/chenglou/react-motion) or [React Router](https://github.com/ReactTraining/react-router) in the middle of your components!).\n\nA better idea is to preserve the user `children`. Keep your logic in each Component and use the React lifecycle to create and destroy things, and **use [React context](https://facebook.github.io/react/docs/context.html) to connect children to parent**.\n\n\u003e You should better keep user `children`, even if it means rendering it in an empty `\u003cspan\u003e`, _current workaround of `gl-react`, looking forward to hearing from you, idea inspired from the great [react-music](https://github.com/FormidableLabs/react-music)_\n\n### What it means for gl-react\n\nThe gl-react v3 implementation truly uses React lifecycle: **a React Component update triggers a GL redraw**. That way, `shouldComponentUpdate` allows to do partial GL re-rendering. Each Node holds a [framebuffer object](https://www.opengl.org/wiki/Framebuffer_Object) (created on mount, destroyed on unmount) that only get redrawn when component updates and schedules a Surface reflow.\n\n`\u003cNode\u003e` receives the `gl: WebGLRenderingContext` from the ancestor `\u003cSurface\u003e` thanks to [React context](https://facebook.github.io/react/docs/context.html). There is also a `glParent` context (a Surface or another Node) that is used to make GL components discoverable each other so we can build a dependency graph. This dependency graph allows to implement the correct `draw` pipeline (and it's pretty trivial, see [_Section \"under the hood of Surface and Node redraw\"_](#under_the_hook_redraw)).\n\n## `\u003cBus\u003e`, a better way to share computation\n\n[gl-react used to automatically factorize the duplicates elements of the GL tree](http://greweb.me/2016/06/glreactconf/) but **it has been decided to remove this feature**: _This was actually a complex mechanism (a bit too \"magic\"), hard to implement and a premature optimization that can have slower performance._\n\nThe new gl-react embraces the React paradigm: The new way to express a Graph (and share computation) is **using a [`\u003cBus\u003e`][bus]**...\n\n### The `` `()=\u003eref` `` pattern\n\nThe problem we want to solve is to **express a graph with React**, which, at first glance, only allow to represent trees, not graphs!\n\nThe way we can solve this is by using refs and a \"ref getter function\":\n\n1. **a Bus with a ref:** `\u003cBus ref=\"myBus\"\u003e{content to inject}\u003c/Bus\u003e`.\n2. **pass a function that resolves the ref** to pipe Bus into another Node. e.g: `()=\u003ethis.refs.myBus`.\n\n[blurmapdyn example](https://gl-react-cookbook.surge.sh/blurmapdyn)\n\n\u003cvideo loop autoplay controls src=\"/images/2016/12/blurmapdyn.mp4\" width=\"100%\"\u003e\u003c/video\u003e\n\na single ConicalGradient should be used for all blur pass:\n![](/images/2016/12/blurmapdynschema.png)\n\nThere are a few other good examples of ref usages:\n\n- [blurmapmouse](https://gl-react-cookbook.surge.sh/blurmapmouse)\n- [blurimgtitle](https://gl-react-cookbook.surge.sh/blurimgtitle) (same example that was features in 2016 React conf!)\n- [behindasteroids](https://gl-react-cookbook.surge.sh/behindasteroids), crazy port of a game I made for js13k.\n\n\u003e The `()=\u003eref` pattern works only if you call the function after component did update (refs are set at this time).\n\n## The good ol' children function\n\nThere is another pattern for more specific needs: instead of composing by giving an element, you can also compose by giving a **Function that returns an element**. Why that? Because, it's a way to nicely give you the redraw function: `redraw =\u003e \u003cVideo onFrame={redraw} /\u003e`:\n\n[Checkout video example ![](/images/2016/12/videoredraw.png)](https://gl-react-cookbook.surge.sh/video?menu=true)\n\n\u003e We really just want to redraw if there is a new video frame.\n\nWe have merged the 2 patterns into one: if you provide a function, it's just called with `redraw`, and the returned value is used as a texture. We have a few cases to detect what kind of texture it is (and also an [extensible mechanism](https://gl-react-cookbook.surge.sh/api#textureloaders) used by implementations to load platform specific objects).\n[(checkout this if you want to see the code)](https://github.com/gre/gl-react/blob/a33e6aa685479d588646b20dd62e1e25a64a5a47/packages/gl-react/src/Node.js#L704-L783)\n\n## Node backbuffering \u0026 Backbuffer symbol\n\nA new feature allows to inject the previous Node state as a texture. This is called backbuffering. One simple usecase is to implement Motion Blur persistence (like the GIF on top of this article).\n\nWe can also accumulate a state, for instance, to implement Game of Life!\n\n[Game of life glider example ![](/images/2016/12/gol.gif)](https://gl-react-cookbook.surge.sh/golglider)\n\nAnd the whole idea of gl-react (and React) is about composition. For instance, doing a rotating effect of that Game of Life is basically just `\u003cRotate\u003e \u003cGameOfLife /\u003e \u003c/Rotate\u003e`.\n\nAn interesting part is that you can update the GameOfLife at a rate that is independent from the **Rotate** rendering: just by making GameOfLife a pure component that receives a tick, or implementing shouldComponentUpdate update (you have as many choices as React have to [shortcut the rendering](https://facebook.github.io/react/docs/react-api.html#react.purecomponent)).\n\n[golrotscu example![](/images/2016/12/golrot.gif)](https://gl-react-cookbook.surge.sh/golrotscu)\n\n\u003e See the counters that indicate the number of redraw. (the capture preview in the Box only get snapshot each 100ms, but in the real canvas, it runs at 60 FPS)\n\nFinally, please checkout [ibex example](https://gl-react-cookbook.surge.sh/ibex) (extracted from another JS13K game! xD).\n\n\u003e You can't leave this article before seeing [ibex example](https://gl-react-cookbook.surge.sh/ibex)! I'm serious, this is probably the most accomplished code I ever wrote! xD\n\n## \u003ca name=\"under_the_hook_redraw\"\u003e\u003c/a\u003e under the hood of Surface and Node redraw\n\nIn order to make redraw efficient, `gl-react` have 2 phases: the `redraw()` phase and the `flush()` phase (reflecting the respective methods available both on `Surface` and `Node`). This is a bit like a rendering engine:\n\n- **`redraw()` phase** sets a dirty flag to a Node and all its \"dependents\" (other nodes, buses, surface). _redraws happen generally bottom-up to the Surface._\n- **`flush()` phase** draws all nodes that have the redraw flag. _draws happens top-down from the Surface._\n\n`redraw()` is directly hooked to React update lifecycle (re-rendering a Node will calls `redraw()` for you).\nTo make this system efficient, **the flush() is by default asynchronous**, i.e. `redraw()` means scheduling a new gl draw.\nSurface have a main loop that runs at 60 fps and call `flush()`. This is very efficient because if Surface does not have the redraw flag, `flush()` does nothing.\n\n\u003e In gl-react inspector, clicking on the redraw count will call `redraw()` on the node / bus. We can illustrate that only \"dependents\" get redrawn using the advanced [blurimgtitle example](https://gl-react-cookbook.surge.sh/blurimgtitle):\n\n[only \"dependents\" get redrawn](https://gl-react-cookbook.surge.sh/blurimgtitle)\n\n\u003cvideo loop autoplay controls src=\"/images/2016/12/blurimgtitle-redraw.mp4\" width=\"100%\"\u003e\u003c/video\u003e\n\nThis redraw/flush phases allow to prevent and skip rendering multiple times a Node. In some cases, we still want to redraw synchronously: with `\u003cNode/\u003e` `sync` prop. For instance, in Game of Life, we don't want to skip an update (the initial update set the initial GoL state, if it was async it might get skipped).\n\n## Bonus\n\n### Flow types\n\nFlow types has been used for more robust code and better user experience. BTW, [WebGLRenderingContext will soon be released in flow](https://github.com/facebook/flow/pull/2764).\n\n### Atom highlighting\n\nIf you are using Atom Editor, you can have JS inlined GLSL syntax highlighted.\n\n![](https://cloud.githubusercontent.com/assets/211411/20623048/0527cce2-b306-11e6-85ee-5020be994c10.png)\n\n_To configure this:_\n\n- add `language-babel` package.\n- Configure `language-babel` to add `GLSL:source.glsl` in settings \"_JavaScript Tagged Template Literal Grammar Extensions_\".\n- (Bonus) Add this CSS to your _Atom \u003e Stylesheet_:\n\n```css\n/* language-babel blocks */\natom-text-editor::shadow .line .ttl-grammar {\n  /* NB: designed for dark theme. can be customized */\n  background-color: rgba(0, 0, 0, 0.3);\n}\natom-text-editor::shadow .line .ttl-grammar:first-child:last-child {\n  display: block; /* force background to take full width only if ttl-grammar is alone in the line. */\n}\n```\n\n### Tests: almost 100% coverage!\n\nThe library is tested directly on the command line, thanks to [Jest][jest] and [headless-gl][headless-gl] _(Big up to [mikolalysenko](https://github.com/mikolalysenko) for [headless-gl][headless-gl]!)_.\n**gl-react have 2000 line of tests, involving a lot of gl calls, and readPixels, and it runs... in a few seconds!** _(to Jest devs: you are wizards!)_\n\n```\n\n PASS  ./all.test.js\n  ‚úì renders a red shader (75ms)\n  ‚úì renders HelloGL (15ms)\n  ‚úì ndarray texture (27ms)\n  ‚úì renders a color uniform (18ms)\n  ‚úì composes color uniform with LinearCopy (21ms)\n  ‚úì no needs to flush if use of sync (24ms)\n  ‚úì Node can have a different size and be scaled up (18ms)\n  ‚úì Surface can be resized (32ms)\n  ‚úì bus uniform code style (17ms)\n  ‚úì bus example 1 (17ms)\n  ‚úì bus example 2 (18ms)\n  ‚úì bus example 3 (17ms)\n  ‚úì bus example 4 (22ms)\n  ‚úì bus example 5 (14ms)\n  ‚úì bus example 6 (24ms)\n  ‚úì bus: same texture used in multiple sampler2D is fine (14ms)\n  ‚úì a surface can be captured and resized (16ms)\n  ‚úì a node can be captured and resized (17ms)\n  ‚úì Uniform children redraw=\u003eel function (22ms)\n  ‚úì Bus redraw=\u003eel function (16ms)\n  ‚úì many Surface updates don't result of many redraws (18ms)\n  ‚úì many Surface flush() don't result of extra redraws (10ms)\n  ‚úì GL Components that implement shouldComponentUpdate shortcut Surface redraws (27ms)\n  ‚úì nested GL Component update will re-draw the Surface (24ms)\n  ‚úì Node `clear` and discard; (24ms)\n  ‚úì Node `backbuffering` (32ms)\n  ‚úì Node `backbuffering` in `sync` (36ms)\n  ‚úì texture can be null (12ms)\n  ‚úì array of textures (22ms)\n  ‚úì Node uniformsOptions texture interpolation (17ms)\n  ‚úì can be extended with addTextureLoaderClass (70ms)\n  ‚úì Surface `preload` prevent to draw anything (59ms)\n  ‚úì Surface `preload` that fails will trigger onLoadError (59ms)\n  ‚úì renders a shader inline in the Node (15ms)\n  ‚úì testing connectSize() feature (17ms)\n  ‚úì handle context lost nicely (43ms)\n  ‚úì Bus#uniform and Bus#index (25ms)\n  ‚úì VisitorLogger + bunch of funky extreme tests (140ms)\n\n-------------------------------|----------|----------|----------|----------|----------------|\nFile                           |  % Stmts | % Branch |  % Funcs |  % Lines |Uncovered Lines |\n-------------------------------|----------|----------|----------|----------|----------------|\nAll files                      |    97.85 |     88.8 |    95.96 |    99.35 |                |\n src                           |    97.82 |    88.95 |     95.9 |    99.35 |                |\n  Backbuffer.js                |      100 |      100 |      100 |      100 |                |\n  Bus.js                       |    96.15 |    74.29 |      100 |      100 |                |\n  GLSL.js                      |      100 |       50 |      100 |      100 |                |\n  LinearCopy.js                |      100 |      100 |      100 |      100 |                |\n  NearestCopy.js               |      100 |      100 |      100 |      100 |                |\n  Node.js                      |    97.65 |    92.71 |    97.01 |    99.01 |    214,216,358 |\n  Shaders.js                   |      100 |    76.92 |      100 |      100 |                |\n  Texture2DLoader.js           |      100 |      100 |      100 |      100 |                |\n  TextureLoader.js             |      100 |      100 |      100 |      100 |                |\n  TextureLoaderNDArray.js      |      100 |      100 |      100 |      100 |                |\n  TextureLoaders.js            |      100 |      100 |      100 |      100 |                |\n  Visitor.js                   |      100 |      100 |       75 |      100 |                |\n  VisitorLogger.js             |      100 |    92.59 |      100 |      100 |                |\n  Visitors.js                  |      100 |      100 |      100 |      100 |                |\n  connectSize.js               |      100 |    85.71 |      100 |      100 |                |\n  copyShader.js                |      100 |      100 |      100 |      100 |                |\n  createSurface.js             |    97.09 |    83.61 |    94.55 |    99.32 |            361 |\n  genId.js                     |      100 |      100 |      100 |      100 |                |\n  index.js                     |      100 |      100 |      100 |      100 |                |\n src/helpers                   |      100 |       75 |      100 |      100 |                |\n  disposable.js                |      100 |       50 |      100 |      100 |                |\n  invariantNoDependentsLoop.js |      100 |      100 |      100 |      100 |                |\n-------------------------------|----------|----------|----------|----------|----------------|\nTest Suites: 1 passed, 1 total\nTests:       38 passed, 38 total\nSnapshots:   4 passed, 4 total\nTime:        2.655s\nRan all test suites.\n```\n\nOne limitation is that all tests need to be in a single file. [I created an issue here](https://github.com/facebook/jest/issues/2029). I think it's either an issue in [Jest][jest] or in [headless-gl][headless-gl].\n\n## Tradeoffs and remaining work\n\nThe library tradeoffs are written in [TRADEOFFS.md](https://github.com/gre/gl-react/blob/master/TRADEOFFS.md). We might cover some unexplored direction in a near future and solve some of them.\n\nv3 is still in development, the main unfinished part is the React Native implementation which is now the main priority of the library.\nIt will probably rely on an awesome initiative: a React Native WebGL implementation started in Exponent by [@nikki](https://github.com/nikki93)!\n\nFor more information, see [v3 alpha: development in progress](https://github.com/gre/gl-react#v3-alpha-development-in-progress).\n","data":{"title":"gl-react v3","thumbnail":"/images/2016/12/thumbnail.png","description":"gl-react has been reimplemented from scratch, feedback from previous mistake and overview of new features.","author":"Gaetan","layout":"post","tags":["react","gl-react","webgl"]}},{"id":"2016-09-19-relay-scrolling-connections","year":"2016","month":"09","day":"19","slug":"relay-scrolling-connections","content":"\n[Relay]: https://github.com/facebook/relay\n[Relay-spec]: https://facebook.github.io/relay/docs/graphql-relay-specification.html#content\n\n\n[Relay][Relay] doesn't solve for you how you should render your components. Relay is \"universal\" and doesn't even assume it will be running in a browser context. It focuses only on providing an abstraction to work with GraphQL ‚Äì the same way React focuses only on rendering. Each library solves one single problem at a time *(and hell, both are complex enough problem to solve already)*.\n\nBecause these libraries are very generic, it's now up to the community to solve the \"more specific\" parts. Just search on NPM and you can find tons of React libraries already, some might help you to solve part of the problem you want to solve.\n\nThis article demonstrates one use-case: **implementing a component handling the scroll of a list to pull more data** of a GraphQL connection with Relay.\n\n\u003c!--more--\u003e\n\n## Usage\n\nIn React you should think in term of components that subdivide individual task to solve. To solve scrolling a connection you should just need this:\n\n```js\n\u003cInfiniteScrollable relay={relay}\u003e\n  ...\n\u003c/InfiniteScrollable\u003e\n```\n\nHere is a real use-case we have at [projectseptember](https://projectseptember.com).\n\n\n```js\nimport React, {\n  Component,\n  PropTypes,\n} from \"react\";\nimport Relay from \"react-relay\";\nimport List from \"material-ui/List\";\nimport Content from \"./Content\";\n\nclass ContentStream extends Component {\n  static propTypes = {\n    relay: PropTypes.object.isRequired,\n    user: PropTypes.object,\n  };\n  render () {\n    const { user, relay } = this.props;\n    return (\n      \u003cInfiniteScrollable relay={relay}\u003e\n        \u003cList\u003e\n          {user.stream.edges.map(e =\u003e\n            \u003cContent content={e.node} key={e.cursor} /\u003e\n          )}\n        \u003c/List\u003e\n      \u003c/InfiniteScrollable\u003e\n    );\n  }\n}\n\nexport default Relay.createContainer(ContentStream, {\n  initialVariables: {\n    first: 50,\n  },\n  fragments: {\n    user: () =\u003e Relay.QL`\nfragment on User {\n  stream (first:$first) {\n    edges {\n      cursor\n      node {\n        ${Content.getFragment(\"content\")}\n      }\n    }\n  }\n}\n    `\n  }\n});\n```\n\nWe don't have to express how to \"pull for more data\" in that code. In fact, this is delegated to `InfiniteScrollable` and we never have to think again about it.\n\n\n## InfiniteScrollable implementation\n\nRelay enforces to implement [a subset of GraphQL spec](https://facebook.github.io/relay/docs/graphql-relay-specification.html#content), like the Connection API. It's a good thing because we can also rely on this fact, and what we only need is the `relay` object to implement a generic pull-on-scroll.\n\n\n```js\nimport {\n  Component,\n  PropTypes,\n} from \"react\";\nimport {findDOMNode} from \"react-dom\";\n\nconst regex = /(auto|scroll)/;\n\nconst style = (node, prop) =\u003e\n  getComputedStyle(node, null).getPropertyValue(prop);\n\nconst scroll = (node) =\u003e\n  regex.test(\n    style(node, \"overflow\") +\n    style(node, \"overflow-y\") +\n    style(node, \"overflow-x\"));\n\nconst scrollparent = (node) =\u003e\n  !node || node===document.body\n  ? document.body\n  : scroll(node)\n    ? node\n    : scrollparent(node.parentNode);\n\nconst resizeEventOn = n =\u003e n===document.body ? window : n;\n\nexport default class InfiniteScrollable extends Component {\n  static propTypes = {\n    children: PropTypes.any.isRequired,\n    relay: PropTypes.object,\n    style: PropTypes.object,\n    loadPixelsInAdvance: PropTypes.number,\n    relayVariable: PropTypes.string,\n    chunkSize: PropTypes.number,\n    // loadMore could even be generalize, this component works if you provide loadMore instead of relay\n    loadMore: PropTypes.func, // (can) returns a promise\n  };\n  static defaultProps = {\n    loadPixelsInAdvance: 1000,\n    relayVariable: \"first\",\n    chunkSize: 50,\n  };\n\n  state = { loading: false };\n\n  resizeBoundOnDom = null;\n\n  componentDidMount () {\n    this.syncScrollBodyListener(this.props);\n    this.checkScroll();\n  }\n\n  componentWillUnmount () {\n    this.unbindResizeEvent();\n  }\n\n  componentDidUpdate () {\n    this.syncScrollBodyListener();\n  }\n\n  unbindResizeEvent () {\n    if (this.resizeBoundOnDom) {\n      this.resizeBoundOnDom.removeEventListener(\"scroll\", this.checkScroll);\n      this.resizeBoundOnDom = null;\n    }\n  }\n\n  getScrollParent () {\n    return scrollparent(findDOMNode(this));\n  }\n\n  syncScrollBodyListener = () =\u003e {\n    const resizeBoundOnDom = resizeEventOn(this.getScrollParent());\n    if (resizeBoundOnDom !== this.resizeBoundOnDom) {\n      this.unbindResizeEvent();\n      resizeBoundOnDom.addEventListener(\"scroll\", this.checkScroll);\n    }\n  };\n\n  loadMoreUsingRelay = () =\u003e {\n    const { relay, relayVariable, chunkSize } = this.props;\n    return new Promise((resolve, reject) =\u003e\n     relay.setVariables({\n       [relayVariable]: relay.variables[relayVariable] + chunkSize\n     }, readyState =\u003e {\n       if (readyState.error) reject(readyState.error);\n       if (readyState.done) resolve();\n     }));\n  };\n\n  checkScroll = () =\u003e {\n    if (this.state.loading) return;\n    const container = this.getScrollParent();\n    if (!container) return;\n    const { height } = container.getBoundingClientRect();\n    const { scrollHeight, scrollTop } = container;\n    const bottom = scrollTop + height;\n    const { loadPixelsInAdvance } = this.props;\n    const advance = bottom - scrollHeight + loadPixelsInAdvance;\n    if (advance \u003e 0) {\n      this.setState({ loading: true }, () =\u003e\n        Promise.resolve({ advance, bottom, scrollHeight, height, scrollTop, loadPixelsInAdvance })\n        .then(this.props.loadMore || this.loadMoreUsingRelay)\n        .then(\n          () =\u003e this.setState({ loading: false }), // technically could recall checkScroll here. in second callback of setState. fork it, try it, adapt it !\n          e =\u003e (console.warn(e), this.setState({ loading: false }))\n        ));\n    }\n  };\n\n  render () {\n    // you might want to render a spinner?\n    // children might be a function?\n    // etc..\n    // these are some variations we could have from this starting point\n    return this.props.children;\n  }\n}\n```\n\n\nThis is a **possible implementation** of this problem. You might want to add more things based on your needs. For instance you could automatically render a loading spinner... or a million other things! Please try it, fork it, give feedback :)\n\nIt is also possible to implement it as a High Order Component (HOC): [https://github.com/facebook/relay/issues/1377](https://github.com/facebook/relay/issues/1377).\n","data":{"title":"Relay, scrolling connections","description":"implement a component handling the scroll of a list to pull more data of a Graphql Connection with Relay","author":"Gaetan","layout":"post","tags":["react","relay"]}},{"id":"2016-07-01-projectseptember-opengl","year":"2016","month":"07","day":"01","slug":"projectseptember-opengl","content":"\n[twitter]: https://twitter.com/ProjSeptEng\n[website]: https://projectseptember.com\n[rn]: http://facebook.github.io/react-native/\n[graphql]: http://graphql.org/\n[scala]: http://scala-lang.org/\n[glreactconf]: /2016/06/glreactconf\n[glreact]: https://github.com/ProjectSeptemberInc/gl-react\n[glreactdom]: https://github.com/ProjectSeptemberInc/gl-react-dom\n[glreactnative]: https://github.com/ProjectSeptemberInc/gl-react-native\n[rnanimation]: https://facebook.github.io/react-native/docs/animations.html\n\nüéâ Hooray! [We][twitter] recently released an iOS app called [Project September][website].\n\nThis application is built with nice tech stack including [React Native][rn] and [GraphQL][graphql]. The backend is powered by [Scala][scala], a robust functional language, and we use many other [cool techs][twitter].\n\nThis fashion app needed some fancy features: one was demo-ed at last [React.js conference][glreactconf] with the ability to do localized blur on text over images.\n\nWe have developed **[`gl-react`][glreact]** to abstract GL in React paradigm ‚Äì with two companion libraries [`gl-react-dom`][glreactdom] and [`gl-react-native`][glreactnative] that glues React Native with OpenGL.\n\nLet's first see 2 demos of OpenGL usage in our app, and then we'll write a bit about how it's hard to get animations right.\n\n\u003c!--more--\u003e\n\n## The Text Over Image blur\n\n### The goal\n\n\u003cimg width=\"50%\" src=\"/images/2016/07/current-1.png\" /\u003e\u003cimg width=\"50%\" src=\"/images/2016/07/current-2.png\" /\u003e\n\n### How it works\n\n\u003cimg src=\"/images/2016/07/initial.png\" /\u003e\n\n**+** **_(layer)_**\n\n\u003cimg src=\"/images/2016/07/layer.png\" /\u003e\n\n**=**\n\n\u003cimg src=\"/images/2016/07/result.png\" /\u003e\n\n### Under the hood\n\n- The shadow intensity, size, position, is procedurally generated, we can adjust that. The shadow color is the blurry image color\n- The text color is determined by the color picked in blurred image at the shadow middle position. **If the `monochrome` value of that color is lower than 60%**, text will be white, otherwise text will be black.\n\nHere is more detail on how the shadow is generated:\n\n\u003cimg src=\"/images/2016/07/under-1.png\" /\u003e\n\n**\\* (multiply alpha)**\n\n\u003cimg src=\"/images/2016/07/under-2.png\" /\u003e\n\n**=**\n\n\u003cimg src=\"/images/2016/07/under-3.png\" /\u003e\n\n**+ (layer)**\n\n\u003cimg src=\"/images/2016/07/under-4.png\" /\u003e\n\n**=**\n\n\u003cimg src=\"/images/2016/07/layer.png\" /\u003e\n\n### Fragment shader\n\n```glsl\nprecision highp float;\nvarying vec2 uv;\n\nuniform sampler2D img;\nuniform sampler2D imgBlurred;\nuniform sampler2D txt;\n\nconst vec2 shadowCenter = vec2(0.5, 0.9);\nconst vec2 shadowSize = vec2(0.6, 0.2);\nfloat shadow () {\n  return 0.8 * smoothstep(1.0, 0.2, distance(uv / shadowSize, shadowCenter / shadowSize));\n}\nfloat monochrome (vec3 c) {\n  return 0.2125 * c.r + 0.7154 * c.g + 0.0721 * c.b;\n}\nvec3 textColor (vec3 bg) {\n  return vec3(step(monochrome(bg), 0.6));\n}\n\nvoid main () {\n  vec4 bg = mix(texture2D(img, uv), texture2D(imgBlurred, uv), shadow());\n  vec4 fg = vec4(textColor(texture2D(imgBlurred, shadowCenter).rgb), 1.0);\n  float fgFactor = 1.0 - texture2D(txt, uv).r;\n  gl_FragColor = mix(bg, fg, fgFactor);\n}\n```\n\n### Integration\n\n```html\n\u003cGL.Node shader=\"{shaders.textOverImage}\"\u003e\n  \u003cGL.Uniform name=\"img\"\u003e {img} \u003c/GL.Uniform\u003e\n  \u003cGL.Uniform name=\"imgBlurred\"\u003e\n    \u003cBlur factor=\"{20}\" passes=\"{6}\" width=\"{width}\" height=\"{height}\"\u003e\n      {img}\n    \u003c/Blur\u003e\n  \u003c/GL.Uniform\u003e\n  \u003cGL.Uniform name=\"txt\"\u003e\n    \u003cText style=\"{titleStyle}\"\u003e{title}\u003c/Text\u003e\n  \u003c/GL.Uniform\u003e\n\u003c/GL.Node\u003e\n```\n\n## Uploading Thumbnail\n\nThis is a video record of our app:\n\n\u003cvideo controls autoplay muted loop width=\"100%\" src=\"/images/2016/07/upload.mp4\"\u003e\u003c/video\u003e\n\nThe uploading spinner effect is implemented with an OpenGL shader. This was not easy to avoid all the blinks we used to have. We have different components to render each step (uploading animation / uploaded final image) and the uploaded image needs to be downloaded again to not render as white. One solution could be to use a monolithic \"thumbnail\" component that do everything. We wanted to keep independent components.\nHopefully, everything now works seamlessly with some \"double buffering\"/swapping mechanism we will explained at the end of this article.\n\n## Animate all the things\n\n### Designing animations\n\n\u003e Fluid, meaningful animations are essential to the mobile user experience.\n\u003e **[‚Äî React Native Animations documentation][rnanimation]**\n\nIt's not easy to design how an application should animate, to define transitions between all the different possible single state and edge-cases of your app. Designing animations, as part of UX design, is a time consuming work but it tends to be underestimated while being essential for moving from a _good app_ to a _very good app_. That tends to be the last 20% remaining missing parts of your app that are the hardest but that makes the 80% of a great UX.\n\n### Implementing animations\n\nNot only it's hard to have figured out the animations (to find the optimal UX) but it can also be quite challenging to implement them in a maintainable and robust way. Turns out most of the times, your code is not ready for it and it implies big refactoring.\n\n#### in React Native\n\nReact Native [Animations API][rnanimation] makes it easier: you just have to switch to one of the `Animated.*` component. In `gl-react` we even support Animated values to flow into the shaders uniforms so it's very convenient to animate a GL effect.\n\nThat said, React Native Animations is not the ultimate silver bullet. There are things Animations won't solve for you. React Native Animated is still a low level API, it's also imperative and not opinionated on how you should turn it into descriptive paradigm.\n\nI guess what's generally hard with animations in React functional/descriptive paradigm (\"always `render()`ing Virtual DOM again\" idea) is to figure out **how to not \"break\" your animations**. For instance, ugly animation interruption could happen if you `render()` a different component: because it forces the component to unmount. If you have an animation happening, you might not want it to stop, or at least you might want to smoothly customize the transition to the new state.\n\nThat's something CSS transitions might help solving, but in React Native we don't have them, so it's not so trivial.\n\n##### our current solution\n\nWe have built our own abstraction to solve this problem: a Component decorator manages to kill a lot of flashes and blinks cases (e.g. images not ready yet, animation getting interrupted).\n\n\u003e What the decoration solves: when moving from A to B, you want B to be ready (e.g. images are loaded), you also want A to have finish its (animated) work.\n\n**A component can express it needs some time to mount _(e.g. an image to load!)_ OR that it needs some time to unmount _(e.g. an \"animating out\")_. This will basically hold the rendering to happen:**\n\nThe decoration can implement \"double buffering\" on a Component: `render()` function keeps rendering Component with the previous \"stable props\" but will also render in background another instance of Component with the next props. When that next props Component is ready and loaded, we can successful swap it to be the new \"stable props\".\n\nYou have the basic idea, the decorator is not so trivial to implement as it also needs to handle some edge-cases, for instance if the decorator receives new props during the transition. We also have a minimal way to express \"styles transitions\" similarly to how CSS Transitions works.\n","data":{"title":"üéâ There are some OpenGL in the Project September fashion app!","author":"Gaetan","layout":"post","tags":["react","opengl","gl-react"]}},{"id":"2016-06-19-glreactconf","year":"2016","month":"06","day":"19","slug":"glreactconf","content":"\n[gl-react]: https://github.com/projectseptemberinc/gl-react\n[gl-react-dom]: https://github.com/projectseptemberinc/gl-react-dom\n[gl-react-native]: https://github.com/projectseptemberinc/gl-react-native\n[reconciliation]: https://facebook.github.io/react/docs/reconciliation.html\n[glsl-spec]: https://www.khronos.org/registry/gles/specs/2.0/GLSL_ES_Specification_1.0.17.pdf\n[gl-react-blur]: https://github.com/gre/gl-react-blur\n[gl-react-negative]: https://github.com/gre/gl-react-negative\n[gl-react-constrast-saturation-brightness]: https://github.com/gre/gl-react-constrast-saturation-brightness\n[gl-react-hue-rotate]: https://github.com/gre/gl-react-hue-rotate\n[gl-react-color-matrix]: https://github.com/gre/gl-react-color-matrix\n\n\u003cscript src=\"http://localhost:35729/\"\u003e\u003c/script\u003e\n\nLast February, I talked about [`gl-react`][gl-react] at React.js conference.\n\n\u003ciframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Xnqy_zkBAew\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\u003e [Checkout talks](https://www.youtube.com/playlist?list=PLb0IAmt7-GS0M8Q95RIc2lOM6nc77q1IY) of this conference if you are interested by React subject.\n\u003e I want to thanks the incredible team behind React.js for the awesome conference and giving me the opportunity to come to San Francisco.\n\n_This article will cover some more technical detail of [`gl-react`][gl-react] that wasn't explained in the talk._\n\n\u003e _Also, I'll try to not go TOO MUCH into technical detail neither, because it would take weeks to cover gl-react features and its implementation tricks!_\n\n\u003c!--more--\u003e\n\n---\n\nWe have developed, at [Project September](http://projectseptember.com), an [universal](https://medium.com/@mjackson/universal-javascript-4761051b7ae9) OpenGL wrapper for React called [`gl-react`][gl-react] working with 2 other libraries: [`gl-react-dom`][gl-react-dom] (wraps WebGL) and [`gl-react-native`][gl-react-native] (wraps OpenGL).\n\nThe library allows to define **advanced effects** on top of **images, videos, texts or any other VDOM Content** (like UI Views).\n\nCheckout the following `gl-react` demos running with the **same codebase across iOS, Android and the Web !!!**\n\n## [**AdvancedEffects**](http://projectseptemberinc.github.io/gl-react-dom/Examples/AdvancedEffects/)\n\n\u003cvideo src=\"/images/2016/03/advanced-effects.mp4\" width=\"100%\" controls autoplay muted loop\u003e\u003c/video\u003e\n\n## [**github.com/gre/gl-react-image-effects**](https://github.com/gre/gl-react-image-effects)\n\n\u003ca href=\"http://greweb.me/gl-react-image-effects/\" style=\"text-align:center\"\u003e\u003cvideo loop autoplay controls src=\"/images/2016/03/image-effects-ios.mp4\" width=\"33%\"\u003e\u003c/video\u003e\u003cimg src=\"/images/2016/03/image-effects-web.png\" style=\"width: 33%;\" /\u003e\u003cimg src=\"/images/2016/03/image-effects-android.png\" style=\"width: 33%\" /\u003e\u003c/a\u003e\n\n## Adopting React paradigm\n\nThere are a few important points in React that `gl-react` follows to fit its paradigm.\n\n### 1. React is about composition\n\n**Components are first class citizen** in React.\nA Component provides isolation by exposing a simple API (props) and encapsulates internal lifecycle.\nOne particular prop is the `children` prop that, by convention, allows to pass-in children components to a component.\n\nFor instance, we can define a component called `Container` taking a `children` prop and use it like this in JSX:\n\n```html\n\u003cContainer\u003e\n  \u003cdiv\u003eHello\u003c/div\u003e\n\u003c/Container\u003e\n```\n\n\u003e Note that \"JSX\" is a syntax sugar that, in this example, transpiles to something like:\n\n```js\nReact.createElement(Container, {\n  children: React.createElement(\"div\", { children: \"Hello\" }),\n});\n```\n\n\u003e There are really no magic here: JSX just tends to be more convenient to write and, most of the time, it would be more annoying to use createElement API.\n\n### 2. React is about Functional Programming...\n\nThis might not be that obvious at a first glance but React is about FP. In React paradigm, you describe the full rendering of your application _(in \"Virtual DOM\")_ for a given application state, and you do this every time something changes in the application. So fundamentally, you just have to implement that **function from State to Virtual DOM** to implement your web application ‚Äì _which is close to render loops in game dev (but it's another subject^^)._\n\n### ...and reconciliation on top of an underlying Imperative API\n\nIn React, you don't go mutating the DOM but you just RENDER EVERYTHING everytime! It's very simple to reason about, it removes lot of inconsistency bugs and React is here to optimize this with [an algorithm called \"reconciliation\"][reconciliation] (or diff/patch).\n\n\u003e Efficiently translating an high level immutable API into a lower level mutable API is the hard work that solves React on top of DOM, as well does gl-react on top of OpenGL.\n\nThe [reconciliation][reconciliation] work done by React is a complex optimization problem that have trade-offs. This work is all about **translating a imperative API (the DOM) to a functional API (React VDOM)**.\n\nIn [`gl-react`][gl-react], we have the **exact same problem to resolve**: [`gl-react`][gl-react] exposes a **functional API** and implements for you the complex work over the mutable, stateful and low-level API that is OpenGL / WebGL.\n\n### 3. React is a thin wrapper\n\nOne other key point of React is that it's a **thin wrapper on top of DOM**. React focus on translating **imperative API =\u003e functional API** with the most minimal \u0026 generic way, meaning that React won't hide you what the DOM elements are about _(same as React Native tries just to be on top of real Native components)_.\n\nWe tried to follow this principle as well when wrapping (Open/Web)**GL**:\nwe want to hide the complex and imperative part of GL but just expose the great functional parts that are the **Fragment Shaders** and the **Framebuffers**.\n\n## Hardcoding the vertex part of the pipeline\n\n![](/images/2016/03/hardcoded_pipeline.png)\n\n`gl-react` _currently_ focuses on what can be achieved with **composing Fragment Shaders** with multiple **Framebuffers** that define a **graph of effects**. The Vertex Data \u0026 Vertex Shaders are currently hardcoded.\n\n\u003e `gl-react` might unlock this hardcoded part in the future ‚Äì [some recent experiments by @snikhilesh](https://twitter.com/snikhilesh/status/707730742994833408) shows a promising overview of what can possibility by done there!\n\nBut for now, we focus on the huge challenge to implement `gl-react` seamlessly between the Web, Android and iOS implementations and to work on performance _(e.g of the content rasterization performance)_.\n\n\u003cblockquote class=\"twitter-tweet\" data-lang=\"fr\"\u003e\u003cp lang=\"en\" dir=\"ltr\"\u003eNotes from \u003ca href=\"https://twitter.com/greweb\"\u003e@greweb\u003c/a\u003e\u0026#39;s \u003ca href=\"https://twitter.com/hashtag/reactjsconf?src=hash\"\u003e#reactjsconf\u003c/a\u003e talk Universal GL Effects for Web and Native \u003ca href=\"https://t.co/JnIOjAQCOK\"\u003epic.twitter.com/JnIOjAQCOK\u003c/a\u003e\u003c/p\u003e\u0026mdash; Michael Chan (@chantastic) \u003ca href=\"https://twitter.com/chantastic/status/702213897520943104\"\u003e23 f√©vrier 2016\u003c/a\u003e\u003c/blockquote\u003e\n\u003cscript async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"\u003e\u003c/script\u003e\n\n## _\"Functional Rendering\"_\n\nWe mentioned earlier that `gl-react` focuses on one important piece of OpenGL that is the fragment shader.\n\n**A Fragment Shader is a function** that independently colors each pixel:\n\n![](/images/2016/03/functional_rendering.png)\n\n[Watch this talk](http://greweb.me/2013/11/functional-rendering/) for more detail on what I call _Functional Rendering_.\n\n\u003e Fragment shaders use a language called GLSL for _OpenGL Shading Language_. GLSL is a DSL dedicated to the functional rendering paradigm with \"graphics-ready\" types (vectors, matrix, sampler2D for textures) and built-in functions (like mix, distance, pow, cos,...).\n\u003e Checkout the [specification][glsl-spec].\n\n## GL*uing* with React\n\n**[`gl-react`][gl-react] is a core library**: it doesn't provide any built-in effects, users have to provide the shaders to render. Hopefully it's fairly simple to implement basic effects _(like saturation, contrast, brightness, inverse, hue,...)_ in GLSL language and _Functional Rendering_ paradigm.\n\n### HelloGL example\n\nLet's create HelloGL, our first fragment shader:\n\n```glsl\nprecision highp float;\nvarying vec2 uv; // This variable vary in all pixel position (normalized from vec2(0.0,0.0) to vec2(1.0,1.0))\nvoid main () { // This function is called FOR EACH PIXEL\n  gl_FragColor = vec4(uv.x, uv.y, 0.5, 1.0); // red vary over X, green vary over Y, blue is 50%, alpha is 100%.\n}\n```\n\nIt's a _Point to Color_ function:\n\n- **the input comes from `varying vec2 uv`**\n- **the output is set in `vec4` `gl_FragColor`** ‚Äì `main()` is called for each pixel with a different `uv` (it's _varying_ like the keyword indicates).\n\nso this HelloGL glsl code basically do:\n\n```js\n[ x, y ] =\u003e [ x, y, 0.5, 1.0 ]\n```\n\n- The **RED** component increases with the X position of the pixel.\n- The **GREEN** component increases with the Y position of the pixel.\n\nwhich renders this nice 2D gradient:\n\n\u003cimg width=\"160\" src=\"/images/2016/03/hellogl.png\" /\u003e\n\nNow, in `gl-react`, we can define \"HelloGL\" as a GL Component with:\n\n```js\nimport GL from \"gl-react\";\nimport React from \"react\";\nconst shaders = GL.Shaders.create({\n  helloGL: {\n    frag: `\nprecision highp float;\nvarying vec2 uv;\nvoid main () {\n  gl_FragColor = vec4(uv.x, uv.y, 0.5, 1.0);\n}`,\n  },\n});\nconst HelloGL = GL.createComponent(() =\u003e \u003cGL.Node shader={shaders.helloGL} /\u003e);\n```\n\nand then use it:\n\n```html\n\u003cHelloGL /\u003e\n```\n\n### ColoredDisc example\n\nGL Component can have props in parameter that can be passed-in as GLSL Uniforms.\n\n\u003cimg class=\"thumbnail-right\" src=\"/images/2016/03/colored-disc.png\" /\u003e\n\n```html\n\u003cColoredDisc fromColor=\"{[\" 1, 0, 1 ]} toColor=\"{[\" 1, 1, 0 ]} /\u003e\n```\n\n\u003cbr /\u003e\n\n```js\nimport GL from \"gl-react\";\nimport React from \"react\";\nconst shaders = GL.Shaders.create({\n  ColoredDisc: {\n    frag: `\nprecision highp float;\nvarying vec2 uv;\nuniform vec3 fromColor;\nuniform vec3 toColor;\nvoid main () {\n  float d = 2.0 * distance(uv, vec2(0.5));\n  gl_FragColor = mix(\n    vec4(mix(fromColor, toColor, d), 1.0),\n    vec4(0.0),\n    step(1.0, d)\n  );\n}`,\n  },\n});\nconst ColoredDisc = GL.createComponent(({ fromColor, toColor }) =\u003e (\n  \u003cGL.Node shader={shaders.ColoredDisc} uniforms={{ fromColor, toColor }} /\u003e\n));\n```\n\n### DiamondCrop example\n\n\u003cimg class=\"thumbnail-right\" src=\"/images/2016/03/diamond-crop.png\" /\u003e\n\n```html\n\u003cDiamondCrop\u003e http://i.imgur.com/rkiglmm.jpg \u003c/DiamondCrop\u003e\n```\n\n\u003cbr /\u003e\n\n```js\nimport GL from \"gl-react\";\nimport React from \"react\";\nconst shaders = GL.Shaders.create({\n  DiamondCrop: {\n    frag: `\nprecision highp float;\nvarying vec2 uv;\nuniform sampler2D t;\nvoid main () {\n  gl_FragColor = mix(\n    texture2D(t, uv),\n    vec4(0.0),\n    step(0.5, abs(uv.x - 0.5) + abs(uv.y - 0.5))\n  );\n}`,\n  },\n});\nconst DiamondCrop = GL.createComponent(({ children: t }) =\u003e (\n  \u003cGL.Node shader={shaders.DiamondCrop} uniforms={{ t }} /\u003e\n));\n```\n\n## Any content can be used\n\nLet's say we define a Blur effect with `gl-react`.\n\n```js\nconst Blur = GL.createComponent(({ children, factor }) =\u003e ...);\n```\n\nHere, we have just defined a GL Component `Blur` that accept a children as a props.\nIt also accept a factor prop to define the intensity of that blur.\nTherefore we can use `Blur` using JSX in many ways.\n\n\u003e **N.B.** If you want such a Blur, checkout [gl-react-blur](https://github.com/gre/gl-react-blur).\n\nFirst of all you can blur an image:\n\n\u003cimg class=\"thumbnail-right\" src=\"/images/2016/03/blur_image.png\" /\u003e\n\n```html\n\u003cBlur factor=\"{2}\"\u003e http://i.imgur.com/rkiglmm.jpg \u003c/Blur\u003e\n```\n\n\u003cbr /\u003e\n\nBut really anything can be passed-in here. For instance, a video\n\n```html\n\u003cBlur factor=\"{0.6}\"\u003e\n  \u003cvideo src=\"/video.mpg\" /\u003e\n\u003c/Blur\u003e\n```\n\nor a canvas:\n\n```html\n\u003cBlur factor=\"{0.7}\"\u003e\n  \u003ccanvas ... /\u003e\n\u003c/Blur\u003e\n```\n\nand where that canvas can be provided by a library, like react-canvas:\n\n```html\n\u003cBlur factor=\"{0.9}\"\u003e\n  \u003cReactCanvas.Surface ...\u003e\n    \u003cReactCanvas.Text ...\u003eHello World\u003c/ReactCanvas.Text\u003e\n  \u003c/ReactCanvas.Surface\u003e\n\u003c/Blur\u003e\n```\n\nIn React Native context, we even have support for ANY view.\nIt can be a simple Text:\n\n\u003cimg class=\"thumbnail-right\" src=\"/images/2016/03/text_blur.png\" /\u003e\n\n```html\n\u003cBlur factor=\"{0.9}\"\u003e\n  \u003cText ...\u003eHello World\u003c/Text\u003e\n\u003c/Blur\u003e\n```\n\n\u003cbr /\u003e\n\nor even a native component like a Switch component\n\n\u003cimg class=\"thumbnail-right\" src=\"/images/2016/03/switch_blur.png\" /\u003e\n\n```html\n\u003cBlur factor=\"{0.9}\"\u003e\n  \u003cSwitch ... /\u003e\n\u003c/Blur\u003e\n```\n\n\u003cbr /\u003e\n\nThe way this is implemented is platform specific. For instance the Web implementation will just render the content into WebGL (so it works with images, videos, canvas, but not any arbitrary DOM element due to Web Security limitations). However, the Native implementation will be able to **rasterize** (almost) any view and inject it as a texture (consider this feature experimental at the moment).\n\n## Compose, Compose, Compose\n\nBut **composition** is probably the MOST important part of this:\nYou can also pass a GL Component in uniforms!\n\nSo all possible composition of previous examples will just work\n\n\u003cimg class=\"thumbnail-right\" src=\"/images/2016/03/diamond_hellogl.png\" /\u003e\n\n```html\n\u003cDiamondCrop\u003e\n  \u003cHelloGL /\u003e\n\u003c/DiamondCrop\u003e\n```\n\n\u003cbr /\u003e\n\n\u003cimg class=\"thumbnail-right\" src=\"/images/2016/03/blur_diamond_hellogl.png\" /\u003e\n\n```html\n\u003cBlur factor=\"{4}\"\u003e\n  \u003cDiamondCrop\u003e\n    \u003cHelloGL /\u003e\n  \u003c/DiamondCrop\u003e\n\u003c/Blur\u003e\n```\n\n\u003cbr /\u003e\n\n`gl-react` makes composition efficient using OpenGL Framebuffers.\nThis approach encourages you to write small and generic shaders (instead of one monolithic and specific shader).\n\n\u003e For this composition to work correctly, the components must be created with `GL.createComponent` or directly be `GL.Node` components.\n\n## \u003cSurface/\u003e\n\nTo actually get a rendering with gl-react, **you need to put your GL Component stack into a \u003cSurface/\u003e element**.\n\nFor instance, to render HelloGL on a 200x200 canvas:\n\n\u003cimg class=\"thumbnail-right\" src=\"/images/2016/03/hellogl.png\" /\u003e\n\n```html\n\u003cSurface width=\"{200}\" height=\"{200}\"\u003e\n  \u003cHelloGL /\u003e\n\u003c/Surface\u003e\n```\n\n```js\nimport { Surface } from \"gl-react-dom\";\nimport { Surface } from \"gl-react-native\";\n```\n\n\u003cbr/\u003e\n\n**Surface** implements the rendering in the contextual platform:\n\n- If you import `{Surface}` from `gl-react-dom` it will renders into a **WebGL Canvas** **_(web)_**, (it's backed by great [stack.gl](http://stack.gl/) libs)\n- If, instead, it comes from `gl-react-native`, **GLKView** **_(iOS)_** / **GLSurfaceView** **_(Android)_** will be used.\n\n**Surface** have roughly the same API across these 2 libraries but some props might exist only on one of the implementations.\n\n## Dynamic Blur Image Title Example\n\nMy talk featured an advanced use-case that we had in my startup, [Project September](http://projectseptember.com/). We are developing a social mobile app with React Native and our designer wanted to have title over image with Blur effects around the title text.\n\n[![](/images/2016/03/hellosf.jpg)](http://greweb.me/reactjsconf2016/)\n\n[Open the demo](http://greweb.me/reactjsconf2016/) ‚Äì [See the code](https://github.com/gre/reactjsconf2016)\n\nThis effect is just exposed as a simple **ImageTitle** React component that we can use like this:\n\n```html\n\u003cImageTitle text=\"Hello San Francisco ‚òª\"\u003e\n  http://i.imgur.com/XXXXXX.jpg\n\u003c/ImageTitle\u003e\n```\n\nThe point of `gl-react` is we all know how to compose React components, just put it in a **Surface** and you obtain a title over image effect like on the image above.\n\n\u003e we can even run the effect over a video\n\n```html\n\u003cImageTitle text=\"Hello San Francisco ‚òª\"\u003e\n  \u003cvideo src=\"video.mp4\" /\u003e\n\u003c/ImageTitle\u003e\n```\n\nwhich is what [our demo](http://greweb.me/reactjsconf2016/) does if you enable the video mode.\n\n## Under the hood\n\n\u003e This section will show **ImageTitle** implementation that will illustrate `gl-react` optimization techniques.\n\nLet's take a quick look at our ImageTitle shader. That shader renders the title text on top of the blurred image. The title text color is chosen based on the average pixel color (if the content is dark, we use a white title, otherwise a black one).\n\nI won't enter more into implementation detail, but here is the fragment shader:\n\n![](/images/2016/03/image-title-shader.png)\n\nNow, let's focus on our JavaScript gl-react code.\n\n**ImageTitle** is a GL Component that takes a few props (basically `title` and `children`) and delegates the job using a few other Components: **Title** that renders the text, **TitleBlurMap** that generates a blur map of that text, **BlurV** that apply the blurmap to generate a variable blur over the content (image/video), **AveragePixels** that generate one pixel out of the content.\nThese 4 elements are then composed into our final ImageTitle shader.\n\n![](/images/2016/03/image-title-imports.png)\n![](/images/2016/03/image-title-component.png)\n\nComposition is the key point here, we have defined our component with simple code, delegated and composed part of the effect with other component.\n\nAnd each sub-component is doing more work. For instance **TitleBlurMap** is itself another GL component, which uses composes a component **Blur** and apply a threshold to generate a black and white blur map:\n\n![](/images/2016/03/titleblurmap_impl.png)\n![](/images/2016/03/titleblurmap_node_detail.png)\n\nAnd so on! **Blur** is itself another GL component!\nAnd like **BlurV**, it is implementing a 4-pass blur, so it will pipe 4 times a Blur1D component:\n\n![](/images/2016/03/blurstack.png)\n\n**Blur** simply recursively composes Blur1D:\n![](/images/2016/03/blur_impl.png)\n\n\u003e Have I lost you? Don't worry, we will show in a few section what the big picture scene looks like.\n\n### \u003ca name=\"dedup\"\u003e\u003c/a\u003e How gl-react transform your Surface and effects stack\n\nWe have just overviewed how deep a GL effects stack can be: going down into each individual component that itself use many other components can ends a with a pretty big tree. That's true for any React application actually, but React is still performant.\n\n**However, we have a fundamental difference between classical React DOM and `gl-react`: a GL effects stack is just a single Canvas element at the end.**\n\n\u003e When you write a tree of GL Components, each component don't get append into the DOM like would do a stack of Virtual DOM elements. Instead we need at the end to render the full Virtual GL tree into one single `\u003ccanvas/\u003e`.\n\nTherefore, we don't treat GL Component the same way React does. `gl-react` will do internal work to **unfold user's Virtual GL tree** and convert it into a **\"scene\" object that contains everything a renderer need to know**. This object is passed as a `\"data\"` props to the underlying implementation (that we call internally **_GLCanvas_**).\n\nIf we inspect with React Dev Tools what our `\u003cSurface/\u003e` actually gets render into you will see something like this:\n\n![](/images/2016/03/resolved_rendering.png)\n\nActually, the `Surface` get rendered into a... `\u003cdiv/\u003e` **(1)**. We need to do this because we need to not only render the Canvas **(3)** but we also need to render any possible content that was passed-in the stack that would need to get rasterized (in web context, it can be a **video** or another **canvas**). In our case, it's the `\u003cTitle/\u003e` component, that is backed with **react-canvas** to draw Text using a Canvas (the only simple way to get texts in WebGL). So this is why we need **(2)**, that is a container for the content, that container is moved behind the canvas and is made invisible (unless you enable some hidden secret props! [read more about advanced props of Surface in the documentation](https://projectseptemberinc.gitbooks.io/gl-react/content/docs/api/Surface.html)).\n\n### How gl-react optimizes the effects stack \u0026 factorize computation\n\nThe previous complex example, if implemented na√Øvely, ends up with this big tree:\n\n**1. Before factorization optimization:** _(na√Øve implementation)_\n![](/images/2016/03/reactjs2016_greweb.036.jpeg)\n\nIt contains a lot of duplicates: the Title rendering appears 6 times\nand the \"Text Blurring 4-Blur stack\" also appears 5 times.\n\nThis is just computing the same thing multiple times where we should be able to compute it once...\n\nTo solve this, we will just use the VDOM **referential transparency**: if 2 VDOM element have the same reference, we can assume it renders the same thing so we can just dedupe to share and render it once.\n\n\u003e This is one of our biggest innovation in `gl-react`: when you give a stack of effects in Surface, we will dedupe the tree.\n\nAt the end of this process our example results of:\n\n**2. After factorization optimization:**\n![](/images/2016/03/reactjs2016_greweb.041.jpeg)\n\nWe have moved from 38 to 13 nodes and reduce the render speed from 20ms to 4ms.\n\n### Conclusion\n\nIf you would implement a stack of effects using the imperative OpenGL API, you would obviously write an ordered sequence of effects to do and that would naturally share the computations in temporary buffers for best performance.\n\n**The important job of gl-react is to allow you to write descriptive code without losing this advantage of using temporary pixel buffers and keeping a thin layer on top of the underlying OpenGL.**\n\n## Other side projects\n\n### gl-react-inspector\n\nOne of the most appreciated part in my talk is the Inspector we specially develop for gl-react.\n\nI initially developed it because I wanted to have charts to show people what gl-react graph looks like and without having to go Inkscape and handcrafting them...\nBut it ended up behind a useful tool to actually develop with, because you can see what's going on underneath (at each node step, and what the texture looks at intermediary steps). It also helps seeing investigating on performance.\n\nOur big future challenge with this is to make it work as a standalone devtools (I imagine it could be part of the React devtools, if we could have plugins there).\nand to make it work with React Native too.\n\n### gl-react-dom-static-container\n\n[https://github.com/gre/gl-react-dom-static-container](https://github.com/gre/gl-react-dom-static-container)\n\n### Some universal GL effects\n\n- [gl-react-blur][gl-react-blur]\n- [gl-react-negative][gl-react-negative]\n- [gl-react-constrast-saturation-brightness][gl-react-constrast-saturation-brightness]\n- [gl-react-hue-rotate][gl-react-hue-rotate]\n- [gl-react-color-matrix][gl-react-color-matrix]\n\n### gl-react-image\n\n[gl-react-image](https://github.com/gre/gl-react-image) is a component that solves preserving ratios of your images (because stretching is the default behavior).\n\n## We need your help!\n\n### What should come soon\n\n- caching framebuffers from one frame to another: allow different interesting things: cache part of the graph (e.g to allow to cache a static intensive part of the graph), cache part of a rendering with `discard;` (e.g if you make a Paint like) or even more crazy things like being able to inject the previous buffer as a texture to implement things like motion-blur or even [cellular automata](http://mathworld.wolfram.com/CellularAutomaton.html).\n\n### What might come after this\n\n- react-native-video / react-native-camera\n- static vertex data as well as static vertex shader is a current and decided (? chosen) limitation of `gl-react`. We want to focus on the incredible capabilities of fragment shaders and work on all optimization that can be made to improve the performance of working with this subset of OpenGL.\n\n### Other features\n\nThis library begin the journey of bringing OpenGL to most people using the React simplicity, hiding some complex parts of OpenGL but allowing to implement the fundamental functional bricks of it.\n\nThere are a bunch of other features that would take me weeks to explain, but feel free to [read the documentation to learn more about the other props and features](https://projectseptemberinc.gitbooks.io/gl-react/content/).\n","data":{"title":"Universal GL Effects for Web and Native","author":"Gaetan","layout":"post","tags":["react","webgl","gl-react"]}},{"id":"2015-10-01-introducing-gl-react","year":"2015","month":"10","day":"01","slug":"introducing-gl-react","content":"\n\u003cimg src=\"/images/2015/10/gl-react.png\" alt=\"\" class=\"thumbnail-left\" /\u003e Last Thursday, my talk at [React Paris Meetup](http://www.meetup.com/ReactJS-Paris/events/226103821/) was about using **the functional rendering** paradigm of **WebGL** in **React**. The library [`gl-react`](https://github.com/ProjectSeptemberInc/gl-react) wraps WebGL in React paradigm with a focus for developing 2D effects, that we need in my current startup, [Project September](https://twitter.com/ProjSeptEng), where I have the chance to develop it.\n\n## [Slides](http://greweb.me/reactmeetup7)\n\n\u003ciframe src=\"http://greweb.me/reactmeetup7\" width=\"600\" height=\"400\" frameborder=\"0\"\u003e\u003c/iframe\u003e\n\n\u003c!--more--\u003e\n\n## Abstract\n\nWe can write effects without having to learn the complex and imperative low-level WebGL API but instead composing React components, as simple as functional composition, using VDOM descriptive paradigm.\n\n[gl-react](https://github.com/ProjectSeptemberInc/gl-react) brings WebGL bindings for react to implement complex effects over content.\n\n[gl-react-native](https://github.com/ProjectSeptemberInc/gl-react-native) is the React Native implementation,\ntherefore allows universal effects to be written both for web and native.\n\nThese libraries totally hides for you the complexity of using the OpenGL/WebGL API but takes the best part of it: GLSL, which is a \"functional rendering\" language that runs on GPU.\n","data":{"title":"Introducing gl-react","description":"This presentation is an introduction to WebGL in React and functional programming concept and showcases made with gl-react and gl-react-native","thumbnail":"/images/2015/10/funrendering.png","author":"Gaetan","layout":"post","tags":["gl-react"]}},{"id":"2015-08-04-making-performant-react-applications","year":"2015","month":"08","day":"04","slug":"making-performant-react-applications","content":"\n*^ Sorry guys, you may have notice the blog post date is wrong. I won't change the URL, but thanks to how time works, this will be fixed in one month anyway :-D*\n\n[ReactEurope](https://twitter.com/chantastic/status/616608931037646850) conference\nwas to me incredibly [inspiring](https://twitter.com/chantastic/status/616670658911715328) and [promising](https://twitter.com/chantastic/status/616995607043903488).\nYersterday got tons of news and tweets from JavaScript community.\n\nOne tweet and blog post by the great [@aerotwist](https://twitter.com/aerotwist) got my attention.\n\n\u003cblockquote class=\"twitter-tweet\" data-cards=\"hidden\" lang=\"fr\"\u003e\u003cp lang=\"en\" dir=\"ltr\"\u003eI often hear claims that ‚Äúthe DOM is slow!‚Äù and ‚ÄúReact is fast!‚Äù, so I decided to put that to the test:\u0026#10;\u0026#10;\u003ca href=\"https://t.co/M1RZZiyVT2\"\u003ehttps://t.co/M1RZZiyVT2\u003c/a\u003e\u0026#10;\u0026#10;üê¢vsüêá\u003c/p\u003e\u0026mdash; Paul Lewis (@aerotwist) \u003ca href=\"https://twitter.com/aerotwist/status/616934953679458304\"\u003e3 Juillet 2015\u003c/a\u003e\u003c/blockquote\u003e\n\nI would like to express here my opinion and feedback on using React.\n\nI've been using React for almost 2 years now, and always in performance intensive use-cases, from Games to WebGL.\n\n\u003ca href=\"http://diaporama.glsl.io/\" target=\"_blank\"\u003e\n\u003cimg src=\"/images/2015/07/diaporama_3.jpg\" alt=\"\" class=\"thumbnail-left\" /\u003e\n\u003c/a\u003e\n\nI've created [glsl.io](http://glsl.io/) and I'm working on [Diaporama Maker](https://github.com/gre/diaporama-maker).\nBoth applications are built with React and combined use of HTML, SVG, WebGL.\n\nDiaporama Maker is probably the most ambitious piece of software I've ever personally done.\n\n\u003cbr style=\"clear: left\" /\u003e\n\n\u003e In short, [Diaporama Maker](https://github.com/gre/diaporama-maker) it is a WYSIWYG editor for web slideshow (mainly photo slideshows). It is a bit like iMovie with the web as first target. [The project is entirely open-sourced.](https://github.com/gre/diaporama-maker)\n\nCurrently, I am able to render the whole application at 60 FPS and this is still unexpected and surprising to me\n(press Space to run the diaporama on [diaporama.glsl.io demo](http://diaporama.glsl.io/)).\nWell, more exactly, this would not have been possible without some optimizations\nthat I'm going to detail a bit at the end of this article.\n\n\u003c!--more--\u003e\n\n## The point is productivity\n\nI don't think Virtual DOM claims to be faster than doing Vanilla DOM, and that's not really the point. **The point is productivity.**\nYou can write very well optimized code in Vanilla DOM but this might require **a lot of expertise**\nand a lot of time even for an experienced team *(time that should be spent focusing on making your product)*.\n\nWhen it comes to adding new features and refactoring old ones, this goes worse.\nWithout a well constrained framework or paradigm, things does not scale far, are time consuming and introduce bugs,...\nEspecially in a team where multiple developers have to work with each other.\n\n\u003e **See Also:** [Why does React scale?](https://www.youtube.com/watch?v=D-ioDiacTm8) by [@vjeux](https://twitter.com/Vjeux).\n\n## What matters to me\n\nThere is a lot of advantages of using Virtual DOM approach before talking about React performances.\n\nOf course, this always depends on what you are building, but I would claim that\n**there is a long way to go using React before experiencing performance issues**,\nand in the worse cases: **you can almost always find easy solutions to optimize these performance issues**.\n\n### DX\n\nReact has an incredible Developer eXperience (that people seem to call DX nowadays!) that can [help you improving UX](https://twitter.com/greweb/status/617258379183005696) and the ability to [measure Performances](https://facebook.github.io/react/docs/perf.html) and [optimize them](https://facebook.github.io/react/docs/component-specs.html#updating-shouldcomponentupdate) when you reach bottlenecks.\n\nYou can easily figure out which component is a bottleneck in the Component tree as shown in following screenshot.\n\n\u003e ![](/images/2015/07/diaporama-perfs.png)\nWith printWasted() you can see how much time React has wasted to `render()` something that didn't change and how much instances has been created. (there is also printInclusive and printExclusive)\n\nThis is a bit equivalent of the Web Console Profiler except it emphasis on your application components which is a very relevant approach.\n\n### React data flow\n\n\u003e I can't imagine re-writing Diaporama Maker in Vanilla DOM.\n\nIn Diaporama Maker, I have a lot of cross dependencies between components,\nfor instance the current `time` is shared and used everywhere in the application.\nAs a matter of fact, dependencies grow when adding more and more features.\n\n\u003e ![](/images/2015/07/diaporama_configure_kenburns.gif)\nusages of time in 3 independent components.\n\n**The descriptive Virtual DOM approach very simply solves this problem**.\nYou just have to pass props in to share data between components:\nthere is one source of trust that climb down your component tree via \"props\".\n\n![](/images/2015/07/diaporama-maker-time-props.jpg)\n\nWith Virtual DOM approach, the cost to add one new dependency to a shared data is small and **does not become more complex as the application grows**.\n\n\u003e ![](/images/2015/07/diaporama_slide_content.gif)\nanother more complex showcase of shared states.\n\nUsing an Event System like you would do in standard Backbone approach tends to lead to imperative style and spaghetti codes (and when using global events, components are not really reusable).\n\nMoreover, I think that `Views\u003c-\u003eModels` Event System approach, if not carefully used, tends to converge to an unmaintainable and laggy applications.\n\n### React is a Component library\n\nReact truly offers **component as first-class citizen**.\nThis means it allows component reusability. I've tried alternative like virtual-dom and I don't think it emphasizes enough on this benefit.\n\nThere are [important good practices](https://twitter.com/chantastic/status/616997918155759616) when using React like minimizing states and props and I'm not going to expand more on this subject. Most of these best practices are not exclusive to React but come from common sense and software architecture in general.\nOne of the important point for performance is to **choose a good granularity of your component tree**.\nIt is generally a good idea to split up a component into pieces as small as possible\nbecause it allows to separate concerns, minimize props and consequently optimize rendering diff.\n\n#### Diaporama Maker architecture\n\nYou would be surprised to know that Diaporama Maker does not even use **Flux** (that might be reconsidered soon for collaborative features). I've just taken the old \"callback as props\" approach all the way down the component tree. That easily makes all components purely modular and re-usable (no dependencies on some Stores).\nI've also taken the [inline style approach]() without actually using any framework (this is just about props-passing `style` objects).\n\nAs a consequence, I've been able to externalize a lot of tiny components that are part of my application\nso I can share them across apps and also in order to people to re-use them.\n\nWhat is important about externalizing components is also the ability to test and optimize them independently (the whole idea of modularity).\n\nHere are all the standalone UI components used by Diaporama Maker:\n\n- [bezier-easing-editor](https://github.com/gre/bezier-easing-editor)\n- [bezier-easing-picker](https://github.com/gre/bezier-easing-picker)\n- [diaporama-react](https://github.com/glslio/diaporama-react)\n- [glsl-transition-vignette](https://github.com/glslio/glsl-transition-vignette)\n- [glsl-transition-vignette-grid](https://github.com/glslio/glsl-transition-vignette-grid)\n- [glsl-uniforms-editor](https://github.com/gre/glsl-uniforms-editor)\n- [kenburns-editor](https://github.com/gre/kenburns-editor)\n\n(each one have standalone demos)\n\n\n## Optimizing performances\n\n\u003cblockquote class=\"twitter-tweet\" lang=\"fr\"\u003e\u003cp lang=\"en\" dir=\"ltr\"\u003eI\u0026#39;ve been working on crazy projects using React (like \u003ca href=\"http://t.co/U2oETh5lhZ\"\u003ehttp://t.co/U2oETh5lhZ\u003c/a\u003e ). most performance issues i\u0026#39;ve met was not because of React\u003c/p\u003e\u0026mdash; Ga√´tan Renaudeau (@greweb) \u003ca href=\"https://twitter.com/greweb/status/617210444839809024\"\u003e4 Juillet 2015\u003c/a\u003e\u003c/blockquote\u003e\n\u003cscript async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"\u003e\u003c/script\u003e\n\nHere are 2 examples of optimizations I had to do in Diaporama Maker that are not because of React:\n\n- It is easy to write not very optimized WebGL, so I work a lot to optimize the pipeline of [Diaporama engine](https://github.com/gre/diaporama)\n- CSS transforms defined on Library images was for a time very intensive for the browser to render so I am now using server-resized \"thumbnails\" instead of the full-size images. Asking the browser to recompute the `transform: scale(...)` of 50 high resolution images can be super costy. (without this optimization, the resize of the application was running at like 2-3 FPS because the library thumbnails need to recompute their scale and crop).\n\nBut what if you still have performance issue due by React? Yes this can happens.\n\n### Timeline Grid example\n\nIn Diaporama Maker, I have a Component that generates a lot of elements (like 1300 elements for a 2 minutes slideshow) and my first naive implementation was very slow. This component is [TimelineGrid](https://github.com/gre/diaporama-maker/blob/b0c6447b127785bea3c2487b0c77037418298b8c/client/ui/TimelineGrid/index.js) which renders the timescale in the timeline. It is implemented with SVG and a lot of `\u003ctext\u003e` and `\u003cline\u003e`.\n\nThe performance issue was noticeable when drag and dropping items across the application. React was forced to render() and compare the whole timescale grid every time. But the timescale does not change! it just have 3 props:\n\n```xml\n\u003cTimelineGrid timeScale={timeScale} width={gridWidth} height={gridHeight} /\u003e\n```\n\n**So it was very easy to optimize it just by using the `PureRenderMixin` to say to react that all my props are immutable.**\n(I could have implemented `shouldComponentUpdate` too).\n\nAfter this step, and for this precise example, I don't think a Vanilla DOM implementation can reach better performance:\n\n- when one of the grid parameter change, **EVERYTHING** need to be recomputed because all scales are changing.\n- React is doing even smarter thing that I would not manually do? Like reusing elements instead of destroying/creating them.\n\nThere might still be ways to go more far in optimizing this example. For instance I could chunk my grid into pieces\nand only render the pieces that are visible, like in an infinite scroll system *(I could use something like [sliding-window](https://github.com/gre/sliding-window) for this)*.\nThat would probably be premature optimization for this example.\n\n## Wrap Up\n\nTo my mind, generic benchmarks always tends to be biased and does not represent use-cases reality unless you are really covering your application itself.\n\nThe TimelineGrid component optimization explained in this article is a very specific and well chosen example,\nbut it is one counter-example for such a benchmark.\n\nEach application has its own needs and constraints and we can't really generalize one way to go.\nAlso Performance should not be the main concern to choose a technology.\n\n\nIt is easy to make Virtual DOM library benchmarks,\ncomparing the performance of rendering and Array diffing,\nbut does that covers 80% of use-cases?\nIs performance really the point?\nWhat tradeoff do you accept to make between Performance and Productivity?\n\nTell me what you think.\n\nIn the meantime, I think we can all continue getting applications done\nand [developing amazing DX](https://github.com/gaearon/react-hot-loader).\n","data":{"title":"Making performant React applications","description":"I would like to express here my opinion and feedback on using React and performance optimization you can do.","thumbnail":"/images/2015/07/diaporama_3.jpg","author":"Gaetan","layout":"post","tags":["react","vdom"]}},{"id":"2014-10-16-webglparis","year":"2014","month":"10","day":"16","slug":"webglparis","content":"\n\u003ciframe width=\"640\" height=\"360\" src=\"http://www.youtube.com/embed/Cmr2RRETCXs?feature=player_embedded\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\n","data":{"title":"[FR] webglparis talk: GLSL.io initiative and WebGL Transitions","description":"A talk I did at webglparis to present GLSL.io and GLSL Transitions initiative.","author":"Gaetan","layout":"post","tags":["webgl","GLSL"]}},{"id":"2014-09-22-ibex-cellular-automata","year":"2014","month":"09","day":"22","slug":"ibex-cellular-automata","content":"\n [gamepost]: /2014/09/ibex\n [js13kgames]: http://js13kgames.com/\n [submission]: http://js13kgames.com/entries/ibex\n [github]: http://github.com/gre/js13k-2014\n [cellular]: http://en.wikipedia.org/wiki/Cellular_automaton\n [wolfram]: http://en.wikipedia.org/wiki/Stephen_Wolfram\n [ankos]: https://www.wolframscience.com/\n [gol]: http://en.wikipedia.org/wiki/Conway's_Game_of_Life\n [cavelikegen]: http://www.roguebasin.com/index.php?title=Cellular_Automata_Method_for_Generating_Random_Cave-Like_Levels\n [logicfrag]: https://github.com/gre/js13k-2014/blob/master/src/shaders/logic.frag\n\n\n\u003ca href=\"/2014/09/ibex\"\u003e\n  \u003cimg src=\"/images/2014/09/ibex-2.png\" alt=\"\" class=\"thumbnail-right\" /\u003e\n\u003c/a\u003e\n\nLast week I finished my [JS13K game called \"IBEX\"][gamepost],\nan apocalyptic game where you have to help some wild ibex to escape from the inferno.\n\n\u003e IBEX received the 16th place (out of 129 games) from the [js13kgames][js13kgames] jury.\n\nThis article is a technical post-mortem about the development of this game in JavaScript / WebGL\nand how the world is just **ruled with [cellular automata][cellular]**\nand computed efficiently in a GLSL shader.\n\n\u003ciframe width=\"50%\" height=\"220\" src=\"//www.youtube.com/embed/nqD2qIy4auU\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\u003c!--more--\u003e\n\n## Cellular automata ruled world\n\nA **Cellular Automaton** (plurial Cellular Automata) is an **automaton** *(in other words, a state machine)*\nbased on **a grid (an array) of cells**.\nIt has been discovered years ago and popularized by [Stephen Wolfram][wolfram]\nin his interesting book [A new Kind of Science][ankos].\n\n\n\u003cfigure class=\"thumbnail-right\"\u003e\n  \u003cimg src=\"/images/2014/09/elementary-automaton.png\" /\u003e\n  \u003cfigcaption\u003e\n    \u003ca href=\"http://mathworld.wolfram.com/ElementaryCellularAutomaton.html\"\u003e\n      elementary automata.\n    \u003c/a\u003e\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nThe simplest possible cellular automaton is the one where, at each generation,\nthe cell value is determined from the **previous and the 2 adjacent cells** (left and right)\nvalue and where the value can only be **0 or 1** (white or black / true or false).\nThe way the cell value is determined is through a set of rules.\n\n\u003e In an elementary cellular automaton, there is a total of 8 rules, which means 256 possible cellular automata.\n\n### 2D cellular automaton\n\n\u003cfigure class=\"thumbnail-left\"\u003e\n  \u003cimg src=\"/images/2014/09/Gospers_glider_gun.gif\" /\u003e\n  \u003cfigcaption\u003e\n    \u003ca href=\"http://en.wikipedia.org/wiki/Conway's_Game_of_Life\"\u003eConway's Game of Life\u003c/a\u003e,\n    a well known 2D cellular automaton.\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nThe kind of Cellular Automaton I focused on for my game is **2D cellular automaton**:\nAt each generation, the cell value is determined from **the previous value and the 8 adjacent cells**\nusing a finite set of rules.\n\nIt is important to understand that these rules are applied in parallel for __all__ cells of the world.\n\n\u003cbr /\u003e\n\n\u003cfigure class=\"thumbnail-right\"\u003e\n  \u003cimg src=\"/images/2014/09/ibex-experiment2.png\" /\u003e\n  \u003cfigcaption\u003e\n    Early version with 4 elements and simple rules:\n    Water falls in Air, Fire grows in Air, Water extinguishes Fire, Earth drops Water + creates Fire\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n**A 2D cellular automaton rule:**\n\n![](/images/2014/09/ibex-rule-2d.png)\n\nWhat I've found is that\n**the WebGL and the GLSL language works well to implement a cellular automaton**.\n\nThe GLSL paradigm is what I like to call [functional rendering](/2013/11/functional-rendering/):\nIt is, to simplify, a function **`(x,y) =\u003e (r,g,b,a)`**:\nYou fundamentally have to implement this function which **gives a color for a given viewport position**,\nand you implement it in a dedicated language which compiles to the GPU.\n\nSo we can implement a 2D cellular automaton where each cell is a real (x,y) position in the Texture\nand where the (r,g,b,a) color is used to encode your possible cell states, and that's a lot of possible encoding!\n\nIn my game, i've chosen to only use the `\"r\"` component to implement the cell state.\nBut imagine all the possibilities of encoding more data per cell (like the velocity, the amount of particle in the cells,...).\n\n**Here is a boilerplate of making a Cellular Automaton in GLSL:**\n\n```glsl\nuniform sampler2D state; // the previous world state texture.\nuniform vec2 size; // The world size (state texture width and height)\n\n/*\n The decode / encode functions provide an example of encoding\n an integer state in the \"r\" component over possible 16 values.\n You can definitely implement your own. Also \"int\" could be something more complex\n */\nint decode (vec4 color) {\n  return int(floor(.5 + 16.0 * texture2D(state, uv).r));\n}\nvec4 encode (int value) {\n  return vec4(float(r) / 16.0,  0.0, 0.0, 1.0);\n}\n\n/*\n  get(x,y) is doing a lookup in the state texture to get the (previous) state value of a position.\n */\nint get (int x, int y) {\n  vec2 uv = (gl_FragCoord.xy + vec2(x, y)) / size;\n  return (uv.x \u003c 0.0 || uv.x \u003e= 1.0 || uv.y \u003c 0.0 || uv.y \u003e= 1.0) ? 0 :\n    decode(texture2D(state, uv).r);\n}\n\nvoid main () {\n  // We get all neighbors cell values from previous state\n  int NW = get(-1, 1);\n  int NN = get( 0, 1);\n  int NE = get( 1, 1);\n  int WW = get(-1, 0);\n  int CC = get( 0, 0);\n  int EE = get( 1, 0);\n  int SW = get(-1,-1);\n  int SS = get( 0,-1);\n  int SE = get( 1,-1);\n\n  int r; // r (for result) is the new cell value.\n\n  ////////////////////////////\n  // NOW HERE IS THE COOL PART\n  // where you implement all your rules (from the 9 state values)\n  // and give a value to r.\n  ////////////////////////////\n\n  gl_FragColor = encode(r);\n}\n```\n\n\u003e**The complete game rules are all implemented in a GLSL fragment shader:\n[logic.frag][logicfrag]**.\nIt is important to understand that this fragment shader takes in input\nthe previous world state (as an uniform texture)\nand computes a new state by applying the rules.\n\nOn the JavaScript side, you need to **give an initial state to the texture**\n(so you need to also encode data the same way it is done in the shader).\nAlternatively you can also make a shader to do this job\n*(generating the terrain can be intense to do in JavaScript, like it is the case for my game...)*.\n\nAlso if you want to **query the world from JavaScript**,\n*(e.g. you want to do physics or collision detection like it is also the case for my game)*,\nyou need to use `gl.readPixels` and then decode data in JavaScript.\n\nI'll explain this a bit later in another article.\nLet's now go back to the Cellular Automaton used in IBEX.\n\n\u003cfigure\u003e\n  \u003cimg src=\"/images/2014/09/ibex-screenshot1.png\" /\u003e\n  \u003cfigcaption\u003e\n    The different elements gameplay.\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\n### The elements\n\nThe game theme was \"Four Elements: **Water, Air, Earth, Fire**\", so I've used\nthese 4 elements as primary elements of the cellular automaton.\n\nEach elements also have secondary elements that can be created from each other interactions:\n**Source, Volcano, Grass, WindLeft, WindRight**.\n\n- The **Volcano** is lava growing in the Earth. It creates Fire (when there is Air).\n- The **Source** is water infiltrating in the Earth. It drops Water (when there is Air).\n- The **Grass** (or Forest) grows on Earth with Water. It is a speed bonus for ibex but it propagates fire very fast. It also stop the water from flowing.\n- The **Wind** (left or right wind) is created randomly in Air. It have effects on Water and Fire propagation and also on ibex speed.\n\n**Some constants...**\n\n```glsl\n// Elements\nint A  = 0; // Air\nint E  = 1; // Earth\nint F  = 2; // Fire\nint W  = 3; // Water\nint V  = 4; // Volcano\nint S  = 5; // Source\nint Al = 6; // Air Left (wind)\nint Ar = 7; // Air Right (wind)\nint G  = 8; // Grass (forest)\n```\n\n\u003cfigure class=\"thumbnail-right\"\u003e\n  \u003cimg src=\"/images/2014/09/ibex-experiment1.png\" /\u003e\n  \u003cfigcaption\u003e\n    Fun and experimental result accidentally produced in an early development of the rules.\n  \u003c/figcaption\u003e\n\u003c/figure\u003e\n\nTo summary, there is 9 possible elements,\nand rules are determined from the 9 previous cells:\nThis makes a LOT of possible rules.\nHowever, the rules involved here remain simple and with just a few rules.\n\n\u003e That is the big thing about cellular automata:\nvery simple rules produce an incredible variety of results.\n\nIn general, we can classify my game rules into 2 kind of rules:\n\"interaction\" rules and \"propagation\" rules.\nThe first kind describes how two (or more!) elements interact each other.\nThe second kind describes the way an element evolve.\nSome rules will also mix them both.\n\n### Some simple \"propagation rule\"\n\n**Earth stays:**\nan Earth is returned if there was an Earth before.\n\n![](/images/2014/09/ibex-rule-earth.png)\n\n**Water falls in Air:**\na Water is created if there was a Water on top.\n\n![](/images/2014/09/ibex-rule-water1.png)\n\n**Fire grows in Air:**\na Fire is created if there was a Fire on bottom.\n\n![](/images/2014/09/ibex-rule-fire1.png)\n\n\nThese rules produce very elementary result, we will now see how we can improve them.\n\n### Weights in rules\n\n**More powerful rules can also be reached by using weights**:\nyou can affect a weight for each neighbor cell to give more or less importance to them.\n\nLet's take a look at a simple example:\n\n![](/images/2014/09/ibex-rule-gencave-example.png)\n\n\u003e N.B.: only the \"sum\" is considered in the rule:\nif an element matches, we sum the weight of the cell, otherwise \"zero\".\n\n**This example is actually a weighted version of [the cave rule you can find here][cavelikegen]:**\n\n\u003cfigure\u003e\n  \u003cfigcaption\u003e\n    Result of the rule, with (Air or Earth) random pick for each  initial cell value.\n  \u003c/figcaption\u003e\n  \u003cimg src=\"/images/2014/09/ibex-gencaveresult.png\" /\u003e\n\u003c/figure\u003e\n\n### Randomness in rules\n\n**Combine Randomness and Weights and you get a very powerful simulation.**\n\nTo avoid seeing some (well known) patterns in the simulation I added some randomness in my rules.\n**With randomness, the results are incredibly powerful.**\n\nIn the following video, notice how cool the fire propagation can result\nby varying the propagation randomness factor.\n\n\u003ciframe width=\"100%\" height=\"420\" src=\"//www.youtube.com/embed/mF-MNHk7u4s\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"\u003e\u003c/iframe\u003e\n\n**The code:**\n\n```glsl\n#define AnyADJ(e) (NW==e||SE==e||NE==e||SW==e||NN==e||SS==e||EE==e||WW==e)\n// ^^^^^^^^ MACRO !\nif (\n  CC == G \u0026\u0026\n  RAND \u003c firePropagation \u0026\u0026\n  ( AnyADJ(F) || AnyADJ(V) )) {\n  r = F;\n}\n```\n\n#### Randomness in GLSL ???\nGLSL is fully stateless and there is **NO WAY** to have a `random()` function in the GPU.\nThe trick to do randomness in GLSL is by invoking some math black magic:\n\n```glsl\nfloat rand(vec2 co){\n  return fract(sin(dot(co.xy ,vec2(12.9898,78.233))) * 43758.5453);\n}\n```\n\n**`rand`** is a [popular](http://stackoverflow.com/questions/4200224/random-noise-functions-for-glsl)\nfunction which returns a pseudo-random value (from 0.0 to 1.0) for a given position.\n\nMy personal **black magic** was to define a convenient macro to have a \"RAND\" word which would get me\na new random number.\n\n```glsl\n#define RAND (S_=vec2(rand(S_), rand(S_+9.))).x\n```\n\n`S_` is a seed which is accumulated when calling this `RAND`.\nBecause this macro will be inlined in the code, `S_` must be defined in a local variable\n(so in summary, `RAND` is doing local side-effect).\n\n```glsl\nvec2 p = gl_FragCoord.xy;\nvec2 S_ = p + 0.001 * time;\n```\n\nNote that **the current pixel position** itself AND **the time** are both used for initializing the seed.\nIt produces **variable randomness over time and for each pixel**.\n\nLet's now see other examples where randomness can be very powerful.\n\n### The Water and Fire interactions\n\n**Fire grows and diverges**:\n\n![](/images/2014/09/ibex-rule-fire2.png)\n\n- the \"left\" and the \"right\" columns in this rule allows **divergence** in the way fire grows:\nInstead of growing straight up, **the fire can also move a bit left or a bit right**.\nA lower weight for these side columns make the fire diverge a bit less than a \"triangle\" propagation.\n\nHere is the GLSL code:\n\n```glsl\n// Fire grow / Fire + Water\nif (\n  -0.05 * float(NW==W) + -0.40 * float(NN==W) + -0.05 * float(NE==W) + // If water drop...\n  -0.50 * float(WW==W) + -0.50 * float(CC==W) + -0.50 * float(EE==W) + // ...or water nearby.\n   0.35 * float(SW==F) +  0.90 * float(SS==F) +  0.35 * float(SE==F)   // Fire will move up and expand a bit.\n \u003e= 0.9 - 0.6 * RAND // The sum of matched weights must be enough important, also with some randomness\n) {\n  r = F;\n}\n```\n\n**Water falls, diverges and creates holes**:\n\n![](/images/2014/09/ibex-rule-water2.png)\n\n\u003cfigure class=\"thumbnail-right\"\u003e\n  \u003cimg src=\"/images/2014/09/ibex-rain.png\"/\u003e\n  \u003cfigcaption\u003eThe rain in IBEX. Notice how Water diverges a bit and creates holes.\u003c/figcaption\u003e\n\u003c/figure\u003e\n\n- Same as the fire rule, we also have **divergence** in the water.\n- However there is one more important thing in the rule:\nthanks to the **double inequality**,\nWater is created only if there is not already too much Water:\nit **results of creating Air between the Water particules**.\nThis make Water elements to be less compact than Fire elements,\nthe water does not visually \"expand\" contrary to the fire.\n- The **randomness** helps a lot here to give **no visible patterns** in this job.\n\n\u003cbr /\u003e\n\nHere are all rules which creates Water:\nin this rules you can also notice how **the Water flows on Earth** and how\nthe **occasional rain** is implemented.\n\n```glsl\nif (\n// Water drop / Water + Fire\n  between(\n    0.3 * float(NW==W) +  0.9 * float(NN==W) +  0.3 * float(NE==W) +\n    0.1 * float(WW==W) + -0.3 * float(CC==F) +  0.1 * float(EE==W) +\n                         -0.3 * float(SS==F)  \n    ,\n    0.9 - 0.6 * RAND,\n    1.4 + 0.3 * RAND\n  )\n\n  || // Water flow on earth rules\n\n  !prevIsSolid \u0026\u0026\n  RAND \u003c 0.98 \u0026\u0026\n  ( (WW==W||NW==W) \u0026\u0026 SW==E || (EE==W||NE==W) \u0026\u0026 SE==E )\n\n  || // Occasional rain\n  !prevIsSolid \u0026\u0026\n  p.y \u003e= SZ.y-1.0 \u0026\u0026\n  rainRelativeTime \u003c 100.0 \u0026\u0026\n  between(\n    p.x -\n    (rand(vec2(SD*0.7 + TI - rainRelativeTime)) * SZ.x) // Rain Start\n    ,\n    0.0,\n    100.0 * rand(vec2(SD + TI - rainRelativeTime)) // Rain Length\n  )\n\n  || // Source creates water\n  !prevIsSolid \u0026\u0026 (\n    0.9 * float(NW==S) +  1.0 * float(NN==S) +  0.9 * float(NE==S) +\n    0.7 * float(WW==S) +                        0.7 * float(EE==S)\n    \u003e= 1.0 - 0.3 * RAND\n  )\n) {\n  r = W;\n}\n```\n\n**Source rules**\n\nThe Source can be created in the Earth by two rules:\nEither there is enough water around,\nOr there is source on top.\n\nNote the important usage of randomness.\n\n![](/images/2014/09/ibex-rule-source.png)\n\n\n### The grass propagation, Limiting the forest height\n\nTo finish, the grass needed a special extension to the so-far-used 2D cellular automaton,\nthe grass cell value is not only being determined from the 8 adjacent cells:\n\nTo have more complex structure, **the grass is determined\nfrom the previous cell at position `(x, y-N)`**,\nwhere x and y is the cell position and N is a variable value (random but constant per cell position).\nIn other word, a forest can grow if the cell at N step under it is not a forest.\nThis extra rule just adds a constraint on the max height that a forest can have.\n\n\u003cfigure\u003e\n  \u003cfigcaption\u003eA Grass can be created if the (x,y-N) cell is not a Grass.\u003c/figcaption\u003e\n  \u003cimg src=\"/images/2014/09/ibex-rule-forest-specific.png\" /\u003e\n\u003c/figure\u003e\n\n\nHere is a demo showing the forest propagation randomness:\n\n\u003ciframe width=\"100%\" height=\"480\" src=\"//www.youtube.com/embed/V_enCKx8XHA\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"\u003e\u003c/iframe\u003e\n\n\n### Drawing into the world\n\n**Drawing into the world is also done in GLSL: through uniforms.**\nAnother alternative way to do that would have be to use `gl.readPixels` to extract it out in JavaScript,\nto write into the Array and inject it back to the shader...\nbut this solution is not optimal because `readPixels` is blocking and costy (CPU time).\n\n```glsl\nuniform bool draw; // if true, we must draw for this tick.\nuniform ivec2 drawPosition; // The position of the drawing brush\nuniform float drawRadius; // The radius of the drawing brush\nuniform int drawObject; // The element to draw\n\n\nvoid main (void) {\n  ...\n  bool prevIsSolid = CC==E||CC==G||CC==V||CC==S;\n\n  if (draw) {\n    vec2 pos = floor(p);\n    if (distance(pos, vec2(drawPosition)) \u003c= drawRadius) {\n    // Inside the brush disc\n      if (drawObject == W) {\n        // Draw Water\n        if (prevIsSolid \u0026\u0026 CC!=G) {\n          // Source is drawn instead if there was a solid cell\n          r = S;\n        }\n        else if (!prevIsSolid \u0026\u0026 mod(pos.x + pos.y, 2.0)==0.0) {\n          // We draw Water half of the time because Water is destroyed when surrounded by Water\n          r = W;\n        }\n      }\n      else if (drawObject == F) {\n        // Draw fire or volcano if solid cell.\n        r = prevIsSolid ? V : F;\n      }\n      else {\n        // Draw any other element\n        r = drawObject;\n      }\n    }\n  }\n\n  ...\n}\n```\n\n## World generation is also a Cellular automaton!\n\nThe world is generated on the fly when the ibex progress to the right. This is done chunk by chunk.\n\n\u003e More precisely, the world height is 256 pixels and a new part of the world is discovered each 128 pixels ‚Äì\nIn other words, the generation is divided into world chunks of `(128 x 256)` pixels.\n\nEach world chunk is generated using a cellular automaton (different from the simulation one).\n\nAs shown in a previous example,\nwe can easily generate \"cave like maps\" from [this technique][cavelikegen].\nI've added to this a [few improvments](https://github.com/gre/js13k-2014/blob/master/src/index.js#L842):\n\n- The [initial random conditions](https://github.com/gre/js13k-2014/blob/master/src/index.js#L881) ensure\nthat **the bottom of the world is Earth** and that **the top of the world is Air**.\n*(that with gradients of randomness)*\n- [Randomness](https://github.com/gre/js13k-2014/blob/master/src/index.js#L896-L906)\nhas been added to the rules to make the terrain evolving a bit more\n*(otherwise it creates stable but small caves)*.\n- The number of generation step is set to 26. the randomness of the rules is decreasing through steps to produce stable results.\n- In an attempt to create **seamless maps**,\nthe initial random state for x=0 is set to the values of x=127 of the previous world chunk.\n[(code here)](https://github.com/gre/js13k-2014/blob/master/src/index.js#L878)\nIt isn't perfect because you can still notice some edges.\n- For **more diversity in generated chunks**, here are the parameters that can [randomly vary](https://github.com/gre/js13k-2014/blob/master/src/index.js#L845-L848):\n  - The **amount of Earth** (can create dense areas VS floating platform areas)\n  - The **chance of Water Source** in the Earth (will creates a lot of forest)\n  - The **chance of Volcano** in the Earth (dangerous world chunk)\n\n![](/images/2014/09/ibex-gen-variety.png)\n\n\n## More articles to come\n\nDid you like this article?\n\nI'll try to write more about these subjects:\n\n- The **\"Pixels paradigm\"**, Pixel as first class citizen: How to query and analyze the pixels world. How to do simple bitmap collision detection.\n- The **game rendering performed in a GLSL shader** and all the graphics details I've spent hours on.\n- **things I've learned from WebGL**, how to solve the bad approaches I've taken,\nand how I could have made a much more efficient game.\n- **what could have made this game even more interesting**,\nand some ideas that was not reachable in a 2 weeks deadline.\n","data":{"title":"Cellular Automata in IBEX","description":"IBEX is my game made for js13kgames. This article explains how the game has been implemented with GLSL and cellular automata.","thumbnail":"/images/2014/09/ibex-2.png","author":"Gaetan","layout":"post","tags":["gamedev","js13k","GLSL"]}},{"id":"2014-09-14-ibex","year":"2014","month":"09","day":"14","slug":"ibex","content":"\n [js13kgames]: http://js13kgames.com/\n [submission]: http://js13kgames.com/entries/ibex\n [github]: http://github.com/gre/js13k-2014\n [cellular]: http://en.wikipedia.org/wiki/Cellular_automaton\n\n\u003ca href=\"http://js13kgames.com/entries/ibex\"\u003e\n  \u003cimg src=\"/images/2014/09/ibex.png\" alt=\"\" class=\"thumbnail-left\" /\u003e\n\u003c/a\u003e\n\n**I've just [finished and submitted][submission] my game for [js13kgames][js13kgames],**\na contest where you have to make a web game in less than 13 kilobytes of a ZIP archive.\n\n**[PLAY IT HERE][submission]** / [github][github]\n\n**I had a lot of fun making this game and I think it is by far the best game I ever finished.**\nIt is a bit a continuation of my [\"antsim\" game](/2014/05/ld29) prototype in the idea\nthat you don't control directly the entities but you are at an higher level with simple interactions.\n\nThe game should be performant enough but however \nrequire that you have a good hardware to support WebGL and some advanced limits (I used too much uniforms).\nI'll talk more about the compatibility and performance issues in a next postmortem article.\n\nIf it doesn't work for you, please report me a dump of [http://webglreport.com/](http://webglreport.com/).\n\n## Playthrough\n\n\u003ciframe width=\"100%\" height=\"440\" src=\"//www.youtube.com/embed/nqD2qIy4auU\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\u003c!--more--\u003e\n\n## The Story\n\nThe world is burning, Protect and Escort a group of wild ibex away from the inferno.\nOn the path there are sleeping ibex to rescue, wake them and they will join the group.\n\n## The Gameplay\n\n\nYou will have to use the **4 elements (Air, Earth, Fire, Water)**\nto make the ibex progress safely in the environnment.\n\nThe 4 elements are primary elements but there are also secondary elements at play:\n\n- The **Volcano** (creates fire)\n- The **Source** (drops water)\n- The **Forest** (created by Earth and Water)\n- The **Wind** (left wind and right wind is created randomly in Air)\n\n\nAll elements interact with each other and **the ibex also react to elements in different ways**.\n\n- Earth is a platform for the ibex\n- Fire scares the ibex\n- Water attracts the ibex\n- Forest makes the ibex running faster\n\n## The Controls\n\nSPACE to draw an element, ARROWS to move, W/Z + X + C + V to switch between Air + Earth + Fire + Water.\n\nKeyboard is recommended but playing only with mouse is also possible (click on elements and DRAG the inner cursor).\n\n\n## Some technical notes\n\nThis game uses a [cellular automaton][cellular] to simulate a world with different elements (Air, Earth, Fire, Water).\nThis cellular automaton is technically computed as a texture in the GPU through WebGL and a GLSL shader (`logic.frag`).\n\nWhile I was writing this game [@42loops](https://twitter.com/42loops) made me buy this awesome book that I'm still reading:\n\n\u003cblockquote class=\"twitter-tweet\" lang=\"fr\"\u003e\u003cp\u003e\u003ca href=\"http://t.co/8SiMZ9PWDX\"\u003epic.twitter.com/8SiMZ9PWDX\u003c/a\u003e\u003c/p\u003e\u0026mdash; Ga√´tan Renaudeau (@greweb) \u003ca href=\"https://twitter.com/greweb/status/506862098597834752\"\u003e2 Septembre 2014\u003c/a\u003e\u003c/blockquote\u003e\n\u003cscript async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"\u003e\u003c/script\u003e\n\nYou can interact with this world by drawing elements with a brush.\n\nHowever, simulation is not enough to make a game so I mainly focused on the story and the gameplay:\nOn top of this simulation are living the ibex. They are managed in JavaScript and behaves with an AI.\nThe pixels collision and the ibex decisions (e.g; react on fire nearby) is based on querying the simulated world (stored in the GPU as a texture).\n\nThe rendering of the game is also performed in WebGL through another shader (`render.frag`).\nMy source code is [on Github][github].\n\n**I will try to write a technical post-mortem to explain you more of this \nand also what went wrong and what could have been more efficient in my usage of WebGL.**\n\n## Bonus\n\nDuring my early development, I've experimented the fire propagation in deep forest:\n\n\u003ciframe width=\"480\" height=\"440\" src=\"//www.youtube.com/embed/YU_pYAauFo4\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\n","data":{"title":"IBEX, my js13k game","description":"IBEX is a game made in 13k of JavaScript code made for the js13kgames one-month contest.","thumbnail":"/images/2014/09/ibex.png","author":"Gaetan","layout":"post","tags":["gamedev","js13k"]}},{"id":"2014-05-04-ld29","year":"2014","month":"05","day":"04","slug":"ld29","content":"\n[ludumdare]: http://ludumdare.com/compo/\n[play]: http://greweb.me/ld29/\n[source]: //github.com/gre/ld29/\n[entry]: http://www.ludumdare.com/compo/ludum-dare-29/?action=preview\u0026uid=18803\n[sunvox]: http://www.warmplace.ru/soft/sunvox/\n[wrong]: /2014/05/ld29/#wrong\n[right]: /2014/05/ld29/#right\n\n\u003cimg class=\"thumbnail-left\" src=\"/images/2014/05/computer_preset.jpg\" alt=\"\"\u003e\n\nI **gamejam-ed** last weekend to the [Ludum Dare][ludumdare] \ntheming *\"Beneath the Surface\"* (29th edition).\n\nI enjoyed that time a lot.\nWhat changed from the previous Ludum Dare for me is that \n**I'm now** ‚Äîand happy to be‚Äî **a father**,\nand I'm enough trained to wake up at 3am so I could be there attending the beginning!\n\nThis article is my diary of this incredible 48 hours past to\ndevelop **\"Anthill\", a minimalist anthill simulation game**.\nThis postmortem will explain what [went right][right] and what [went wrong][wrong]\nfor this compo.\n\n\nThe Game\n===\n\nI haven't played any Ant Simulation Game \nbut I recently played a lot [\"Banished\"](http://www.shiningrocksoftware.com/),\nan awesome city-building strategy game,\nand I was inspired by the \"assign jobs to people\" gameplay of this game.\n\n- **[Play the Game][play]**\n- [Ludum Dare entry][entry]\n- [Source code][source]\n\n\u003ciframe width=\"640\" height=\"360\" src=\"http://www.youtube.com/embed/DBt-5Qmzu1k?feature=player_embedded\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\nTimelapse\n===\n\nDeveloping a complete game in 48 hours (including sleeping) is tough,\nespecially that it is also about making the graphics and the music!\n\nThe game resulting of these 2 days is more a **prototype** than a finished game:\nthe simulation remains minimalist and fastly boring,\nthe food is the only resource you have to care about.\n\n\nHere is a **300x accelerated screencast of the developement of \"Anthill\"**:\n\u003ciframe width=\"640\" height=\"360\" src=\"http://www.youtube.com/embed/VhH7of4gAHk?feature=player_embedded\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\u003c!--more--\u003e\n\n\u003ca name=right\u003e\u003c/a\u003e What went right\n---\n\n### Music\n\nBest achievement of my LD entry was ‚Äìto me‚Äì **making the music**.\n\n\u003cimg class=\"thumbnail-left\" src=\"/images/2014/05/music_preset.jpg\" alt=\"\"\u003e\n\nI'm not a musician, neither a pianist, but I played a lot with [Audio](/tags/audio/) the past year and I've managed to make a music for this game ‚Äîand had a lot of fun making it.\nI've used my *Yamaha P105* as a MIDI controller for making the game music and connecting it \nto [SunVox][sunvox], a modular tracker software.\n\n#### Play the music - feedback appreciated\n\n\u003ciframe width=\"100%\" height=\"450\" scrolling=\"no\" frameborder=\"no\" src=\"https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/playlists/32426683\u0026amp;auto_play=false\u0026amp;hide_related=false\u0026amp;visual=true\"\u003e\u003c/iframe\u003e\n\n\u003cimg class=\"thumbnail-right\" src=\"/images/2014/05/playing_midi.png\" alt=\"\"\u003e\n\nMost of the notes you heard have been **recorded live from my hand**, and I was quite proud of that, because I'm quite a noob on a piano.\nDoing that way, you keep some imperfection in the music (little delay on the notes, especially in the intro music) and I think that sometimes makes the music better.\n\nI've talked a few times about SunVox, also [I still want to make my own web version](/2013/07/zound-live/) \nof a **modular audio tracker** that anyone (first myself!) could use \nand also could embed live in the game (and, for instance, having game variable impacting the audio experience).\n\n### Graphics\n\nAlso, I'm more a developer than a graphist so I would say this was a success^^\n\n\u003cblockquote class=\"twitter-tweet\" lang=\"fr\"\u003e\u003cp\u003eMy best \u003ca href=\"https://twitter.com/search?q=%23gamedev\u0026amp;src=hash\"\u003e#gamedev\u003c/a\u003e graphics ever! \u003ca href=\"http://t.co/FksRoq1r9x\"\u003epic.twitter.com/FksRoq1r9x\u003c/a\u003e\u003c/p\u003e\u0026mdash; Ga√´tan Renaudeau (@greweb) \u003ca href=\"https://twitter.com/greweb/statuses/460558902812086272\"\u003e27 Avril 2014\u003c/a\u003e\u003c/blockquote\u003e\n\u003cscript async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"\u003e\u003c/script\u003e\n\n### Using a library\n\nI've entirely **focused on making my game** rather than making the technical stack.\nUsually, I tend to develop my own framework with the game.\nIt is good to learn and discover some [cool ways of programming a game](/2014/01/promisify-your-games/)\nbut this time I really wanted to get things done...\n\n### Phaser.io\n\n...so I've used **[Phaser](http://www.phaser.io/)** which is a trendy and awesome framework.\nIt is also built on top of the **Pixi.js** rendering library (I used in [LD27](http://www.ludumdare.com/compo/ludum-dare-27/?action=preview\u0026uid=18803)) and **p2.js** physics engine,\nwhich are two brillant and performant JavaScript libraries.\n\nHopefully I started my project with a \"Phaser template\" I bootstraped during the previous warmup weekend.\nI've still lost some time playing with the Phaser API but I could have relied on Phaser features and performance so I think it worth it the second day.\n\n### Google Hangout with Ludum Dare friends\n\nI used Google Hangout during the whole Ludum Dare, as a way to put my webcam in the timelapse, \nbut more importantly to have other Ludum Dare friends coming in it!\n\nThis was a cool first experiment as an alternative to the \"stream my dev\" approach.\nIt was quite fun and social to have [@mrspeaker](http://twitter.com/mrspeaker)\nshowing me his crazy Metal Meter game advancement.\n\n![](/images/2014/05/hangout.png)\n\n\u003ca name=wrong\u003e\u003c/a\u003e What went wrong\n---\n\n### Achievability \n**My [initial plan](https://github.com/gre/ld29/blob/master/TODO.md) was too ambitious.**\nIt seems to always happen with me: I get the first day a lot of ideas and motivation, and I tend to underestimate the work to be done. Then at the end of the first day, I'm frustrated to find out I won't have enough time to do all my plans.\nIndeed it is good to have a lot of ideas but, next Ludum Dare, I'll try to be sure to have achievable goals.\n\n\u003e Next time, let's **remove half the features** I establish!\n\nSo, the second day morning, I've been replanning from scratch and reprioritize my features to basically have something finished and working as a game.\nBecause of this decision, the result you can see is a game, but far from the one I originally wanted. \n**This version is only about digging, collecting mushrooms and avoiding stravation**.\n\nAlso I originally scheduled to work that way **Day 1: developing-only, Day 2: music and graphics + new features**. I'm not sure this approach can really works, especially it doesn't scale, we better work by adding complete features one after the other. That theorically works, but it is however difficult to apply in practice with the stress of the Ludum Dare countdown!\n\n### Path finding performance\n\n**Implementing my own path finding algorithm was fast but not optimal** at all because of performances!\nI recently released a bugfixed version which just use an existing [path finding library](http://qiao.github.io/PathFinding.js/visual/) to fix my bad implementation \nand also to avoid an ant to request path finding each frame when looking for a task to be affected on.\n\n### Finding the good simulation parameters\n\nI underestimated a bit the amount of work needed to find the good parameters of the simulation.\nThose are very important to **make the game well-balanced** (e.g. not impossible but also not super-easy), I don't think I found the optimal parameter in the released version.\n\nI think I need two important things to make this parameter search easier:\n\n- a framework to maintain parameters presets and change them live.\n- make the simulation running at any speed to ease the development. (faster simulation)\n\n### [bug] A task is not always completed by the closest ant\n\nThere is a inconvenient **proximity bug** in my game that you may have noticed:\nwhen a new job is created, it may happen that, even if a ant was here nearby, a very far ant is assigned to this job.\n\nI'm using a `foreach ant in noJobAnts { findAntTask(ant,availableTasks) }` loop.\nIn this first approach, even if a ant will take the closest task in `findAntTask`, the first assigned ant will occupy the task even if a closer ant was after in the noJobAnts list.\n\nAnother approach to solve this issue would be to do `foreach task in availableTasks { findTaskAnts(task,noJobAnts) }`.\nIn this second approach, the `findTaskAnts` should sort the noJobAnts list for the given task proximity.\n\nHowever it is not that trivial because tasks also have different priorities for a given ant, and `findAntTask` solved that.\nIn my original plan, there is some basic tasks like \"clean the dirt\" or \"clean the corpse\" which can be done by any ant. However, a specialized ant (let's say an Harvester) may have some more important tasks to be done first, so this is why we need this priority.\n\nIn other word, fixing this bug is not trivial and I need to find the optimal loop for that!\n\nTo be continued...\n---\n\nI really wish to continue this game because I enjoyed making it and I would enjoy playing the one I have in mind! \nTime will tell if I can accomplish that wish!\n\n![](/images/2014/05/anthill.png)\n\nYou can make games!\n---\n\nI strongly think anyone a bit motivated and inspired can participate [Ludum Dare][ludumdare].\n\nWhoever you are, you can make games!\n\n- If you are a developer, you can develop games easily! But that's not enough, you will require graphics and music!\n- If you are a graphist, you can make crazy beautiful games! There is tools to help you making the game without coding.\n- If you are a game designer or story teller, you can make awesome games too! You could also just make a cool text adventure game?\n\n\u003ciframe width=\"640\" height=\"480\" src=\"//www.youtube.com/embed/PVbCECjxFds\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n","data":{"title":"48 hours to prototype an Ant Sim Game","description":"I gamejam-ed last weekend to the Ludum Dare 29 theming \"Beneath the Surface\" to develop \"Anthill\", a minimalist anthill simulation game.","thumbnail":"/images/2014/05/anthill.png","author":"Gaetan","layout":"post","tags":["gamedev","ludumdare","javascript","phaser"]}},{"id":"2014-03-12-panzer-dragoon-1k","year":"2014","month":"03","day":"12","slug":"panzer-dragoon-1k","content":"\n[demo]: http://js1k.com/2014-dragons/demo/1790\n[source]: https://gist.github.com/gre/9504494\n[jscrush]: http://www.iteral.com/jscrush/\n[jscrush-npm]: https://github.com/gre/jscrush/\n[demojs]: http://demojs.org/\n[js1k]: http://js1k.com/\n[p01]: http://www.p01.org/\n\n[\u003cimg src=\"/images/2014/03/js1k.png\" alt=\"\" class=\"thumbnail-left\" /\u003e][demo]\n\nThis article introduces my journey into the JS1K world\nand a few tricks I've used for my entry [\"Panzer Dragoon 1k\"][demo] ([source][source]).\n\nWelcome to the world of hacks, tricks and getting-things-done-at-any-price.\nYou will turn the worst JavaScript practices and ugliest JavaScript facts to your advantage.\nWelcome to the world where coding the bad way is satisfying!\n\nPanzer Dragoon 1k\n---\n\n- **[Play the JS1K entry][demo]**\n- [Source Code][source]\n\nPanzer Dragoon Original Game\n---\n\n\u003ciframe width=\"640\" height=\"480\" src=\"//www.youtube.com/embed/peoRBj9U-jI\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\u003c!--more--\u003e\n\nJS1K\n===\n\n**[JS1K][js1k]** is a competition where you have to make a demo (or a game, or anything)\nin less than **1 kilobytes** of JavaScript: **less than 1024 characters of source code**.\n\nTo reach that goal you will need ideas, JS ninja tricks \nand most important: **patience and perseverance**!\nBut really, **anyone can participate**.\n\nI recommend that you take a look at [js1k.com][js1k] and browse the existing entries.\nThere is awesome guys participating to this yearly event,\nI had the chance to meet some of them at [DemoJS 2013][demojs] Paris event.\nAlso checkout [www.p01.org][p01] which contains some very good examples of crazy short demos.\n\nTools\n---\n\nBut first things first: you need tools to minimize source code (it can simply be removing comments and spaces, minifiers, or it can be much more crazy tools like crushers).\n\nPersonally, I'm using:\n\n```bash\ncat source.js | uglifyjs -c unused=false | tee minified.js | jscrush \u003e crushed.js \u0026\u0026 wc -c *.js\n```\n\nThis small homemade command (to use in a npm script) results for my game in:\n\n```\n    1019 crushed.js\n    1756 minified.js\n    7241 source.js\n```\n\nIf you are interested, I've made this toolkit available in a complete boilerplate \nthat you can easily fork for your own usage: \n[https://gist.github.com/gre/9364718](https://gist.github.com/gre/9364718).\n\n\u003e **P.S.** [`jscrush`][jscrush-npm] is a npm module that you can directly use from the CLI \nbut it is a port of the awesome [www.iteral.com/jscrush/][jscrush].\n\nThe beginning: Saving bytes\n===\n\nIt quickly becomes frustrating to compete in JS1K\nbecause you are basically trying to put a cow in a car (or an elephant if you are ambitious!).\nBut this frustration actually becomes addictive!\n\n**Saving bytes** is your job - once you get your first working prototype, and inevitably blow your byte limit.\n\nWhen you reach that limit, a good idea is to practice an **\"add feature -\u003e remove code\"** development loop \nthat really makes you think hard about your ultimate goal, and helps improve your entry.\n\n\u003e **The JS1K-based development:**\n\u003e Adding more and more features,\n\u003e figuring out how to fill everything in,\n\u003e re-thinking your demo to only keep the essential features.\n\u003e This will keep making your demo better.\n\nYou have to make a very hard choice: ***Which feature to remove?***\nIt is all about budget, not in term of money but in term of bytes!\nA bit like in daily life: making choices with limited resources!\n\nJSCrush\n---\n\n***JSCrush*** is a crazy tool you may want to use to go deeper in the bytes reducing.\n\nIt basically implements a compression algorithm which is based on substring occurences.\nThe challenge of such a tool is not only to make a good compression but to make \na very small decompressor embedded in the result code because \nthis decompressor might be an overhead *(~ +60 bytes with small code)*.\n\nIf you are using *JSCrush* which I recommend for saving extra bytes,\nyou may want to use some tricks to go even further with it!\n\nThe first time, you usually can save about 20% of bytes with classic 1k minified code.\nBut if you optimize your code **for** *JSCrush*, you can save much more!\nI've achieved about a 40% code reduction in my demo!\n\nMost of the tricks is about finding code patterns (same succession of JavaScript source code characters) \nand trying to duplicate them.\n\nWhen I say **duplicating code**, it is really about **DUPLICATING code!**\n\n\u003e Once \"indexed\", a duplicated code is likely to just take one more byte in the final crushed JavaScript!\n\nSome tips and tricks\n===\n\n[\u003cimg src=\"/images/2014/03/js1k_2.png\" alt=\"\" class=\"thumbnail-right\" /\u003e][demo]\n\nThis section will share with you a non-exhaustive list of tricks.\nI'm not going to talk so much about the basic and classic ones, \nbut a few novel ones that I found to work well in my entry.\nYou may prefer to directly read the [annotated source code of \"Panzer Dragoon 1K\"][source] instead!\nOf course, most of those tricks work closely with the `| minify | jscrush` transformation.\n\n\u003e Careful! Some tricks might be counter-intuitive as first glance, \n\u003e again it ties-in with the way JSCrush is working.\n\nReduce your language\n---\n\nAll existing functions and properties are costly in bytes,\neach time you use another one, it definitely add bytes.\nTo save bytes, you have to limit your set of functions/properties to use\nor find ways to access them indirectly.\n\n  - **Use as few variables as you can**: this is valid in computer science in general: the best systems are those with the fewest possible variables (states). if one variable can be computed out of others, it should be removed. Also consider allocating some temporary variables to re-use like in an assembly registry (e.g. `i`, `j`).\n  - **Reduce the set of functions** you authorize yourself to use! You may just need to use \"fillRect\" for everything, or \"arc\". Also don't use both `Math.min` and `Math.max`, one can be implemented with the other.\n  - **Minimize the different values / colors** you are using (most of the time digits are fine, but `#RGB` colors are costy).\n\nDuplicated wins!\n---\n\n- Generally: try to **duplicate the exact same code everywhere**!\n\n- **Do not use explicit aliasing** like `M=Math` and `C=M.cos`, JSCrush does that job for you.\n- **Get rid of intermediary computation.** Prefer inline and duplicated computation over variable assigment.\n- Also, `a*(b+c)` might be more bytes than `a*b+a*c` if `a` is an expression. (but doesn't work in all cases)\n\n**A few examples:**\n\n```javascript\na = b+c; translate(a, a); // NOPE!\ntranslate(b+c, b+c); // YES!\n```\n\n```javascript\nsize = a+b+10; fillRect(x-size, y-size, 2*size, 2*size); // no please don't!\nfillRect(x-(a+b+10), y-(a+b+10), 2*(a+b+10), 2*(a+b+10)); // YEAH!\n```\n\n```javascript\nfillRect(10,10,20,20);\n...\nfillRect(9,9,18,18); // Can you afford to use 10,10,20,20 instead?\n```\n\nIn my demo, I was able to factorize some code. For instance the way I draw and update the x,y of my opponents and particles are the same duplicate chunk of code:\n\n```javascript\n    bga();\n    arc(\n      // Update\n      e[0] += e[3],\n      e[1] += e[4],\n      e[2],\n      0, 9);\n    fl();\n```\n\n- You sometimes can **save bytes by adding more code**! For instance, if you need `fillStyle` and `strokeStyle`, it may save bytes to always set both color at the same time! `fillStyle = strokeStyle = ...` even if you only need once.\n- Always **use the same `function parameters`**. In my game, I use `function(e){` everywhere even if I don't use that `e` in all my functions. This is saving a bunch of bytes with JSCrush.\n- **Here's a particularly crazy trick:** If you have different collections of complex objects, you can simply represent each item by a vector (an array) and figure out how you can make use the same indexes for the use-case.\n\nIn my game:\n\n```javascript\no = []; // an opponent: [ 0: x, 1: y, 2: health, 3: vx, 4: vy, 5: locked, 6: hitTime ]\np = []; // a particule: [ 0: x, 1: y, 2: size,   3: vx, 4: vy, 5: damage ]\n```\n\n- You also may find better way of managing collections. Instead of using `t.push(o)` to add, `t.splice(i, 1)` to remove, and `for(i=0;e=o[i];i++){...}` to iterate. I am using `t[Math.random()]=o` to add, `delete t[i]` to remove and `for(i in o){ e=o[i]; ... }` to iterate. It saved a lot of bytes if you already use `Math.random()` somewhere else! For-in loops are also quite short and can by used for other tricks (e.g. *Programmatical aliasing*).\n\n\u003cblockquote class=\"twitter-tweet\" lang=\"fr\"\u003e\u003cp\u003eMy \u003ca href=\"https://twitter.com/search?q=%23js1k\u0026amp;src=hash\"\u003e#js1k\u003c/a\u003e uses `t[Math.random()]=insert`, for-in loops and `delete t[i]` rather than push and splice. Saving bytes with jscrush\u003c/p\u003e\u0026mdash; Ga√´tan Renaudeau (@greweb) \u003ca href=\"https://twitter.com/greweb/statuses/439324052403277824\"\u003e28 F√©vrier 2014\u003c/a\u003e\u003c/blockquote\u003e\n\n\n- Use just **one letter variable names** (mangling variables won't work because they are in window scope, and IMHO it is better for you to write them by hand)\n- You will probably need to **initialize some variables**, but do it only if necessary (if you have `ReferenceError`) and **use the multi-assignment syntax**: `A = B = 0` if you can. You should never have constant variables, it saves bytes to directly use the value inline.\n- **`with(c){ ... }`** in your main loop may save bytes. It makes all functions and properties of c (the drawing context) in the scope.\n\nLanguage tricks\n---\n- **Never use `var`**, just put everything in `window`\n- **Programmatically aliasing `c`'s method** may save you a lot of bytes (or may not, you have to check!). You also have to find the code which suit the best your use case. Be careful about collision. Here is mine: `for (e in c) c[e[0]+e[2]+(e[6]||\"\")] = c[e];`\n- Do not waste ANY value returned by assignment and operators (i++, x=.., x+=...). I'm sure you can do it somewhere else!\nTypical example:\n\n```javascript\nx += vx; y += vy; /* ... */ fillRect(x, y, s, s); // NOPE!\n/* ... */ fillRect(x += vx, y += vy, s, s); // YES!\n```\n\n- Try to not separate update from drawing logic. Mixing them may save bytes.\n- You don't want to use `addEventListener`, just define listeners straight on window! e.g. `onclick = function(){...`\n\nMake your JS1K now!\n---\n\n[\u003cimg src=\"/images/2014/03/js1k_3.png\" alt=\"\" class=\"thumbnail-left\" /\u003e][demo]\n\nI'm really eager to see all JS1K entries \nbecause I usually enjoy reading people's code and \nespecially all the crazy tricks that I can learn from your code :-)\n\nThis article was just sharing a bunch of tricks which work for my entry,\nbut you will find much better tricks for your demo -\nso please do it and make your crazy work!\n\n\n---\n*Special thanks to [@mrspeaker](http://twitter.com/mrspeaker) for fixing my English*.\n","data":{"title":"Panzer Dragoon 1k","description":"Panzer Dragoon 1k is a 2D remake of Panzer Dragoon in 1k of JavaScript I made for JS1K 2014","thumbnail":"/images/2014/03/js1k.png","author":"Gaetan","layout":"post","tags":["gamedev","js1k","javascript","canvas"]}},{"id":"2014-01-12-promisify-your-games","year":"2014","month":"01","day":"12","slug":"promisify-your-games","content":"\n [promise]: /2013/07/q-a-promise-library/\n [ld]: http://www.ludumdare.com/compo/\n [game]: http://greweb.me/ld28/\n [qimage]: https://npmjs.org/package/qimage\n [qajax]: https://npmjs.org/package/qajax\n [submission]: http://www.ludumdare.com/compo/ludum-dare-28/?action=preview\u0026uid=18803\n [zanimo]: https://github.com/peutetre/Zanimo\n [npmjs]: https://npmjs.org/\n [github]: https://github.com/gre/ld28\n [app.js]: https://github.com/gre/ld28/tree/master/src/app.js\n\nOne month ago was the [LudumDare][ld] #28 gamejam theming *\"You Only Get One\"*.\n\n\u003ca href=\"http://greweb.me/ld28/\"\u003e\n  \u003cimg src=\"/images/2014/01/ld28.png\" alt=\"\" class=\"thumbnail-left\" /\u003e\n\u003c/a\u003e\n\nI [submitted][submission] a [mini-game][game] which ranked 105th out of 2064 entries and also 26th in the \"theme\" category.\n\nThis is of-course a web game implemented in JavaScript and using HTML and CSS.\n\nBut actually, my main goal was not really making a game done \nbut more about technically **making a state-of-the-art Promise-based game**.\n\nI think [Promises][promise] contains very interesting advantages in a game development design:\n*Resource loading managment*, *game scenes chaining*, *animations*... are some use-cases.\n\n\n* Checkout the [source code][github] on Github - [`src/app.js`][app.js] is the entry point\n* LudumDare entry is [here][submission].\n* [Play the Game][game].\n\n\n\u003c!--more--\u003e\n\n## FP in game development\n\nUsing some Functional Programming paradigm in game development is interesting,\nand here I'm just talking at least about the basic stuff:\n**Avoid globals, Minimize state variables**.\n\nI've written a few games where restarting the game without using `location.reload()` was a challenge\nbecause the game variable states was so spread everywhere!\n\nBy doing more FP, you can have this restart feature by design without need to \"reset\" all variables\nbecause your start function just takes everything it needs in parameter and restarting is just about re-calling that function.\n\n## Promises as first-class citizen\n\n### Game scenes chaining\n\nLike maybe 99% of games, my game has an **intro** (menu), a **main** scene and an **outro** scene (gameover / finish).\n\nWhen you develop a game with just one big `render()` loop,\nit easily becomes a pain when you want to add more steps to the scene,\nit doesn't scale and fastly become spaghetti code:\nyou tend to have to figure out in which state you are (or which part of the animation timeline) from the game state.\n\nHopefully, scene management is very easy to do with Promises:\n\n```javascript\nfunction start () {\n  return Q()\n    .then(intro)\n    .then(_.partial(runMiniGames, 20))\n    .then(outro);\n}\n\nQ.all([/*..something to load..*/])\n .then(start/*, ..*/) // start game when ready\n .done(); // just help Q to trigger errors if some.\n```\n\nHow beautiful to read! Call `intro` then run 20 mini-games then perform `outro`.\n\n#### No game state shared, pure functions\n\n**intro** is the menu screen where you can choose the game difficulty.\n**outro** is the game end screen where the final score is displayed.\nThere is however **no global variables shared**, those are just passed from one function to another.\n\nLet's look deeper in how it works:\n\n* The `intro()` function just returns a Promise resolved when the user made a \"difficulty\" choice. That Promise actually contains the difficulty (0, 1 or 2).\n* The `runMiniGames` function takes 2 parameters: the number of games and the difficulty. `_.partial(runMiniGames, 20)` is just an helper for making a 20 mini-games function which takes the difficulty in parameter. This difficulty is given by the previous Promise. The `runMiniGames` function returns a Promise of Score (integer).\n* That score is then fed into the `outro(score)` function which displays this score to the user.\n\n\u003e **TL;DR.** This is just about plumbing 3 functions together!\n\nCheckout also the [`runMiniGames` implementation](https://github.com/gre/ld28/blob/master/src/app.js#L24-L37).\n\n#### Speed up the development\n\nAnd you know what? It make development easier and faster because you can easily skip some part in any Promises chain:\n\n```javascript\nfunction start () {\n  return Q(0)\n    //.then(intro) // Directly jump to the games\n    .then(_.partial(runMiniGames, 20))\n    .then(outro);\n}\n```\n\nI used that a lot and not only for this part, but for all part during the game development.\n\n\n### Loading resources\n\nPromises also help you to wait resources before starting the game.\nYou don't have to make yet another loading library, Promise already are ready for that, \nand you can also have proper error managment or even \"progress\" loading display (Q has Progress event in a Promise).\n\nEach loading resource is a Promise and you can combine them all using `Q.all`.\n\nHere is an example using [Qajax][qajax] and [Qimage][qimage].\n\n```javascript\nQ.all([\n  Qajax(\"music.wav\").then(mapToAudio),\n  Qajax.getJSON(\"map.json\"),\n  Qimage(\"images/logo.png\"),\n  Qimage(\"images/textures.png\")\n]).spread(function (music, map, logo, textures) {\n  \n  // Start the game !\n\n}, function (error) {\n\n  // display proper error\n\n}, function (progressEvent) {\n\n  // maybe you want to display a loading progress bar \n  // with that third progress callback.\n\n});\n```\n\nHere is a similar example:\n\n```javascript\nvar musicPromise = Qajax(\"music.wav\").then(mapToAudio);\nvar mapPromise = Qajax.getJSON(\"map.json\");\nvar texturesPromise = Qimage(\"images/textures.png\");\nQ.all([ mapPromise, texturesPromise ]).spread(startGame, errorLoading);\n// because maybe you don't want to wait the music for starting the game:\nmusicPromise.then(function (a) { a.play(); });\n\nfunction startGame (map, textures) {\n  // ...\n}\nfunction errorLoading (e) {\n  // ...\n}\n```\n\n### Mini games workflow\n\nMy games is divided into a set of mini-games which are all independent but share a common interface.\nThis interface was quite a WIP at the end of the weekend development but it does the job.\n\nHere is the template I used for my game: [src/games/\\_template.js](https://github.com/gre/ld28/blob/master/src/games/_template.js).\n\nA Game instance has different methods, and especially `enter` and `leave` method which are call on game enter and on game leave. It also has a `.end` Promise which is resolved when the Game end.\n\n* A mini-game when solved gives a score depending on how well the user succeed it (through the `end` Promise).\n* A mini-game have a timeout and if the player doesn't terminate it, it passes to the next game without scores.\n\nThose `enter()` and `leave()` methods return Promise in order to be plugged in the game workflow (we can wait them to finish before moving to next state).\n\nFor instance, we don't start the game timeout before it actually starts (just wait the `enter()` Promise to be resolved) and also we don't switch to the next game before the `leave()` Promise is done).\n\nCheckout also the [`nextMiniGame` implementation](https://github.com/gre/ld28/blob/master/src/app.js#L51-L76).\nThe result of that function is the score of the mini-game and that we sum up all scores from the previous score.\n\n#### Composability\n\nThe `enter()` and `leave()` methods can be composed of animations which can themselves be composed of animations.\n\n**We can easily subdivided work into different level of Promises chain.**\nHere is a little schema to summary that composability:\n\n![](/images/2014/01/ld28_composition_schema.svg)\n\n### Promise Animations\n\nIn my game, all the animations are controlled with Promises more exactly using CSS3 Transitions \nvia **[Zanimo][zanimo] Promise library** because it fits my game (DOM-based game).\nThe fact that a Promise can be waited and chained **gives a powerful controls over CSS Transitions for making animations**.\n\nYou can easily trigger animations **one after another** for moving an element in multiple places.\nYou can also perform **multiple animations at the same times** (on 2 different elements) and **wait for both to finish**\nbefore triggering a third animation.\n\nSee for instance how `enter()` and `leave()` animations are done in mini-games.\n\nIn the animation ending the \"memo\" game I used concurrent animations:\nall memo cards are randomly moved out.\n\n```javascript\n/* // FYI\nCard.prototype.transform = function (x, y, scale, duration) {\n  return Zanimo.transition(this.el, \"transform\",\n    \"translate(\"+x+\"px, \"+y+\"px) scale(\"+scale+\")\", duration||0);\n};\n*/\n\nfunction animateOut (dispersion) {\n  return Q.all(_.map(cards, function (card) {\n    if (card.destroyed) return Q(); // no animation because card is destroyed\n    return Q()\n      .then(function(){\n        return card.transform(card.x, card.y, card.number === 1 ? 1 : 0.8, 100);\n      })\n      .delay(Math.floor((card.number===1 ? 500 : 0)+300*Math.random()))\n      .then(function () {\n        var x = Math.round((Math.random()\u003c0.5 ? -card.w/dimensions.width-dispersion*Math.random() : 1+dispersion*Math.random())*dimensions.width);\n        var y = Math.round((Math.random()\u003c0.5 ? -card.h/dimensions.height-dispersion*Math.random() : 1+dispersion*Math.random())*dimensions.height);\n        return card.transform(x, y, 0, 500);\n      });\n  }));\n}\n\n// Usage in Memo.leave() :\nreturn Q.delay(50)\n  .then(function(){ return animateOut(0.5); })\n  .delay(100);\n```\n\n\nIn the calculation game I used a chain of animations subdivided in functions:\n\n```javascript\nreturn Q.delay(50)\n  .then(fadeOutInvalids)\n  .then(displaySolution)\n  .then(displayEquality)\n  .delay(500)\n  .then(fadeOut)\n  .then(hideEquality)\n  .delay(200);\n```\n\n[Full code here](https://github.com/gre/ld28/blob/master/src/games/calculation.js#L411-L500).\n\n### \"Wait for next click\"\n\nWhile my game are just based on click user interaction,\nI've made a [`waitNextClick`](https://github.com/gre/ld28/blob/master/src/waitNextClick.js) function\nwhich returns a Promise of click for the given element.\n\n```javascript\nvar Q = require(\"q\");\n\nmodule.exports = function waitNextClick (btn) {\n  var d = Q.defer();\n  btn.addEventListener(\"click\", function listener (e) {\n    btn.removeEventListener(\"click\", listener);\n    d.resolve(e.target);\n  }, false);\n  return d.promise;\n};\n```\n\nThis was quite an interesting solution which is just like a jQuery \"once\" event but in Promise paradigm.\n\nI was able to combine that function with `Q.race` which wait for one of the given Promise to be redeemed.\n\n\u003e `Q.race(_.map(btns, waitNextClick))`\n\nFor instance in the cats game, I just wait the first \"This one\" button to be clicked:\n\n```javascript\nvar houseChoice = Q.race(_.map(this.houses, function (catHouse) {\n    return waitNextClick(catHouse.btn)\n    .then(function () {\n      return catHouse;\n    });\n// houseChoice is a Promise of House choosen by the player.\n```\n\n### Using the \"progress\" event\n\nI also used a bit the \"progress\" event of a Q Promise, which is a way to notify that a Promise is being resolved.\n\n* I used that \"progress\" event on the `game.end` Promise for notifying that the player is winning some scores in a mini-game while playing.\n* I also used it to make a timeout ticking the remaining time before the timeout is reached and that Promise resolved.\n\nSee both usages [here](https://github.com/gre/ld28/blob/master/src/app.js#L62-L70):\n\n```javascript\nreturn Q.race([\n  gameEnd\n    .progress(function (score) {\n      stats.setScore(totalScore+score);\n    }),\n  timeoutWithTicks(gameEnd, timeout)\n    .progress(stats.setTimeProgress)\n    .then(_.bind(game.submit, game))\n]);\n```\n\n## Code organization using NPM + Browserify\n\nNPM \u0026 Browserify has also been used because I find this stack very productive,\nespecially when writing a game from scratch.\n\nBrowserify has been trendy the last past year, but there is here an interesting way of organizing your code\nand especially reusing it.\nYou can find a lot of [available modules using NPM][npmjs], \nBrowserify will just make you able to require them using `require(\"modulename\")`.\n\n","data":{"title":"Promisify your games","description":"a game showcase using Q Promise as first-class citizen and driven with CSS3 Animations via Zanimo.","thumbnail":"/images/2014/01/ld28.png","author":"Gaetan","layout":"post","tags":["gamedev","promise","Q","ludumdare"]}},{"id":"2013-11-12-functional-rendering","year":"2013","month":"11","day":"12","slug":"functional-rendering","content":"\n[Gamelier]: http://gamelier.org/gaetan-renaudeau-on-procedural-vs-functional-rendering/\n[slides]: http://greweb.me/prez-functional-rendering\n\nI've done a talk at **[Gamelier][Gamelier]** last Monday about \nhow to think the **rendering** in a more **functional** way.\n\n## Abstract\n\nMost of today 2D graphics libraries restrict us to a set of primitive procedures (`drawRect`, `drawCircle`, `drawImage`,...) but when it comes to bring more interesting features you tends to be stuck with it. Let's see how we can just do things with a function of `(Vec2 =\u003e Color)`.\n\nThis is the way (*WebGL*) **GLSL** has already took and the presentation examples will be built on it.\nLet's see what are the multiple benefits of taking that paradigm of rendering.\n\n## Talk\n\n\u003ciframe src=\"//player.vimeo.com/video/78804695?title=0\u0026amp;byline=0\u0026amp;portrait=0\" width=\"600\" height=\"337\" frameborder=\"0\" webkitallowfullscreen mozallowfullscreen allowfullscreen\u003e\u003c/iframe\u003e\n\n### [Open the slides][slides]\n\n## Checkout more presentations at Gamelier.org\n\n[![](/images/2013/11/gamelier.png)][Gamelier]\n","data":{"title":"Functional Rendering","description":"This talk explains how to think the rendering in a more functional way. Let's see how we can just do things with a function of (Vec2 =\u003e Color).","author":"Gaetan","layout":"post","tags":["functional","rendering","GLSL"]}},{"id":"2013-09-28-webaudioapi","year":"2013","month":"09","day":"28","slug":"webaudioapi","content":"\n\u003ca href=\"http://greweb.me/webaudioapi-introduction\"\u003e\n\u003cimg src=\"/images/2013/09/webaudioapiprez.png\" alt=\"\" class=\"thumbnail-left\" /\u003e\n\u003c/a\u003e\n\n\u003ca href=\"http://greweb.me/webaudioapi-introduction\"\u003eOpen the presentation\u003c/a\u003e\n/\n\u003ca href=\"http://greweb.me/webaudioapi-introduction?azerty=1\"\u003eAZERTY version\u003c/a\u003e\n","data":{"title":"Slides: Web Audio API, Overview","description":"A presentation overview of the Web Audio API","author":"Gaetan","layout":"post","tags":["audio"]}},{"id":"2013-09-17-timelapse","year":"2013","month":"09","day":"17","slug":"timelapse","content":"\n[webaudioapi]: https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html\n[glslheroku]: http://glsl.heroku.com/\n[glsl.js]: /2013/02/glsl-js-a-javascript-glsl-library-dry-efficient/\n[js13kgames]: http://js13kgames.com/\n[beez]: /2013/09/beez\n[fm]: /2013/08/FM-audio-api\n[zound]: /2013/08/zound-wip-v1/\n[entry]: http://js13kgames.com/entries/timelapse\n[github]: https://github.com/gre/js13k\n\n\u003cimg src=\"/images/2013/09/timelapse.png\" alt=\"\" class=\"thumbnail-left\" /\u003e\n\nWhile continuing to experiment with [Web Audio API][webaudioapi] and [GLSL][glsl.js],\nI've made **[a game called Timelapse][entry]** for [js13kgames][js13kgames]\n(an HTML5 game competition where entries must be less than 13 kb zipped).\n\nThis article is a **postmortem overview of my game development** which will try to explain\nwhat was my game mecanism ideas and show you some interesting parts with **screenshots, audios and source code snippets**.\n\n## The Game\n\n[Open the game on js13kgames][entry] / [github][github].\n\n**The game intends to work on Desktop and Mobile**.\nHowever, *Chrome* is recommended \n(*Firefox Aurora* also supports it but audio is a bit wrong, but Mozilla devs should improve this [soon](https://twitter.com/padenot/status/375924494537195520)).\nToday, it works on *Android Chrome Beta* on a Nexus 4, unfortunately with some clicks in the audio (Web Audio API is bleeding-edge).\n\n\u003c!--more--\u003e\n\n## Experimenting with stuff\n\nLast months, I've been playing with Web Audio API and released a few experiments like \n[Beez][beez], [FM Synthesis][fm] and [Zound][zound].\n\nMy game development started last weekend as an experiment, I tried to make some **dubstep-like sound**, \nstarting with a **[\"Wob Wob Wob\" sound](http://jsfiddle.net/greweb/CrXYw/4/)**.\n\nI also started to dig into [glsl.heroku.com][glslheroku], \nI definitely wanted to make some **cool and unusual graphics with glitchy style** to fit with dubstep audio part, \nI unfortunately hadn't enough deepen this glitchy part as I would have liked.\n\nGLSL quite fits this need: it mays look strange and hard code language as a start \nbut **it's very easy and free to do anything with it**. \nI used my [glsl.js][glsl.js] wrapper to easily have a shader rendering the whole Canvas.\n\n\u003e GLSL is a totally different way of thinking the rendering: \nthe main principle is to define **a function which returns a Color for a given Position**. \nI use to call it \"Functional Rendering\" in opposite to \"Procedural Rendering\". I'll talk again about those concepts soon.\n\nI started my graphics by forking [this very interesting glow effect](http://glsl.heroku.com/e#10795.2).\n\n## Prototyping the game ideas\n\nThen I started to really think about the game I could do, \nI sketched some game mecanisms and thought about the gameplay.\n\nMy game was designed to be a **one-button** [DDR](https://en.wikipedia.org/wiki/Dance_Dance_Revolution)-like **rhythm game**\nwith the main idea that **the user controls the speed** *(the BPM, beats per minute)* of the song.\nI wanted an **inertia system**: tap a bit early and your song will speed up, tap a bit late and the song will slow down.\nThis speed freedom isn't without constraints: You can reach a gameover if your speed isn't enough fast, or contrariwise if it is too fast (like a overheat).\n\n\u003e The game listen to your inputs to adapt the song BPM.\n\nThe game is basically about SPACE-typing on each beat, but also introduce some **freestyle \"dubstep\" phase**\n*(It's not really dubstep though!)*: the gameplay is either typing on the key like hell or holding and releasing some \"riff\". \n\nThe score mecanisms give good scores for very precise beats and will be negative for very bad/loss rhythms.\nDuring the freestyle section, each action gives a score, also each riff (holding the button for more than 1 tick) gives a score. Making small riffs has been designed to give more points that a very long riff so you have to find the good balance.\n(The score also increases at the end of each freestyle phase).\n\n\u003cimg src=\"/images/2013/09/highscores.png\" style=\"max-width: 300px; width: 50%\" /\u003e\n\nI also had to find a game end, I first thought about trying to make the game harder and harder but it wasn't trivial to make\nbecause I wanted to keep my *\"player is free to take any speed he wants\"* idea.\n\nInstead, I chose to **limit the game time by one minute**, \nwhich makes my game a psychedelic rush game if you want a good score: \nA good strategy to make a good score is to first speed up the song inertia as fast as possible, and then keep the rhythm on an high BPM.\n\n\u003e That mecanism is interesting because it is also harder to make precise scores on higher speed, it can even be risky (reaching the BPM limit, failing some beats), the player has to find the speed it fits the most!\n\n## The game experience\n\nI wanted my game experience to be both on the **graphics** and on the **audio** aspects:\nyou have both a feedback on your actions with the graphics using a color (\n\u003cspan style=\"color:#0F0\"\u003egreen=good\u003c/span\u003e,\n\u003cspan style=\"color:#CC0\"\u003eyellow=meh\u003c/span\u003e,\n\u003cspan style=\"color:#F00\"\u003ered=bad\u003c/span\u003e\n) and with the audio (different sound depending on the rate of the action).\n\n\u003cimg src=\"/images/2013/09/good.png\" style=\"width: 30%\" /\u003e\n\u003cimg src=\"/images/2013/09/timelapse.png\" style=\"width: 30%\" /\u003e\n\u003cimg src=\"/images/2013/09/toofast.png\" style=\"width: 30%\" /\u003e\n\nThe audio BPM is also graphically visualized using a circle with a rotating pulse\nwhich also helps you on the rhythm.\n\nDuring the freestyle phase, the circle turns fully highlighted and the audio \"wob wob wob\" part is playing.\nEach user freestyle \"riff\" (hold a note) will randomly change the delay of a \"repeater\", an important part on the audio of that section I will discuss in the *Audio Section*.\n\n\u003cimg src=\"/images/2013/09/killer-riff.png\" style=\"max-width: 300px; width: 100%\" /\u003e\n\nIf the player runs the song too fast, an overheat happens and the circle turns very light:\n\n\u003cimg src=\"/images/2013/09/lighted.png\" style=\"max-width: 300px; width: 100%\" /\u003e\n\nOn the contrary, it turns very dark and glitchy when the BPM is very slow:\n\n\u003cimg src=\"/images/2013/09/slow.png\" style=\"max-width: 300px; width: 100%\" /\u003e\n\n## More about the audio\n\nAs described in the [specification][webaudioapi], the *Web Audio API* is an audio routing graph composed of low level audio nodes.\nUsing it raw can be quite verbose, I've made my own reusable component using those nodes.\nMy convention is to use Javascript constructor for those components and to have a \"inp\" and an \"out\" *AudioNode* field.\n\nFirst I create a `ctx` *AudioContext* (works for Chrome \u0026 Firefox Aurora):\n\n```javascript\nvar ctx = new (window.AudioContext || window.webkitAudioContext)();\n```\n\nThis `ctx` variable now offers all methods to work with sound.\n\n### Global effects: Reverbation, Compressor\n\nWeb Audio API have a **Convolver** node which allows to make diverse audio effects like **reverbation**, which is basically emulating your song played in a room. You can find more information [here](http://creativejs.com/resources/web-audio-api-a-bit-more-advanced/).\n\nI've used a simple Reverb effect to pass the whole sound. This simple reverb implementation can be found on [https://github.com/web-audio-components/simple-reverb](https://github.com/web-audio-components/simple-reverb).\n\nAnother **very** important brick of the Audio graph is the **Compressor**.\nThe [Web Audio API][webaudioapi] have a built-in Compressor with some parameters.\n\nA compressor dynamically adapts the input sound to a normalized output. It ensures the output is not distorted (saturated because amplitude is too high) or inaudible because too low.\nIn other words, it consists of dynamically raise the volume if the input is lower, and decrease the volume if the input is higher, that a given rate.\n\nHere is the global audio setup I've used as an output for all different sounds of the song:\n\n```javascript\n  var out = ctx.createGain(); // My global output\n  var outCompressor = ctx.createDynamicsCompressor();\n  var reverb = new Reverb(0.5);\n  out.gain.value = 0; // We will increase the main volume when the song starts\n  out.connect(reverb.inp);\n  reverb.out.connect(outCompressor);\n  outCompressor.connect(ctx.destination);\n```\n\n### Ambiant sounds\n\nI've used a soft **sine Oscillator** and some **Noise generator** (protected by a bandpass Filter) for the **ambiant sound**.\nThat gives more depth to the song.\n\nIt was also used to give more audio feedback on the gameplay:\n\n* The **Oscillator frequency follows the BPM** (goes higher in frequency with the song speed).\n* The BPM also affects the **frequency of a LFO** which oscillate the **volume of the Noise** to make an **helicopter-like sound**.\n* The Oscillator is fastly **detuned on each user action**, and especially if the user tap too early it will produce a \"bip\" like you can hear in the following Soundcloud.\n* Finally, a second noise passed into a highpass filter will be louder if the player is in danger (BPM is too slow or too fast). *(we won't show the code for this one)*\n\nI have muted all other sounds to make you hear only the ambiant sound when speeding up the song up to a gameover:\n\n\u003ciframe width=\"100%\" height=\"166\" scrolling=\"no\" frameborder=\"no\" src=\"https://w.soundcloud.com/player/?url=http%3A%2F%2Fapi.soundcloud.com%2Ftracks%2F110774935\"\u003e\u003c/iframe\u003e\n\nThe Noise component:\n\n```javascript\n  function Noise () {\n    // Here we loop on a 2s noise buffer, it is more efficient that generating on the fly\n    var bufferSize = 2 * ctx.sampleRate,\n    noiseBuffer = ctx.createBuffer(1, bufferSize, ctx.sampleRate),\n    output = noiseBuffer.getChannelData(0);\n    for (var i = 0; i \u003c bufferSize; i++) {\n      output[i] = Math.random() * 2 - 1;\n    }\n    var whiteNoise = ctx.createBufferSource();\n    whiteNoise.buffer = noiseBuffer;\n    whiteNoise.loop = true;\n\n    var gain = createGain();\n    whiteNoise.connect(gain);\n\n    var filter = ctx.createBiquadFilter();\n    gain.connect(filter);\n    filter.type = \"lowpass\"; // Generally lowpass, but can be overrided\n\n    this.white = whiteNoise;\n    this.gain = gain;\n    this.out = this.filter = filter;\n  }\n\n  Noise.prototype = {\n    start: function (time, duration) {\n      this.white.start(time, 0, duration);\n    }\n  };\n```\n\nHere is some code I used for making the ambiant sound:\n\n```javascript\n  var bpmOsc2mult = 3;\n  var bpmNoiseMult = 10;\n  var noiseBpmGain = ctx.createGain();\n  noiseBpmGain.connect(out);\n  var noiseBpm = new Noise();\n  noiseBpm.out.connect(noiseBpmGain);\n  noiseBpm.start(0);\n  noiseBpm.gain.gain.value = 0.2;\n  noiseBpm.filter.type = \"bandpass\";\n  noiseBpm.filter.Q.value = 20;\n  noiseBpm.filter.frequency.value = 0;\n\n  var bpmNoiseLfoMult = 0.05;\n  var bpmNoiseLfoPow = 1.3;\n  var lfoBpm = ctx.createOscillator();\n  lfoBpm.start(0);\n  var lfoBpmGain = ctx.createGain();\n  lfoBpmGain.gain.value = 0.8;\n  lfoBpm.connect(lfoBpmGain);\n  lfoBpmGain.connect(noiseBpmGain.gain);\n\n  var osc2 = new OscGain();\n  osc2.type = \"sawtooth\";\n  osc2.osc.frequency.value = vars.bpm * bpmOsc2mult;\n  osc2.osc.detune.value = 5;\n  osc2.gain.gain.value = 0.1;\n  osc2.out.connect(out);\n  osc2.start(0);\n```\n\n### NOTES\n\nTo easily define melodies, I first define \"NOTES\", a map of `note -\u003e frequency`. \nFor instance `NOTES.A4` is `440` Hz:\n\n```javascript\nvar NOTES = (function () {\n  var notes = {};\n  var toneSymbols = \"CcDdEFfGgAaB\";\n  function noteToFrequency (note) {\n    return Math.pow(2, (note-69)/12)*440; // Beauty of audio math!\n  };\n  for (var octave = 0; octave \u003c 8; ++octave) {\n    for (var t = 0; t \u003c 12; ++t) {\n      notes[toneSymbols[t]+octave] = noteToFrequency(octave * 12 + t);\n    }\n  }\n  return notes;\n}());\n```\n\nMy convention here is to use a cap for major notes like `D` and no cap for minor notes like `d` (the black keys on a Piano).\nThe following number is the octave. Notes are defined with two characters: Like A1, C2, B3, a4, ...\nYou will see that's quite convenient to use the `with(NOTES){ ... }` syntax.\n\nSee Also [Frequencies of notes](https://en.wikipedia.org/wiki/Frequencies_of_notes).\n\n### FM Synth \"bass\" melody\n\nI used some [FM synth][fm] for making the main \"bass\" melody:\n\n\u003ciframe width=\"100%\" height=\"166\" scrolling=\"no\" frameborder=\"no\" src=\"https://w.soundcloud.com/player/?url=http%3A%2F%2Fapi.soundcloud.com%2Ftracks%2F110622269\"\u003e\u003c/iframe\u003e\n\n\nFirst, I made a \"OscGain\" and a \"FM\" components.\n\n```javascript\n  function OscGain (t) {\n    this.osc = ctx.createOscillator();\n    if (t) this.osc.type = t;\n    this.out = this.gain = ctx.createGain();\n    this.osc.connect(this.gain);\n  }\n  OscGain.prototype = {\n    start: function (time, duration) {\n      this.osc.start(time, 0, duration);\n    }\n  };\n\n  function FM () {\n    OscGain.call(this);\n    this.mod = new OscGain();\n    this.mod.out.connect(this.osc.frequency);\n  }\n  FM.prototype = {\n    start: function (time, duration) {\n      this.osc.start(time, 0, duration);\n      this.mod.start(time, duration);\n    }\n  };\n```\n\nAnd used this melody:\n\n```javascript\nwith (NOTES) {\n  bassMelo = [G4,D4,F4,C4];\n}\n```\n\n```javascript\n  // Usage for the bass:\n  var bass = new FM();\n  bass.out.connect(out);\n  function tick (i, time) {\n    // ...\n    // Change the note each 4 tick\n    var oscFreq = bassMelo[Math.floor(i/4) % bassMelo.length];\n    bass.osc.frequency.value = oscFreq * 2.0;\n    bass.mod.osc.frequency.value = oscFreq * 0.5;\n    bass.mod.gain.gain.value = oscFreq * 0.5;\n    // ...\n  }\n```\n\n\u003e **N.B.**:\n\u003e The modulator frequency is 1/4 of the oscillator frequency which gives a cool bass sound. \u003cbr/\u003e\n\u003e Also, That tick function is called at a (variable) frequency of `60 / BPM` Hz (BPM means Beat Per Minute, here it's more a Tick Per Minute) with the tick number `\"i\"` and the tick time `\"time\"`.\n\n### FM Synth \"main\" melody\n\n\u003ciframe width=\"100%\" height=\"166\" scrolling=\"no\" frameborder=\"no\" src=\"https://w.soundcloud.com/player/?url=http%3A%2F%2Fapi.soundcloud.com%2Ftracks%2F110620277\"\u003e\u003c/iframe\u003e\n\nThis \"main\" synth is also a Frequency Modulation, but using a 3/4 ratio on the modulator frequency,\nand with an envelope on each notes.\n\n```javascript\nwith (NOTES) {\n  melo1 = [E3,G3,D3,G3,E3,A3,C3,G3];\n  melo2 = [E3,B3,D3,G3,E3,C4,C3,D3];\n}\n```\n\nMaking an envelope consists of scheduling the amplitude through time with a *Gain*. See my [FM Article][fm].\n\n```javascript\n  function envelope (gainNode, time, volume, duration, a, d, s, r) {\n    var gain = gainNode.gain;\n    gain.cancelScheduledValues(0);\n    gain.setValueAtTime(gain, 0, time);\n    gain.linearRampToValueAtTime(volume, time + a);\n    gain.linearRampToValueAtTime(volume * s, time + a + d);\n    gain.setValueAtTime(volume * s, time + a + d + duration);\n    gain.linearRampToValueAtTime(0, time + a + d + duration + r);\n  }\n```\n\nAlso, the melody periodically switch into \"arpeggio note\" with this function:\n\n```javascript\n  var DELTAS = [\n    Math.pow(2, 0),\n    Math.pow(2, 1),\n    Math.pow(2, 2)\n  ];\n\n  function applyArpeggio (freqParam, baseFreq, time, duration, arpDuration, deltas) {\n    if (!deltas) deltas = DELTAS;\n    var length = deltas.length;\n    var ranges = [];\n    cancelScheduledValues(freqParam, 0);\n    for (var t = 0, i = 0; t \u003c= duration; t += arpDuration, i = (i+1) % length) {\n      setValueAtTime(freqParam, baseFreq * deltas[i], time + t);\n    }\n  }\n```\n\nThe Arpeggio effect is about fastly changing some octaves higher (like C3,C4,C5,C3,C4,C5,... very fastly).\nI've keeped that \"deltas\" a parameter to try other arpeggios, I've only used `[1,2,4]` multiplicators in the game.\n\nIndeed, thanks to the magic of audio math, \nincrementing the octave means multipling the frequency by 2,\nmore generally increment by N octaves means multiplying by `2 ^ N`.\n\nFinally, here is \"meloNote\", the function which triggers one melody note.\n\n```javascript\n  function meloNote (noteFreq, time, arpeggio, metallic) {\n    var fm = new FM();\n    var duration = 0.3;\n    var release = 0.1;\n    fm.osc.type = \"triangle\";\n    fm.osc.frequency.value = 4 * noteFreq;\n    fm.mod.osc.frequency.value = 3 * noteFreq;\n    fm.mod.osc.type = \"sine\";\n    fm.out.connect(meloOut.inp);\n    setTimeout(function () {\n      fm.out.disconnect(meloOut.inp);\n    }, 1000);\n    startNode(fm, time, 0, 1);\n    arpeggio \u0026\u0026 applyArpeggio(fm.osc.frequency, 4 * noteFreq, time, duration+release, 0.025);\n    envelope(fm.gain, time, 0.5, duration, \n        0.01, 0.02, 0.6, 0.2);\n    envelope(fm.mod.gain, time, 4 * noteFreq * metallic, duration, \n        0.05, 0.1, 0.6, 0.2);\n  }\n```\n\n\u003e **N.B.** The metallic parameter is a parameter from 0 to 1 to give a more metallic sound. \nIt changes the modulator intensity. In fact, that 3/4 ratio on the FM is the reason metallic sound.\n\nThis function is called each tick with a new note:\n\n```javascript\nfunction tick (i, time) {\n  // ...\n  var r = risk(); // How the player is in danger\n  var metallic = 0.4 * r + 0.3 * smoothstep(-1, 1, Math.cos(Math.PI * i / 16));\n  var melo = i % 16 \u003c 8 ? melo1 : melo2;\n  var octave = i % 32 \u003c 16 ? 0 : 1;\n  var m = melo[i % 8] * (1 \u003c\u003c octave);\n  meloNote(m, time, meloIsArpeggio, metallic);\n  // ...\n}\n```\n\n\n### Repeater of freestyle part\n\nA **\"repeater\" with random delay add crazyness in the freestyle section**. The delay time is randomly changed each time you hold the key so it gives cool feedback.\n\n\u003ciframe width=\"100%\" height=\"166\" scrolling=\"no\" frameborder=\"no\" src=\"https://w.soundcloud.com/player/?url=http%3A%2F%2Fapi.soundcloud.com%2Ftracks%2F110612559\"\u003e\u003c/iframe\u003e\n\nThe *Repeater* is made of a *Delay* piped in a *Gain* and piped back in the delay input to produce feedback (echo).\nA particularity of this component is the input *Gain* is also the output *Gain*.\n\n![](/images/2013/09/repeater_schema.png)\n\nImplementation of a Repeater:\n\n```javascript\n  function Repeater (delayValue, repeatGainValue) {\n    var out = ctx.createGain();\n    var delay = ctx.createDelay(1); // The Max Delay\n    delay.delayTime.value = delayValue;\n    out.connect(delay);\n    var repeatGain = ctx.createGain();\n    repeatGain.gain.value = repeatGainValue;\n    delay.connect(repeatGain);\n    repeatGain.connect(out);\n    this.delay = delay;\n    this.repeater = repeatGain;\n    this.gain = this.inp = this.out = out;\n  }\n```\n\n### Playing with Stereo\n\nDoing stereo with Web Audio API can be a bit verbose without wrapping it,\nhere is the Stereo component:\n\n```javascript\n  function Stereo (left, right) {\n    var merger = ctx.createChannelMerger();\n    var inp = ctx.createGain();\n    inp.connect(left.inp);\n    inp.connect(right.inp);\n    this.inp = inp;\n    left.out.connect(merger, 0, 0);\n    right.out.connect(merger, 0, 1);\n    this.left = left;\n    this.right = right;\n    this.out = merger;\n  }\n```\n\n### Drumbox\n\n**The Drumbox is simply made of a Kick, a Snare and a Hihat.**\n\n\u003ciframe width=\"100%\" height=\"166\" scrolling=\"no\" frameborder=\"no\" src=\"https://w.soundcloud.com/player/?url=http%3A%2F%2Fapi.soundcloud.com%2Ftracks%2F110781486\"\u003e\u003c/iframe\u003e\n\nA **Snare** is implemented with a Noise and a Filter:\n\n```javascript\n  function Snare (volume, freqFrom, freqTo) {\n    var noise = new Noise();\n    noise.filter.type = \"lowpass\";\n    noise.filter.Q.value = 5;\n    noise.gain.gain.value = 0;\n    this.noise = noise;\n    this.out = noise.out;\n    this.volume = volume || 1;\n    this.freqFrom = freqFrom || 800;\n    this.freqTo = freqTo || 1000;\n    this.release = 0.3;\n  }\n\n  Snare.prototype = {\n    trigger: function (time) {\n      this.noise.start(time, 1);\n      envelope(this.noise.gain, time, this.volume, 0.05, \n          0.01, 0.03, 0.25, this.release);\n      var f = this.noise.filter.frequency;\n      f.setValueAtTime(this.freqFrom, time);\n      f.linearRampToValueAtTime(this.freqTo, time+0.1);\n    }\n  };\n```\n\nThe **HiHat** is also made with a Noise and a Filter, \nexcept the Filter is a highpass filter (only high frequency are audible).\n\nFinally, The **Kick** is made with a `Kicker` and a `Snare`.\n\nHere is the Kicker implementation:\n\n```javascript\n  function Kicker (freq, attack, duration, fall) {\n    OscGain.call(this);\n    this.gain.gain.value = 0;\n    this.osc.frequency.value = freq;\n    this.freq = freq || 50;\n    this.fall = fall || 0;\n    this.attack = attack || 0;\n    this.duration = duration || 0;\n    this.volume = 1;\n  }\n\n  Kicker.prototype = {\n    start: function (time, duration) {\n      startNode(this.osc, time, 0, duration);\n    },\n    trigger: function (time) {\n      var a = this.attack, d = this.attack + 0.06, s = 0.8, r = 0.1;\n      this.start(time, this.duration + 1);\n      envelope(this.gain, time, this.volume, this.duration, a, d, s, r);\n      setValueAtTime(this.osc.frequency, this.freq, time);\n      linearRampToValueAtTime(this.osc.frequency, 0, time + this.fall);\n    }\n  };\n```\n\nAnd finally, here is my \"kick\" method called each time a user press the key:\n\n```javascript\n  kick: function (t, errorRate) {\n    errorRate = errorRate * errorRate * errorRate;\n    var freq = mix(100, 120, errorRate);\n    var speed = mix(0.2, 0.3, errorRate) * 100 / vars.bpm;\n    var kick = new Kicker(freq, 0.01, speed, speed);\n    kick.volume = 1.5;\n    kick.osc.type = \"sine\";\n    var filter = ctx.createBiquadFilter();\n    filter.frequency.value = mix(200, 300, errorRate);\n    filter.Q.value = 10 + 10 * errorRate;\n    kick.out.connect(filter);\n    filter.connect(drumOut.inp);\n    setTimeout(function () {\n      filter.disconnect(drumOut.inp);\n    }, 1000);\n    kick.trigger(t);\n\n    var snare = new Snare(0.5, 1000, 10);\n    snare.out.connect(drumOut.inp);\n    setTimeout(function () {\n      snare.out.disconnect(drumOut.inp);\n    }, 1000);\n    snare.trigger(t);\n\n    E.pub(\"kick\", t);\n  }\n```\n\n\n### Stereo Drumbox\n\nI've used two Repeaters (one for the left, on for the right) on the Drumbox to produce some stereo echo effects.\n\nThe effect can be very weak to hear, so I made in the following audio example 2 different delays so you can understand what I mean:\n\n\u003ciframe width=\"100%\" height=\"166\" scrolling=\"no\" frameborder=\"no\" src=\"https://w.soundcloud.com/player/?url=http%3A%2F%2Fapi.soundcloud.com%2Ftracks%2F110622238\"\u003e\u003c/iframe\u003e\n\nAll sounds from the drumbox (snare, hihat, kick) is piped into \"drumOut\", which have that Stereo system.\n\nHere is the code of the \"drumOut\" (the output where goes all Drum Box sounds):\n\n```javascript\n  var drumOut = (function () {\n    // using the second example delay effect (listen to the soundcloud sound)\n    var left = new Repeater(0.1, 0.5);\n    var right = new Repeater(0.2, 0.7); // Playing with different values for stereo effects\n    right.gain.gain.value = 0.8; // move the drum a bit to the left\n    return new Stereo(left, right);\n  }());\n  drumOut.out.connect(out);\n```\n\n## The GLSL shader\n\nHere is the GLSL code I used for the game, the final version is a bit crazy because I incrementally add features to it!\nAll the graphics are defined here!\n\n```glsl\n#ifdef GL_ES\nprecision mediump float;\n#endif\n\n#define BPM_MIN 30.0\n#define BPM_MAX 150.0\n\nuniform vec2 resolution;\nuniform float time;\nuniform float kick;\nuniform float kickSpeed;\nuniform float bpm;\nuniform float lvl;\n\nuniform bool dubstepAction;\nuniform float useraction;\nuniform float successState;\n\nuniform float dubloading;\nuniform bool dubphase;\nuniform float pulseOpenFrom;\nuniform float pulseOpenTo;\n\nconst vec2 center = vec2(0.5, 0.5);\n\nconst float PI = 3.14159265359;\nconst float PI_x_2 = 6.28318530718;\n\nconst vec3 COLOR_NEUTRAL = vec3(0.1, 0.2, 0.7);\nconst vec3 COLOR_SUCCESS = vec3(0.0, 0.7, 0.1);\nconst vec3 COLOR_ERROR = vec3(0.7, 0.0, 0.05);\n\nfloat expInOut (float a) {\n  return 0.0==a ? 0.0 : 1.0==a ? 1.0 : 1.0 \u003e (a *= 2.0) ? 0.5 * pow(1024.0,a-1.0):0.5*(-pow(2.0,-10.0*(a-1.0))+2.0);\n}\n\nfloat random (vec2 pos) {\n  return fract(sin(dot(pos.xy ,vec2(12.9898,78.233))) * 43758.5453);\n}\nvec3 random3 (vec2 pos) {\n  return vec3(\n    random(pos),\n    random(pos*3.),\n    random(pos*13.)\n  );\n}\n\nfloat distanceRadius (float a, float b) {\n  float d = mod(distance(a, b), PI_x_2);\n  return d \u003c PI ? d : PI_x_2 - d;\n}\n\nfloat spiralDistance (vec2 v, float r) {\n  float d = length(v);\n  float a = (PI + atan(v.x, v.y))/PI_x_2;\n  float n = log(d/r)+a;\n  return distance(1.0, 2.0 * smoothstep(0.0, 1.0, fract(n)));\n}\n\nfloat bpmToSec (float bpm) {\n  return 60. / bpm;\n}\n\nfloat circlePulse (\n  vec2 v, float kickForce,\n  float kickGlitchFreq, float kickGlitchAmp,\n  float thin, float pulseAngle, bool dubphase,\n  float waveFreq, float waveAmp, float waveDuration,\n  float bullForce\n) {\n  float angle = atan(-v.x, -v.y);\n  float clock = distanceRadius(0.0, angle) / PI;\n  float distAngle = distanceRadius(angle, PI_x_2 * pulseAngle) / PI;\n  float f = mix(1.0, smoothstep(-1.0, 1.0, cos(kickGlitchFreq * (clock+0.1*angle+kickForce))), kickGlitchAmp);\n  float r = mix(0.35, 0.2, kickForce*f);\n  float sc = smoothstep(1.0-waveDuration, 1.0, distAngle);\n  float intensity = 0.1+0.05*sc;\n  r /= mix(0.95, 1.0, waveAmp*sc*cos(angle*waveFreq));\n  float a = mod(PI_x_2+atan(v.x, v.y), PI_x_2)/PI_x_2;\n  float ring = abs(length(v)-r) - 0.03*bullForce*(!dubphase ? \n    smoothstep(1.0-1.5*waveDuration, 1.0, clock) : \n    (\n    a \u003c pulseOpenFrom ? smoothstep(0.05, 0.0, distance(a, pulseOpenFrom)) : \n    a \u003e pulseOpenTo ? smoothstep(0.05, 0.0, distance(a, pulseOpenTo)) : \n    1.0\n    )\n  );\n  float value = smoothstep(0.0, intensity, ring);\n  float returnValue = 1.0/sqrt(abs(value))/1.0 * pow(thin, 2.);\n  if ( length(v) \u003c r) {\n    float sr = PI;\n    float s = spiralDistance(v, sr);\n    float a = (PI + atan(v.x, v.y))/PI_x_2;;\n    float v = \n      smoothstep(0.02, 0., distanceRadius(PI+pulseAngle*PI_x_2, a*PI_x_2)/PI) *\n      smoothstep(0.2, 0., s);\n    returnValue += v * 2.0;\n    s = 1.0 - pow(smoothstep(0.0, 0.3, s), 0.3);\n    returnValue += s;\n  }\n  float centerIntensity = dubphase ? 0.1 : 0.1*dubloading;\n  if (centerIntensity \u003e 0.0) {\n    float s = bpmToSec(bpm);\n    float c = mix(1.0, 10.0, mod(time, s)/s) * smoothstep(centerIntensity, 0.0, length(v));\n    returnValue += c;\n  }\n  return returnValue;\n}\n\nvoid main (void) {\n  vec3 c = vec3(0.0);\n  vec2 p = gl_FragCoord.xy / resolution;\n  float sec = bpmToSec(bpm);\n  float statePower = smoothstep(0.8, 0.0, time-useraction);\n  float colorPower = dubstepAction ? 1.0 : statePower;\n  float cPulse = circlePulse(\n    p - center,\n    smoothstep(kickSpeed, 0.0, time-kick),\n    20.0,\n    0.5,\n    0.5 + 0.5 * smoothstep(smoothstep(0.6, 1.0, statePower), 0.0, distance(smoothstep(0.8, 1.0, statePower), distance(p, center))),\n    mod((time-kick)/sec, 1.0),\n    dubphase,\n    1.2*sqrt(bpm) + 4.0*statePower,\n    2.0,\n    min(0.5, bpm / 800.0),\n    1.0 - statePower\n  );\n  vec3 mainColor = mix(\n    COLOR_NEUTRAL,\n    mix(COLOR_ERROR, COLOR_SUCCESS, successState),\n    colorPower);\n  \n  c += cPulse * mainColor;\n\n  c = clamp(\n    c,\n    vec3(0.05, 0.05, 0.05),\n    vec3(1.0, 1.0, 1.0)\n  );\n\n  float bpmLight = smoothstep(BPM_MIN, BPM_MAX, bpm);\n  c = mix(c * (0.5 * random(p + time) + 0.5 * random(floor(p * 100.) + 0.01*time) - 0.5 * random(floor(p * 10.) + time)), c, min(1.0, 15.0*bpmLight));\n\n  c *= 0.1 + max(0.95, 100.0*(bpmLight-0.85));\n\n  gl_FragColor = vec4(c, 1.0);\n}\n```\n\n\n## Bonus\n\n**Did you recognize the melody I used in the freestyle part?**\nThe melody doesn't keep the rhythm though, but you should be able to recognize it!\n\n\u003ciframe width=\"100%\" height=\"166\" scrolling=\"no\" frameborder=\"no\" src=\"https://w.soundcloud.com/player/?url=http%3A%2F%2Fapi.soundcloud.com%2Ftracks%2F110614352\"\u003e\u003c/iframe\u003e\n","data":{"title":"Making a rhythm game with bleeding-edge web","description":"While continuing to experiment with Web Audio API and GLSL, I've made a game called Timelapse for js13kgames (an HTML5 game competition where entries must be less than 13 kb zipped).","thumbnail":"/images/2013/09/timelapse.png","author":"Gaetan","layout":"post","tags":["js13k","GLSL","audio","gamedev"]}},{"id":"2013-09-04-beez","year":"2013","month":"09","day":"04","slug":"beez","content":"\n[webaudioapi]: https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html\n[zenexity]: http://zenexity.com\n[github]: http://github.com/gre/beez\n[app]: http://beez.greweb.fr/\n[webrtc]: http://www.webrtc.org/\n[webrtcapi]: http://www.w3.org/TR/webrtc/\n[websocketapi]: http://www.w3.org/TR/websockets/\n[fm_article]: /2013/08/FM-audio-api\n[playframework]: http://playframework.com/\n\n\u003cimg src=\"/images/2013/09/beez.png\" alt=\"\" class=\"thumbnail-left\" /\u003e\n\nHere is **Beez**, a web real-time audio experiment \nusing smartphones as synthesizer effect controllers.\n\nThis is our second [Web Audio API][webaudioapi] experiment made in one Hackday at [Zenexity][zenexity] (now Zengularity).\n\nThis time, we were much more focused on having the best **latency performance**:\nwe used the bleeding-edge [WebRTC][webrtcapi] technology,\nwhich allows you to link clients in Peer-to-Peer instead of a classical Client-Server architecture.\n\n* [The project on Github][github]\n* [Test it now!][app] (Chrome)\n\n\n### Live demo of the Hackday application\n\n\u003ciframe width=\"640\" height=\"480\" src=\"//www.youtube.com/embed/QwU6IMNLF0o\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\n*Bonus for the one who recognizes the melody :-)*\n\n\u003c!--more--\u003e\n\n## The Experiment\n\nThe experiment consists in controlling an audio stream running on a desktop web page with \nsome audio effect pads running on phones via a mobile web interface.\n**Our main goal was to make the best real-time experience.**\n\n### Hive and Bees\n\nAn **Hive** is controlled by different **Bees**, eventhing connected in Peer-to-Peer (via WebRTC).\n\n#### The Hive\n\nThe **Hive** is a web page where the sound is generated and visualized *(Web Audio API)*.\nIt also shows you in real-time the different effects XY pads and allows you to control them.\n\n![](/images/2013/09/hive.png)\n\n#### A Bee\n\nThe **Bee** is a mobile web page which allows you to control the different sound effects with XY pads.\nIt only works on Android Chrome now *(WebRTC required)*.\n\n![](/images/2013/09/bee.png)\n\n### Audio tech\n\nWe used [Web Audio API][webaudioapi] for generating the sound client-side on the Hive:\n\nWe have a note sequencer which plays a ***famous melody*** through a [Frequency Modulator][fm_article] and different other effects.\nSome controls allow you to change the **BPM**, **gain** of the carrier and the modulator, **finetune**, \nfrequency **multiplicator** (0.25, 0.5, 1, 1.5, 2) of the carrier and the modulator,\n**reverbation**, **filter** (frequency and resonance).\nThere is also a delay effect made on both left and right channels to produce a cool stereo effect.\n\nThat's quite basic audio stuff so far, I can't wait to experiment deeper and try to generate more complex sounds with that awesome Audio API.\nAgain, our main goal was to make a P2P connection between the hive and its bees.\n\n### Network architecture\n\n\u003cimg src=\"/images/2013/09/beez_arch.png\" alt=\"\" class=\"thumbnail-left\" /\u003e\n**Every bee are connected to the hive with a bi-directionnal Peer-to-Peer connection thanks to [WebRTC][webrtc].**\n\nEverytime a (bee) user moves an effect controller with his phone, a position event is sent to the hive.\n\nBasically:\n\n```javascript\nxyAxis.on(\"change:x change:y\", function () {\n  hive.send([\"tabxy\", this.get(\"tab\"), this.get(\"x\"), this.get(\"y\")]);\n});\n```\n\n*As you can see, we used Backbone.js models for events.*\n\nHowever, A lot of Touch events per second can be triggered by an Android device, and it may depends on the device speed. We shouldn't send to the network all of these events because it can saturate it and cause some lags. To avoid that we need to **throttle the touch events before sending the event to the network**.\n\nThis is done transparently with the [`_.throttle`](http://underscorejs.org/#throttle) function and we choose to **throttle by 50 milliseconds** which is **about 20 events per second** which is ok for human eye.\n\n```javascript\nxyAxis.on(\"change:x change:y\", _.throttle(function () {\n  hive.send([\"tabxy\", this.get(\"tab\"), this.get(\"x\"), this.get(\"y\")]);\n}, 50));\n```\n\nWe use different other events:\n\n- A bee can send `\"tabxychanging\"` and `\"tabopen\"` respectively to informs the cursor has been pressed/released and to inform a new tab has been opened.\n- When a Hive receive a `\"tabopen\"`, it will send back to the bee a `\"tabxy\"` event in order to inform what is the current value of that tab so we can init the cursor to the current xy axis position on the bee interface.\n\n## WebSockets vs WebRTC\n\n\u003cimg src=\"/images/2013/09/websocket.png\" class=\"thumbnail-left\" /\u003e\n\nMost \"real-time\" web experiments you see on the Internet today use [WebSockets][websocketapi].\nWebSockets are good, it's a significant evolution from the Ajax years.\n\nWebSocket is a protocol on top of TCP, which **links a browser with a server in a bidirectional text communication**.\nGetting 2 clients to communicate generally consists in broadcasting messages from the server to all clients \n(see the schema).\n\n**This architecture has some advantages:**\n\n* Simple to understand, Easy to use.\n* We can easily implement some server validation.\n\n**But also has some drawbacks:**\n\n* Not always easy to traverse **proxies**. *(e.g. through an nginx front server)*\n* Only text communication.\n* **bandwith** intensive. *(all the bandwidth goes back and forth with your server)*\n* **CPU** intensive. *(e.g. receiving 20 messages per second from 10 clients can be a lot for a small server, especially if you are doing some message processing)*\n\n(The last two \"cons\" are scalability issues)\n\n\u003chr style=\"clear:both\" /\u003e\n\n\u003cimg src=\"https://upload.wikimedia.org/wikipedia/commons/a/ac/Logo-webrtc.png\" class=\"thumbnail-left\" /\u003e\n\n[WebRTC][webrtc] (*Web Real Time Communication*), \nis a new web technology which helps to connect browsers in a **Peer to Peer** way.\n\nWebRTC has been designed for transfering binary data like files, audio, video (e.g. a webcam stream).\nOf-course, we can still use it for text.\n\n\u003cbr style=\"clear:both\" /\u003e\n\n\u003cimg src=\"/images/2013/09/webrtc.png\" class=\"thumbnail-right\" /\u003e\n\nUnlike WebSockets, multiple steps are required to **establish a P2P connection between two web clients**.\nIt is due to the fact that the two clients must resolve the closest network path to communicate with each other.\nThat resolving phase requires a communication between the clients, and for that we can use WebSockets as a *\"Control Channel\"*.\n\nBut once the two web clients are connected, they basically don't need the web server anymore and can **communicate directly together**.\nIf the two clients are in the same local network, they should directly communicate through that local network.\nThat **reduces the server load** and should significantly **decrease the latency**.\n\n### Playframework / Akka Actors\n\n[Playframework][playframework] has been used on the server side for making that WebSocket Control Channel,\nand akka was convenient for handling peer communication and rooms management.\n\n## About the melody\n\nYou still didn't guess where does the melody came from?\n\nWell, here is the answer:\n\n\u003ciframe width=\"640\" height=\"480\" src=\"//www.youtube.com/embed/3rU_ei_x0Ag\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\n## Awesome team!\n\nFinally I want to thanks my team-mates: [@mrspeaker](http://twitter.com/mrspeaker), [@etaty](http://twitter.com/etaty), [@NicuPrinFum](http://twitter.com/NicuPrinFum), [@drfars](http://twitter.com/drfars), [@srenaultcontact](http://twitter.com/srenaultcontact)\nwith who we were able to make that demo from scratch in one day!\n","data":{"title":"Beez, WebRTC + Audio API","description":"Here is Beez, a web real-time audio experiment using smartphones as synthesizer effect controllers. This is our second Web Audio API experiment made in one Hackday at Zenexity.","thumbnail":"/images/2013/09/beez.png","author":"Gaetan","layout":"post","tags":["WebRTC","audio","hackday"]}},{"id":"2013-08-21-FM-audio-api","year":"2013","month":"08","day":"21","slug":"FM-audio-api","content":"\n [zoundarticle]: /2013/07/zound-live/\n [zoundrepo]: http://github.com/gre/zound-live/\n [zoundfm]: https://github.com/gre/zound-live/blob/master/app/assets/javascripts/modules/SimpleFM.js\n [fmwiki]: http://en.wikipedia.org/wiki/Frequency_modulation_synthesis\n\nThe main principle of [Frequency Modulation (FM)][fmwiki] is to **pipe an Oscillator (the Modulator)\ninto the frequency of another Oscillator (the Carrier)**.\n\nThis article will explain to you how FM Synthesis works with **interactive demos**.\nIn the meantime, all demos are implemented with the brand new **Web Audio API**,\nso feel free to hack the code for your own purpose.\n\nThis article will also introduce some Audio concepts like **LFO**, **Envelope** and **Finetuning**.\n\nI've recently implemented a very first FM in [ZOUND live][zoundarticle] - *a HTML5 collaborative audio tracker*,\ngiving much more powerful Synthesizers (see in the following video).\n\n\u003ciframe width=\"640\" height=\"480\" src=\"//www.youtube.com/embed/El4JvaDWQUM\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n[*(here is the implementation of that FM)*][zoundfm]\n\n\n\u003c!--more--\u003e\n\n## Dive into Frequency Modulation Synthesis\n\nAs mentioned previously, FM is about **piping an Oscillator (the \u003cu\u003eModulator\u003c/u\u003e) into the frequency of another Oscillator (the \u003cu\u003eCarrier\u003c/u\u003e)**.\n\nThe Modulator oscillation only affects the oscillation frequency of the Carrier but is not directly an audio signal.\n\n![](/images/2013/08/fm_principle.png)\n\nThe result of that modulation differs depending on each oscillator **frequency** and **amplitude**:\n\n[![](/images/2013/08/Frequencymodulationdemo-td.png)](http://en.wikipedia.org/wiki/File:Frequencymodulationdemo-td.png)\n\n***N.B.*** *Our interactive demos in this article will always play a sound and visualize it (waveform / spectrum analyzer).\nYou will have different kind of controls depending on each specific aspect I want to illustrate.*\n\n*The demos should work on Chrome. __However if you get an AudioContext failure, please reload the page__ (you may not be able to start them all in one row).*\n\n### LFO\n\n**Low-Frequency Oscillation (LFO)** is very used in electronic music for making rythmic audio effects.\n\nLFO is simply a specific subset of a oscillator in a sense that **its oscilation frequency is under\nthe human audible range (20 Hz)** and is then not generally used as an audio signal but as an effect controller.\n\nFor instance the frequency / the amplitude of an oscillator, or in the following example the frequency of the cut-off filter:\n\n\u003caudio src=\"http://upload.wikimedia.org/wikipedia/commons/e/e4/Lfo-cutoff-frequency-wobble-bass.ogg\" controls\u003e\u003c/audio\u003e\n\nNow, as a first demo,\nlet's see what happens if our **FM Modulator is an LFO**,\n*(i.e. if that Modulator is in low frequency range)*.\n\n\u003ciframe width=\"100%\" height=\"310\" src=\"http://fiddle.jshell.net/FvnJx/58/show/light/\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\"\u003e\u003c/iframe\u003e\n\u003ca href=\"http://jsfiddle.net/FvnJx/58/\" target=\"_blank\" style=\"display: block; text-align: right\"\u003eOpen on jsfiddle\u003c/a\u003e\n\nObserve in the Carrier graphs how **the waveform is regulary compressed and decompressed**. If you increase the Modulator frequency, it will speed up this effect. A real FM is about speeding up that effect up to the audible range...\n\n***N.B.*** *With _Web Audio API_ (more generally with any modular synthesizers) we can easily control any module parameter with an LFO:*\n\n```javascript\nlfo.connect(carrier.frequency);\n```\n\n#### Modulator in audible range\n\nNow, if we increase the frequency to the hearing range, here is what happens:\n*(in that example you can also change the Carrier frequency)*\n\n\u003ciframe width=\"100%\" height=\"310\" src=\"http://fiddle.jshell.net/x4CWR/36/show/light/\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\"\u003e\u003c/iframe\u003e\n\u003ca href=\"http://jsfiddle.net/x4CWR/36/\" target=\"_blank\" style=\"display: block; text-align: right\"\u003eOpen on jsfiddle\u003c/a\u003e\n\n\nIt's as if that **once the Modulator reaches that audible barrier, it kind of becomes a second audible synthesizer**,\neven if it only modulate the frequency of the actual synthesizer.\nHowever, it's completely different than playing the two synthesizers directly into the output,\nagain the modulator influence the frequency of the carrier and is not directly piped into the output audio signal.\n\n*There is especially cool sound produced when the Modulator frequency is closed to the Carrier frequency. For more infos, see the \u003cu\u003eFinetuning\u003c/u\u003e section.*\n\n\n### Frequency ratios: harmonic or dissonant sounds\n\nOne thing you may also have notice in the previous example is that most of the generated sounds was quite dissonant, non harmonic.\n\nNow, if we add more restrictions and only **snap the possible modulator frequencies**\nto a **multiple of the carrier frequency**, here is what happens:\n\n\u003ciframe width=\"100%\" height=\"310\" src=\"http://fiddle.jshell.net/Euezv/17/show/light/\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\"\u003e\u003c/iframe\u003e\n\u003ca href=\"http://jsfiddle.net/Euezv/17/\" target=\"_blank\" style=\"display: block; text-align: right\"\u003eOpen on jsfiddle\u003c/a\u003e\n\nThis harmonic result is due a simple fact in music: **Mutiplying a note frequency by 2 is equivalent to Increasing that note by one octave,** meaning the note has the same tone but is one-octave higher. (and vice versa for the division). *BTW, you may have noticed that fact by repetition of peaks in the previous example Spectrum Visualization.*\n\nNow we can release some restrictions by also allowing frequencies multiple of `carrier frequency / 4`, which means allowing to increase/decrease by an **octave**, a **semi-octave** or a **quarter-of-octave**.\n\n\u003ciframe width=\"100%\" height=\"310\" src=\"http://fiddle.jshell.net/DFSwN/13/show/light/\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\"\u003e\u003c/iframe\u003e\n\u003ca href=\"http://jsfiddle.net/DFSwN/13/\" target=\"_blank\" style=\"display: block; text-align: right\"\u003eOpen on jsfiddle\u003c/a\u003e\n\n\n*Eventually you could even allow more freedom using multiple of `carrier freq / 12`, because an octave is equally divided by 12 in the [Chromatic scale](http://en.wikipedia.org/wiki/Chromatic_scale).*\n\n### Mixing the power of the Modulator effect\n\nA very interesting part of the job is also to change the **amplitude of the modulator**. So far, we used a full amplitude modulating the carrier frequency from 0 to 2-times its original frequency which produces a quite rough sound.\n\nTry to change the modulator amplitude on the following demo:\n\n\u003ciframe width=\"100%\" height=\"310\" src=\"http://fiddle.jshell.net/DAT5S/12/show/light/\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\"\u003e\u003c/iframe\u003e\n\u003ca href=\"http://jsfiddle.net/DAT5S/12/\" target=\"_blank\" style=\"display: block; text-align: right\"\u003eOpen on jsfiddle\u003c/a\u003e\n\nTechnically, we can easily control that range to any by changing the gain of the Modulator with a `GainNode` which is just a tool to scale the amplitude of a signal.\n\n### Envelope\n\nNow, we need to add an **Envelope** for automating that amplitude change you just experiment with.\n\nAn envelope in electronic music will generally look like this:\n\n[![](/images/2013/08/500px-ADSR_parameter.svg.png)](http://en.wikipedia.org/wiki/File:ADSR_parameter.svg)\n\nAn Envelope corresponds to a **note lifespan**.\nIt is the minimum required for making our Synth.\n\nWe will generally **automate that amplitude through time for each note triggered**.\n\nHere is a demo.\nPlay, try to hold and release a note (using the Play button or SPACE), and observe how the Spectrum Analyzer is moving:\n\n\u003ciframe width=\"100%\" height=\"400\" src=\"http://fiddle.jshell.net/tyEKr/32/show/light/\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\"\u003e\u003c/iframe\u003e\n\u003ca href=\"http://jsfiddle.net/tyEKr/32/\" target=\"_blank\" style=\"display: block; text-align: right\"\u003eOpen on jsfiddle\u003c/a\u003e\n\n**Two different envelopes** has been used: one for the **Modulator** and one for the **Carrier** which produce **different sound effects in a note lifespan**.\n\n*We won't make an interactive demo for changing these envelope parameters,\nbut you can try them in the ZOUND project (or see again the video).*\n\n### Finetuning\n\nAnother interesting effect occurs **when the frequency of the Modulator is very close to the frequency of the Carrier**.\nIn the following example, we have set both oscillators to the same frequency but we expose a \"detune\" parameter which allows to change a bit the frequency of the Modulator.\n\n\u003ciframe width=\"100%\" height=\"400\" src=\"http://fiddle.jshell.net/X95S6/10/show/light/\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\"\u003e\u003c/iframe\u003e\n\u003ca href=\"http://jsfiddle.net/X95S6/10/\" target=\"_blank\" style=\"display: block; text-align: right\"\u003eOpen on jsfiddle\u003c/a\u003e\n\nYou can slightly notice that a sound is regulary looping like if it was an LFO effect. You can also visualize it on the graph.\n\nThis effect corresponds to the **[phase](http://tinyurl.com/nzkus8) change between both oscillators**: it regulary change from **\"in-phase\"** state (where it have exactly the same sine waveform at the same time) to a desynchronize **\"out-of-phase\"** (because of the small detune), and then slightly go to the next \"in-phase\" step. More the frequencies are close, more it takes time to oscillate from phase to phase.\n\nThis effect is especially awesome when you start mixing multiple synths together and finetune a bit each one so they don't sound exactly on the same frequency.\n\n### Modulating the Modulator\n\nThere is so much more possibilites to play with,\nfor instance, the previously introduced Envelope could be mixed\nwith an LFO to change the Modulator effect in a rythm,\nbut now let's see how we can...\n\n**...modulate the modulator!**\n\nEventually we can make a stack of modulators and use different kind of waveforms\nfor more powerful effects:\n\n![](/images/2013/08/fm_multiple.png)\n\n\u003e Be careful when playing with stack of modulators, it is quite easy to have saturated or noisy sounds.\n\nAs an example, I made this experiment which randomly takes different frequencies and amplitude for a stack of 5 modulators:\n\n[**-\u003e http://jsfiddle.net/s2MMR/45/ \u003c-**](http://jsfiddle.net/s2MMR/45/)\n\nCareful! this experiment is a bit crazy! but it shows how different patterns can be when playing with FM.\n\n\u003c!-- TODO soon...\n## Last demo, polished FMs playing a famous song...\n\nAs a last demo example, and in a more readable \u0026 simple code, here is a polished example of FM.\n--\u003e\n\n----\n\nAlso, **If you are interested by ZOUND live, [fork it on Github][zoundrepo].**\n","data":{"title":"Frequency Modulation (FM) with Web Audio API","description":"","author":"Gaetan","layout":"post","tags":["fm","audio"]}},{"id":"2013-08-08-zound-wip-v1","year":"2013","month":"08","day":"08","slug":"zound-wip-v1","content":"\nHere is a preview video of the Work In Progress development of [ZOUND live](/2013/07/zound-live/),\nthe **collaborative modular audio tracker** built with bleeding edge **web technologies**: *Web Audio API, Web MIDI API, WebSocket \u0026 Playframework*.\n\n\u003ciframe width=\"640\" height=\"480\" src=\"//www.youtube.com/embed/621dpTK8OOc\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\u003cimg src=\"/images/2013/07/nanokontrol.jpg\" alt=\"\" class=\"thumbnail-left\" style=\"width: 150px\"\u003e\n\n**N.B.** In the video, I've used the nanoKONTROL2 for changing some properties in real time and using the MIDI API.\n\n----\n\n**If you are interested by the project, [fork it on Github](https://github.com/gre/zound-live).**\n\nMore to come, stay tuned!\n","data":{"title":"ZOUND live v1 in development","description":"Here is a preview video of the Work In Progress development of ZOUND live, the collaborative modular audio tracker built with bleeding edge web technologies: Web Audio API, Web MIDI API, WebSocket \u0026 Playframework.","author":"Gaetan","layout":"post","tags":["MIDI","audio","zound"]}},{"id":"2013-08-05-zanimo","year":"2013","month":"08","day":"05","slug":"zanimo","content":"\n [0]: /pages/a-world-of-promises/\n [1]: http://t.co/OeSukzxv3F\n [2]: http://github.com/42loops/Zanimo.js\n [3]: http://twitter.com/42loops\n [4]: http://github.com/kriskowal/q\n\n# A [World Of Promises][0], episode 4\n\n\u003cimg src=\"/images/2013/07/zanimo_animation_thumbnail.png\" alt=\"\" class=\"thumbnail-left\" style=\"width: 200px\" /\u003e\n\n*This fourth article on [Q][1] will introduce [Zanimo.js][2], \na Promisified animation library which helps to chain different\nCSS transitions with only Promises.\nIt is very interoperable with any other Promise library,\nmeaning that you can easily chain Zanimo animations with other asynchronous actions.*\n\n[Zanimo.js][2] is a smooth library developed by [@42loops][3] animation library based on the library [Q][4], a Javascript implementation of Promises.\n\nThis article is an introduction of the concept of Promises in Javascript with the library Q and will show a real use case of it: Promises used on top CSS Transitions to make powerful and efficient DOM animations in Javascript.\n\n\u003c!--more--\u003e\n\n## Zanimo.js\n\nTODO: overview of the library\n\n## Examples\n\n### Recursive tree particle animation\n\n[![tree-particle](/images/2013/05/tree-particle.png)][1]\n\nIt turns out that promise is very good tool for animating any recursive things.\n\nTODO\n","data":{"published":false,"title":"Qep4.: Zanimo.js, a promise-based animation library","description":"Zanimo.js, a promise-based animation library","thumbnail":"/images/2013/07/zanimo_animation_thumbnail.png","author":"Gaetan","layout":"post","tags":["AWOP","javascript","promise","Q","animation","library"]}},{"id":"2013-07-30-zound-live","year":"2013","month":"07","day":"30","slug":"zound-live","content":"\n[zound]: /2012/08/zound-a-playframework-2-audio-streaming-experiment-using-iteratees/\n[webmidiapi]: http://webaudio.github.io/web-midi-api/\n[webaudioapi]: https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html\n[tracker]: http://en.wikipedia.org/wiki/Tracker_(music_software)\n[zenexity]: http://zenexity.com\n\n\u003cimg src=\"/images/2013/07/nanokontrol.jpg\" alt=\"\" class=\"thumbnail-left\" /\u003e\n\nLast week, I initiated, with my [Zenexity][zenexity] Hackday team, **\"ZOUND live\"**\nfollowing the previous [\"ZOUND\"][zound] experiment but being much more ambitious this time:\nusing both the **Audio API**, the _new_ **MIDI API** and electronic music software experience,\nwe start our own **web collaborative audio modular tracker**.\n\n### Live demo of the Hackday application\n\n\u003ciframe width=\"640\" height=\"480\" src=\"//www.youtube.com/embed/uyHWhCnE4L0\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\u003c!--more--\u003e\n\n## Inspiration\n\nA lot of features have been inspired from existing software like _SunVox_ or _Renoise_.\nHowever, our version uses 100% web technologies and add collaborative and real time aspects.\n\n\u003cimg src=\"/images/2013/07/sunvox.png\" style=\"max-width: 300px\" /\u003e\n\n### Our Tracker\n\nThe application has a [tracker][tracker] where you can put notes.\n\n\u003cimg src=\"/images/2013/07/tracker.png\" style=\"max-width: 300px\" /\u003e\n\n### Our Audio modules\n\nThe application integrates a [modular music](http://en.wikipedia.org/wiki/Modular_software_music_studio) concepts.\n\n\u003cimg src=\"/images/2013/07/nodeeditor.png\" /\u003e\n\n## The web techs\n\n### About Web MIDI API\n\nWe bought a few **cheap MIDI controllers to interact with our application**.\n\n\u003cimg src=\"/images/2013/07/midicontrollers.jpg\" class=\"thumbnail-right\" style=\"max-width: 250px\" alt=\"\" /\u003e\n\nMIDI means _Musical Instrument Digital Interface_,\nit is the **protocol** used by a lot of electronic musical instruments for a few decades.\n\nThe [Web MIDI API](webmidiapi) is a recent specification which makes MIDI devices accessible from a web page,\nvia a Javascript API.\n\nRecently, _Chrome_ has started to [implement it](https://code.google.com/p/chromium/issues/detail?id=163795)\nand it is available under _Chrome Canary_ (the dev version) via a _flag_ that you need to enable.\n\nThis is the perfect time to start experimenting it!\n\nHowever, what I feared the most happened on the Hackday: **the MIDI API was broken on the morning\nafter a Chrome update during the night!** A first version of a browser MIDI permission was implemented\nbut I never succeeded to make it working. The state of the API seems to be still broken on Mac as of writing.\n\n\u003cblockquote class=\"twitter-tweet\"\u003e\u003cp\u003e\u003ca href=\"https://twitter.com/greweb\"\u003e@greweb\u003c/a\u003e If you know how to build chromium, I may be able to provide a patch to enable it. But it isn't so long until Canary supports it.\u003c/p\u003e\u0026mdash; „Å®„Çà„Åó„Åæ (@toyoshim) \u003ca href=\"https://twitter.com/toyoshim/statuses/360685543778041857\"\u003eJuly 26, 2013\u003c/a\u003e\u003c/blockquote\u003e\n\u003cscript async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"\u003e\u003c/script\u003e\n\nWell, that was already too late for the Hackday,\nFortunately we fallbacked on an alternative which relies on a Java applet to access MIDI devices, it was a laggy polyfill though...\n\n**Lesson learned:** a nightly feature is a nightly feature, never assume features you add via flags are stable _(I never did, but it was a Hackday afterall!)_.\n\nBTW, cheers to \u003ca href=\"https://twitter.com/toyoshim\"\u003e@toyoshim\u003c/a\u003e who is implementing the MIDI API in Chrome :-)\n\n### Using Web Audio API\n\nThe [Web Audio API][webaudioapi] is _a high-level JavaScript API for processing and synthesizing audio in web applications_.\n\nThe good thing about this API: it is already an **modular audio API**, so it's not so hard to build a modular audio application on top of it!\n\n### Playframework\n\n[Playframework](http://playframework.com/) has been used for **broadcasting events\nbetween clients via WebSocket and synchronize everything on the interface**.\nIt is only broadcasting and does not save the song yet.\n\n### Backbone.js\n\n[Backbonejs](backbonejs.org) was used for the **models**, **views** and its nice **event system**.\nIt was a good library for prototyping and architecture the different parts of the application.\n\nI found Backbone.js especially good when linking all parts together and especially for the network logic.\nThis leads to a very reactive style of programming:\n\n\u003cscript src=\"https://gist.github.com/gre/6107277.js\"\u003e\u003c/script\u003e\n\n## The team\n\n**This project has been started during our monthly Hackday at [Zenexity][zenexity],\nI want to thank my 6 awesome coworkers for being part of the project:**\n\n- [@mrspeaker](http://twitter.com/mrspeaker) for his awesome electronic music knowledge.\n- [@bobylito](http://twitter.com/bobylito) for his brilliant ideas and his JavaScript skills.\n- [@mandubian](http://twitter.com/mandubian) for his playframework experience and JSON superpower!\n- [@etaty](http://twitter.com/etaty) for helping with the server synchronization.\n- [@skaalf](http://twitter.com/skaalf) for his cool DrumBox module.\n- [@Noxdzine](http://twitter.com/Noxdzine) for his talentuous design.\n\nThis was actually my first real project managment and it was quite cool!\n\n**Hackday is only one day** and such an ambitious project is hard to achieve one in a row,\nthe project architecture needed to be a bit ready and having a PoC working before the Hackday. Also I wanted everyone to have fun by experimenting with the Audio API parts and not to be blocked on boring parts.\n\nAs a team manager, I also had to define goals to achieve for the Hackday.\n\nWoo, I realize that's **not an easy task to manage a team when running out of time!**\n\nBut fortunately, I think we fulfilled it just in time!\n\nWe ended the Hackday with a **Real Time demonstration of our application** with 4 people interacting together\nwith MIDI controllers.\n\n## More to come!\n\nToday, we have a **first working version of a\ncollaborative tracker with basic modular audio features**:\n\n- MIDI note support + MIDI control assignation allowing to change module properties.\n- a unique tracker with a 32 lines loop and 23 tracks.\n- Synchronisation of everything: the tracker and modules for all connected clients.\n- off-mode allowing one user to prepare a track which is muted for other users.\n- play/pause and record mode!\n- cursor of users displayed on the tracker.\n\n**Stay tuned because there is so much features to come!**\n\n[**The project on Github**](http://github.com/gre/zound-live)\n","data":{"title":"ZOUND live project initiated","description":"Last week, I initiated ZOUND live following my previous \"ZOUND\" experiment but being much more ambitious this time: using both the Audio API, the new MIDI API and electronic music software experience, we start our own web collaborative audio modular tracker.","thumbnail":"/images/2013/07/nanokontrol.jpg","author":"Gaetan","layout":"post","tags":["MIDI","audio","hackday","zound"]}},{"id":"2013-07-17-QanimationFrame","year":"2013","month":"07","day":"17","slug":"QanimationFrame","content":"\n [0]: /pages/a-world-of-promises/\n [1]: http://github.com/gre/qanimationframe\n [2]: https://dvcs.w3.org/hg/webperf/raw-file/tip/specs/RequestAnimationFrame/Overview.html\n [3]: http://creativejs.com/resources/requestanimationframe/\n [4]: http://www.paulirish.com/2011/requestanimationframe-for-smart-animating/\n\n# A [World Of Promises][0], episode 3\n\n\u003cimg src=\"/images/2013/07/qanimationframe.jpg\" alt=\"\" class=\"thumbnail-left\" style=\"width: 200px\" /\u003e\n\n*This third article on [Q][1] is a little parenthesis to the Qep articles series,\nfeaturing the `requestAnimationFrame` Javascript function and its general usage,\nand [QanimationFrame][1], its Promisified version used as a \"wait for DOM to be ready\" API.*\n\n\u003c!--more--\u003e\n\n## `requestAnimationFrame`\n\n`requestAnimationFrame` is a function which **delays a Javascript function execution to the next browser render frame**.\nIt takes one argument in parameters which is **the function to call on next repaint**.\n*(N.B. there is not anymore a second DOM parameter like a few months ago, see the [spec][2])*\n\n### ...for animation loop\n\n`requestAnimationFrame` helps to easily make a **render loop**:\n\n```javascript\n(function loop(){\n  requestAnimationFrame(loop);\n  render();\n}());\n```\nIn that example, the `render` function can contains any Javascript code which updates\nsome graphics either using Canvas or DOM.\n\nA good practice is to always **compute time-relative** animations and \nnever assume the framerate to be constant.\n\n```javascript\nfunction badRenderFunction() {\n someObject.x += 0.1; // 10 pixels per 100 frame.\n // not so good with non-constant framerate\n}\n```\n\n```javascript\nvar lastTime = Date.now();\nfunction goodRenderFunction() {\n var now = Date.now();\n var delta = now-lastTime; // in milliseconds\n lastTime = now;\n someObject.x += 0.01 * delta; // 10 pixels per second\n // good because function of time\n}\n```\n\nMore information on `requestAnimationFrame` can be found [here][3] or [here][4].\n\n### ...for waiting a DOM update\n\n**We will now focus on another interesting benefit of that function:**\n\nInstead of using `requestAnimationFrame` for a render loop,\n**you can use it only once** in order to **wait for the next DOM update**.\n\nThere is a lot of use-cases where you need to wait for the next DOM update \nand `requestAnimationFrame` is perfect for that.\n\nMost of the code you can see on the internet rely on using a `setTimeout` with an arbitrary time\ngiven in second parameters *(sometimes 30, sometimes 0 !?)*.\nThis is, in my humble opinion, a wrong approach because you will never know if the repaint has \nreally been performed.\n\n## QanimationFrame\n\n`QanimationFrame` is a function which takes a **DOM Element** in parameter and return a \n**Promise of that \"ready\" DOM element**.\n\n**`QanimationFrame (elt: DOM Element) =\u003e Promise[DOM Element]`**\n\n**N.B.:** Even if `requestAnimationFrame` doesn't have anymore a second *DOM element* parameter,\nI found it quite cool that you can give it as argument and retrieve it back to manipulate it.\nIt also makes the function more composable because it behaves like an identity Promise function.\nWe will also see benefits when using with other DOM Promise libraries.\n\n### Basic example\n\n```javascript\nvar elt = document.createElement(\"div\");\nelt.innerHTML = \"Hello world\";\n// wait for the DOM to be ready before using the height\nQanimationFrame(elt).then(function (elt) {\n  console.log(\"height=\"+elt.offsetHeight);\n});\n```\n\n### Composability\n\n```javascript\nfunction createDivInBody (html) {\n  var elt = document.createElement(\"div\");\n  elt.innerHTML = html;\n  document.body.appendChild(elt);\n  return elt;\n}\n\nvar height = \nQ.fcall(createDivInBody, \"Hello world!\u003cbr/\u003eHow are you today?\")\n .then(QanimationFrame)\n .then(function (elt) {\n   return elt.offsetHeight;\n });\n\nheight.then(function(height){\n  console.log(\"height is \"+height);\n});\n```\n\nThere is of-course a lot of more examples and use-cases of that library.\n\n## Next episode\n\nNext episode is a big one!\n\nWe will introduce you a Promisified animation library called **Zanimo.js** which\nhelps to chain different **CSS transitions with only Promises**.\nIt is very interoperable with any other Promise library,\nmeaning that you can easily chain Zanimo animations with other asynchronous actions.\n","data":{"title":"Qep3.: QanimationFrame","description":"This third article on Q is a little parenthesis to the Qep articles series, featuring the requestAnimationFrame Javascript function and its general usage, and QanimationFrame, its Promisified version used as a \"wait for DOM to be ready\" API.","thumbnail":"/images/2013/07/qanimationframe.jpg","author":"Gaetan","layout":"post","tags":["AWOP","javascript","promise","Q","library"]}},{"id":"2013-07-13-deferred","year":"2013","month":"07","day":"13","slug":"deferred","content":"\n[0]: /pages/a-world-of-promises/\n[1]: http://github.com/kriskowal/q\n[2]: http://github.com/gre/qimage\n[3]: http://wiki.commonjs.org/wiki/Promises/A\n[4]: http://dom.spec.whatwg.org/#promises\n[5]: http://wiki.commonjs.org/wiki/Promises/B\n[6]: http://wiki.commonjs.org/wiki/Promises/D\n[7]: https://npmjs.org/package/qimage\n\n# A [World Of Promises][0], episode 2\n\n\u003cimg src=\"/images/2013/07/qimage_then_thumbnail.jpg\" alt=\"\" class=\"thumbnail-right\" style=\"width: 200px\" /\u003e\n\n*This second article on [Q][1] will introduce you how to easily \nturn a callback API into a promise API using Deferred objects.\nIt will also present the new W3C specification of Promise and finish\nwith the implementation of [Qimage][2], a simple Image Promise wrapper.*\n\n\u003c!--more--\u003e\n\n## Deferred objects\n\n[Q][1] splits the concept of Promise in two parts: one part is the **Deferred object**, another is the **Promise object**.\n\nA **Deferred object** is an object which just aims to control the state of a Promise.\nIt allows to do one of the two following actions (one time only):\n\n* `.resolve(value)`: moving from *pending* to ***fulfilled* with a value**.\n* `.reject(error)`: moving from *pending* to ***rejected* with an error**.\n\n\u003cimg src=\"/images/2013/07/promise.png\" style=\"max-width: 300px; width: 100%\" /\u003e\n\nA **Promise object** can be obtained from a Deferred object via the `promise` field.\nIn [Q][1], a Promise is **read-only**: you can basically only do `.then` with it \nand there is no such `resolve` or `reject` method on a Promise.\n\n \u003e a Deferred is \"resolvable\", a Promise is \"thenable\".\n\n***N.B.***: *this separation also exists in other languages but with different names (for instance in Scala: Promise / Future).*\n\n### `Q.defer()`\n\nThe method Q.defer() will return a new **Deferred object** initialized in a *pending* state.\n\n```javascript\nvar d = Q.defer();\nsetTimeout(function () {\n  d.resolve(42);\n}, 500);\nvar promise = d.promise;\n// ...\npromise.then(function (value) {\n  console.log(\"the universe = \"+value);\n});\n```\n\nNote that the [Promises/A][3] spec only specifies the concept of **Promise**.\nIt does not defines the \"Deferred\" part.\nUp to the Promise library to implement its own way of resolving / rejecting the value of a Promise.\n\nThere is also [Promises/B][5] and [Promises/D][6] to define that though.\n\n### About the DOM.Promise specification\n\na [new DOM specification draft][4] has born recently and is a bit different from the Q style,\nthe \"Deferred\" object (called a **Resolver**) is given as an argument of the function given at Promise instanciation.\n\n```javascript\nvar promise = new /*DOM.*/Promise(function (resolver) {\n  setTimeout(function () {\n    resolver.resolve(42);\n  }, 500);\n});\npromise.then(function (value) {\n  console.log(\"the universe = \"+value);\n});\n```\n\n## Qimage: Wrapping the Image API\n\nWe will now show you how to easily **wrap the DOM Image API into a Promise API with Q**.\nBefore showing the implementation, let's explore the possibilities of such API.\n\n**`Qimage (url: String) =\u003e Promise[DOM Image]`**\n\nHere is how we want our `Qimage` API to look like:\n\n```javascript\nvar promise = Qimage(\"http://imagesource.com/image.png\");\npromise.then(function (image) {\n  // image instanceof Image\n}, function (error) {\n  // error instanceof Error\n});\n```\n\nWe can use it like this:\n\n```javascript\nQimage(\"images/foo.png\").then(function (img) {\n  document.body.appendChild(img);\n}, function (error) {\n  document.body.innerHTML = \"Unable to load the image\";\n});\n```\n\nNow we define `Qimage` as a Promise library, **we can then use all the power of Promises,\ncombine Promises together, chain different Promise APIs**...\n\n```javascript\nQ.all([\n  Qimage(\"res1.png\"),\n  Qimage(\"res2.png\"),\n  Qimage(\"res3.png\")\n])\n.spread(function (res1, res2, res3) {\n  document.body.appendChild(res1);\n  document.body.appendChild(res2);\n  document.body.appendChild(res3);\n});\n```\n\nThis wrapper makes a simple but powerful Image Loading library module.\n\n### Implementation\n\nHere is how `Qimage` works:\n\n```javascript\nvar Qimage = function (url) {\n  var d = Q.defer();\n  var img = new Image();\n  img.onload = function () {\n    d.resolve(img);\n  };\n  img.onabort = function (e) {\n    d.reject(e);\n  };\n  img.onerror = function (err) {\n    d.reject(err);\n  };\n  img.src = url;\n  return d.promise;\n};\n```\n\n...and that's it!\n\nNote that **the Deferred object is isolated** in the `Qimage` function scope.\n\nOnly the (read-only) Promise is accessible from the outside when returned by the function.\n\n**How simple is wrapping a callback API into a Promise API!**\n\n---\n\n[Qimage][2] is released as a micro-lib and available on [Github][2] and [NPM][7].\n\n## Next episode\n\nNext episode will feature **`requestAnimationFrame`**, \n*a fundamental function generally used for performing efficient Javascript animations*.\nWe will show you **QanimationFrame** and how we can use it as a **Promisified \"wait for DOM to be ready\" API**.\n\n","data":{"title":"Qep2.: Deferred objects, Qimage","description":"This second article on Q will introduce you how to easily turn a callback API into a promise API using Deferred objects. It will also present the new W3C specification of Promise and finish with the implementation of Qimage, a simple Image Promise wrapper.","thumbnail":"/images/2013/07/qimage_then_thumbnail.jpg","author":"Gaetan","layout":"post","tags":["AWOP","javascript","promise","Q","library"]}},{"id":"2013-07-10-q-a-promise-library","year":"2013","month":"07","day":"10","slug":"q-a-promise-library","content":"\n[0]: /pages/a-world-of-promises/\n[1]: http://github.com/kriskowal/q\n[2]: http://github.com/gre/qimage\n[3]: https://github.com/kriskowal\n[4]: http://wiki.commonjs.org/\n[5]: http://domenic.me/2012/10/14/youre-missing-the-point-of-promises/\n[6]: https://raw.github.com/kriskowal/q/master/design/README.js\n[7]: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Guide/EventLoop\n[8]: http://jquery.com/\n[9]: http://wiki.commonjs.org/wiki/Promises/A\n[10]: http://tritarget.org/blog/2012/11/28/the-pyramid-of-doom-a-javascript-style-trap/\n\n# [A World Of Promises][0], episode 1\n\n*This article is the first of a series of small articles \non the Q Javascript library and its eco-system.\nThis article is a brief introduction to Q Promises.*\n\n\u003cimg src=\"/images/2013/07/promise_then_thumbnail.jpg\" alt=\"\" class=\"thumbnail-left\" /\u003e\n\n[Q][1] is a **Promise library** in **Javascript** \ncreated 4 years ago by [Kris Kowal][3] who is one of the main contributor to [CommonJS][4]\nwhere we can find the **[Promises/A][9]** specification.\n\n[Q][1] is probably **the most mature and powerful Promise library in Javascript**\nwhich inspired a lot of libraries like [jQuery][8].\nIt exposes a complete API with, in my humble opinion, \ngood ideas like the separation of concerns between a \"Deferred\" object (the resolver) \nand a \"Thenable\" Promise (the read-only promise).\n\nThis article is a brief introduction to Q Promises with some examples.\nFor more information on the subject, I highly recommend reading\nthe article [\"You're Missing the Point of Promises\"][5] \nand [the Q implementation design README][6].\n\n\u003c!--more--\u003e\n\n## What is a Promise\n\nA **Promise** is an object representing a **possible future value** which has \na `then` method to access this value via callback. A Promise is initially \nin a *pending* state and is then either *fulfilled* with a value or *rejected* with an error.\n\n![Schema representing Promise states: pending -\u003e fulfilled|rejected](/images/2013/07/promise.png)\n### Some properties\n\nIt is **immutable** because the Promise value never changes and each `then` creates a new Promise. \nAs a consequence, one same Promise can be shared between different code.\n\nIt is **chainable** through the `then` method (and other Q shortcut methods),\nwhich transforms a Promise into a new Promise without knowing what's inside.\n\nIt is **composable** because the `then` method will unify any Promise returned as \na result of the callback with the current Promise (act like a map or flatmap). \n[Q][1] also has a `Q.all` helper for combining an Array of Promise into one big Promise.\n\n## A solution against the [Pyramid of Doom][10] effect\n\n*Javascript* is by nature an **asynchronous language** based on an [event loop][7] which enqueue events.\nAs a consequence, there is no way to block long actions (like Image Loading, ajax requests, other events), but everything is instead asynchronous:\nMost of Javascript APIs are using **callbacks** - functions called when the event has succeeded.\n\n**Problem with callbacks** is when you start having a lot of asynchronous actions.\nIt quickly becomes a [Callback Hell](http://callbackhell.com/).\n\n### Example\n\nHere is a simple illustration, let's say we have 2 functions, \none for **retrieving some photos meta-infos from Flickr** with a search criteria: `getFlickrJson(search, callback)`, \nanother for **loading an image from one photo meta-info**: `loadImage(json, callback)`. \nOf-course both functions are asynchonous so they need a callback to be called with a result.\n\nWith this callback approach, we can then write:\n\n```javascript\n// search photos for \"Paris\", load and display the first one\ngetFlickrJson(\"Paris\", function (imagesMeta) {\n  loadImage(imagesMeta[0], function (image) {\n    displayImage(image);\n  });\n});\n```\n*(Imagine what it can look like with more nested steps.)*\n\n\u003e we can easily turn a *callback* API into a *Promise* API\n\n#### Promise style\n\n`getFlickrJson` and `loadImage` can now be rewritten as Promise APIs:\n\nEach function has clean signatures:\n\n* `getFlickrJson` is a `(search: String) =\u003e Promise[Array of ImageMeta]`.\n* `loadImage` is a `(imageMeta: ImageMeta) =\u003e Promise[DOM Image]`.\n* `displayImage` is a `(image: DOM Image) =\u003e undefined`.\n\n...and are easily pluggable together:\n\n```javascript\ngetFlickrJson(\"Paris\")\n  .then(function (imagesMeta) { return imagesMeta[0]; })\n  .then(loadImage)\n  .then(displayImage, displayError);\n```\n\n**This is much more flatten, concise, maintainable and beautiful!**\n\nNote that if we want to be safer we can write:\n\n```javascript\nQ.fcall(getFlickrJson, \"Paris\")\n  .then(function (imagesMeta) { return imagesMeta[0]; })\n  .then(loadImage)\n  .then(displayImage, displayError);\n```\n\n`Q.fcall` will call the function with the given parameters and ensure wrapping the returned value into a **Promise**.\nSo my code should continue working even if we change signatures to:\n\n* `getFlickrJson` is a `(search: String) =\u003e Array of ImageMeta`.\n* `loadImage` is a `(imageMeta: ImageMeta) =\u003e DOM Image`.\n\nOne other cool thing about this chain of Promises is **we can easily add more steps** between two `then` step, for instance a DOM animation, a little delay, etc.\n\n### Error Handling\n\nBut a much important benefit is, unlike the callbacks approach,\nwe can properly **handle the error in one row** because one of the following steps eventually fails:\n\n* `getFlickrJson` fails to perform the ajax request to retrieve the Flickr JSON data.\n* The array returned by Flickr was empty so `loadImage` throws an exception.\n* The `loadImage` fails (e.g. the image is unavailable).\n\nThis is called **propagation** and is exactly how **exceptions** work.\n\n**Promise Error Handling** really looks like **Exception Handling**.\n\nIf it would be possible to have two methods:\n\n* `getFlickrJsonSync` is a `(search: String) =\u003e Array of ImageMeta`.\n* `loadImageSync` is a `(imageMeta: ImageMeta) =\u003e DOM Image`.\n\nThen, the blocking code would look like this:\n\n```javascript\ntry {\n  var imagesMeta = getFlickrJsonSync(\"Paris\")\n  var firstImageMeta = imagesMeta[0]\n  var image = loadImageSync(firstImageMeta)\n  displayImage(image);\n} catch (e) {\n  displayError(e);\n}\n```\n\n*...which is very close to Promise style.*\n\n**Q Promises also unify Exceptions and Rejected Promises**:\nthrowing an exception in any Q callback will result in a rejected Promise.\n\n```javascript\nvar safePromise = Q.fcall(function () {\n  // following eventually throws an exception\n  return JSON.parse(someUnsafeJsonString);\n});\n// safePromise is either fulfilled with a JSON Object\n// or rejected with an error.\n```\n\n\u003e Error handling with the callbacks approach is hell:\n\n```javascript\ngetFlickrJson(\"Paris\", function (imagesMeta) {\n  if (imagesMeta.length == 0) {\n    displayError();\n  }\n  else {\n    loadImage(imagesMeta[0], function (image) {\n      displayImage(image);\n    }, displayError);\n  }\n}, displayError);\n```\n\n\n## Next episode\n\nNext episode, we will show you how to create your own Promises with *Deferred* objects.\nWe will introduce **Qimage**, a simple Image loader wrapped with Q.\n\n---\n\nSpecial Kudos to \u003ca href=\"http://twitter.com/42loops\"\u003e@42loops\u003c/a\u003e \u0026 \u003ca href=\"http://twitter.com/bobylito\"\u003e@bobylito\u003c/a\u003e\nfor bringing Q in my developer life :-P\n","data":{"title":"Qep1.: Q, a Promise library","description":"This article is the first of a series of small articles on the Q Javascript library and its eco-system. It is a brief introduction to Q Promises.","thumbnail":"/images/2013/07/promise_then_thumbnail.jpg","author":"Gaetan","layout":"post","tags":["AWOP","javascript","promise","Q","library"]}},{"id":"2013-05-07-playframework-simple-deployment-scripts","year":"2013","month":"05","day":"07","slug":"playframework-simple-deployment-scripts","content":"\n\u003cscript src=\"https://gist.github.com/gre/5528826.js\"\u003e\u003c/script\u003e\n\n","data":{"title":"Playframework simple deployment scripts","description":"Playframework simple deployment scripts","author":"Gaetan","layout":"post","tags":["sysadmin","playframework"]}},{"id":"2013-02-11-glsl-js-a-javascript-glsl-library-dry-efficient","year":"2013","month":"02","day":"11","slug":"glsl-js-a-javascript-glsl-library-dry-efficient","content":"\n[2]: http://gre.github.io/glsl.js/examples/balls\n[3]: http://gre.github.io/glsl.js/examples\n[4]: http://gre.github.io/glsl.js/docs\n[5]: http://github.com/gre/glsl.js\n[6]: http://gre.github.io/glsl.js/test\n[7]: http://gre.github.io/glsl.js/examples/pong/\n[8]: http://glsl.heroku.com\n[13]: http://gre.github.io/glsl.js/examples/helloworld\n[15]: http://www.khronos.org/registry/gles/specs/2.0/GLSL_ES_Specification_1.0.17.pdf\n[16]: http://glsl.heroku.com/\n[24]: http://gre.github.io/glsl.js/examples/canvas-text/\n[25]: http://gre.github.io/glsl.js/examples/video/\n[26]: http://gre.github.io/glsl.js/examples/mario_sprites/\n\n[![glsl_mario](/images/2013/02/glsl_mario.jpg)][2]\n\n**TL;DR. WebGL is super powerful and efficient. This library abuses this power for efficient 2D.**\n\nglsl.js is a subset of a WebGL library which focuses on making the GLSL (OpenGL Shading Language) easy and accessible for vizualisation and game purposes (2D or 3D).\n\n- **[Bouncing balls example video tutorial][2]**\n- [Open other examples][3]\n- [API Documentation][4]\n- [Fork me on Github][5]\n- [Unit tests][6]\n\n[![glsl_pong](/images/2013/02/glsl_pong.jpg)][7]\n\n## Why?\n\n**WebGL is a very low level and stateful API**. Actually the WebGL API **is** the OpenGL API.\n\nI wanted to make a graphic library where you wouldn‚Äôt have to know about this API but still have access to the powerful OpenGL Shading Language called GLSL.\n\nDo you know [glsl.heroku.com][8]? It‚Äôs a cool platform for demoscene where you can experiment some nice effects in GLSL. My library extends this concept of rendering in one whole fragment shader (which takes the plain canvas) but also provides a way to inject your own Javascript variables.\n\n### DRY\n\n**WebGL is not DRY at all**, you always have to repeat yourself both on the GLSL and on the Javascript part (especially for synchronizing variables).  \nWorse than that, you have to know in your Javascript code what are the GLSL types of every variable to synchronize.\n\nHow boring is that:\n\n```javascript\n// Synchronizing the new values of 2 variables in pure WebGL.\n\nvar myInt = 1;\nvar myIntLocation = gl.getUniformLocation(program, \"myInt\");\nmyInt;\ngl.uniform1i(myFloatLocation, myInt); // 1i means one integer\n\nvar myVector2 = { x: 1.3, y: 2.4 };\nvar myVector2Location = gl.getUniformLocation(program, \"myVector2\");\ngl.uniform2f(myVector2Location, myVector2.x, myVector2.y); // 2f means float[2]\n```\n\n**glsl.js** provides a DRY and simple way to synchronize Javascript variables.\n\nFirst, the library will handle for you the UniformLocations.\n\nMore important, and unlike the WebGL API and many WebGL libraries, **you will never have to define the type of your variables from the Javascript with glsl.js!** You just define it once in your shader!\n\nHow it works behind is the framework will statically parse your GLSL and infer types to use for the synchronization. The right `gl.uniform*` function is called by Javascript reflection.\n\nIt now simply becomes:\n\n```javascript\n// Set the values of 2 variables in glsl.js\nthis.set(\"myInt\", 1);\nthis.set(\"myVector2\", { x: 1.3, y: 2.4 });\n// ... see also this.sync() and this.syncAll()\n```\n\n\u003c!--more--\u003e\n\nMore technically, **glsl.js** is a subset\\* of a WebGL library which focus on **making the GLSL (OpenGL Shading Language) easy and accessible** for vizualisation and game purposes (2D or 3D).\n\n\u003e \\* Subset, because we only focus on using a _fragment shader_ (the _vertex shader_ is static and take the full canvas size), But don‚Äôt worry, you have a long way to go with just one _fragment shader_.\n\nThe concept is to split the **rendering part in a GLSL fragment** from the **logic part in Javascript** of your app/game. Both part are linked by **a set of variables** (the state of your app/game).\n\n![schema](https://f.cloud.github.com/assets/211411/133026/5ed79ff8-709b-11e2-85dd-60332f74dc31.png)\n\n**glsl.js** aims to abstract every GL functions so you don‚Äôt have to learn any OpenGL API.  \nWhat you only need to care about is the logic in Javascript and the rendering in GLSL.\n\nBy design, **you can‚Äôt mix logic and render part**, this approach really helps to focus on essential things separately.\n\n### Efficiency\n\nBasically, this design is efficient because the Javascript logic will take some CPU while the rendering will take the graphic card.\n\nToday, WebGL is widely supported on modern desktop browsers. It‚Äôs not yet the case on mobile and tablet.\n\nHowever, using Chrome Beta, I‚Äôm able to run my HTML5 game at 60fps on my Nexus 4, which is quite promising for the future.\n\n\u003ciframe width=\"640\" height=\"360\" src=\"http://www.youtube.com/embed/EzTCdjpdTfk?feature=player_embedded\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\n_Enough talking, let‚Äôs see some examples now‚Ä¶_\n\n### [][11]Hello World Example\n\n[11]: #hello-world-example\n\nHere is an Hello World example. For more examples, see [/examples][3].\n\n```html\n\u003ccanvas id=\"viewport\" width=\"600\" height=\"400\"\u003e\u003c/canvas\u003e\n\u003cscript id=\"fragment\" type=\"x-shader/x-fragment\"\u003e\n  #ifdef GL_ES\n  precision mediump float;\n  #endif\n  uniform float time;\n  uniform vec2 resolution;\n  void main (void) {\n    vec2 p = ( gl_FragCoord.xy / resolution.xy );\n    gl_FragColor = vec4(p.x, p.y, (1.+cos(time))/2., 1.0);\n  }\n\u003c/script\u003e\n\u003cscript src=\"../../glsl.js\" type=\"text/javascript\"\u003e\u003c/script\u003e\n\u003cscript type=\"text/javascript\"\u003e\n  var glsl = Glsl({\n    canvas: document.getElementById(\"viewport\"),\n    fragment: document.getElementById(\"fragment\").textContent,\n    variables: {\n      time: 0, // The time in ms\n    },\n    update: function (time, delta) {\n      this.set(\"time\", time);\n    },\n  }).start();\n\u003c/script\u003e\n```\n\n[![screenshot](https://f.cloud.github.com/assets/211411/132729/e702c2b4-7090-11e2-8904-49e904e6c5a2.png)][13]\n\n### [][14]GLSL: OpenGL Shading Language\n\n[14]: #glsl-opengl-shading-language\n\n\u003e GLSL is a high-level shading language based on the syntax of the C programming language. (Wikipedia)\n\nGLSL gives a very different way of thinking the rendering: basically, in a main function, you have to **set the color (`gl_FragColor`) of a pixel for a given position (`gl_FragCoord`)**.\n\nAs a nice side effect, GLSL is vectorial by design: it can be stretch to any dimension.\n\nGLSL is efficient because it is compiled to the graphic card.\n\nGLSL provides an interesting collection of **types** (e.g. `int`, `float`, `vec2`, `vec3`, `mat3`, `sampler2D`,‚Ä¶ and also arrays of these types) and **functions** (e.g. `cos`, `smoothstep`, ‚Ä¶).\n\n[Here is a good reference for this][15].\n\nYou can also deeply explore the awesome collection of [glsl.heroku.com][16]. Any of glsl.heroku.com examples are compatible with **glsl.js** if you add some required variables (\\*time\\*, _mouse_, ‚Ä¶).\n\n### [][17]App/Game Logic\n\n[17]: #appgame-logic\n\nYou must give to Glsl a `canvas` (DOM element of a canvas), a `fragment` (the GLSL fragment code), the `variables` set, and the `update` function.\n\nThen you can start/stop the rendering via method (`.start()` and `.stop()`).\n\nThe `update` function is called as soon as possible by the library. It is called in a `requestAnimationFrame` context.\n\nYou must define all variables shared by both logic and render part in a Javascript object `{varname: value}`.  \nVariables must match your GLSL uniform variables. Every time you update your variables and you want to synchronize them with the GLSL you have to manually call the `sync` function by giving all variables name to synchronize.\n\n**Exemple:**\n\n```javascript\nGlsl({\n¬† canvas: canvas,\n¬† fragment: fragCode,\n¬† variables: {\n¬† ¬† time: , // The time in seconds\n¬† ¬† random1:\n¬† },\n¬† update: function (time, delta) {\n¬† ¬† this.set(\"time\", time);\n¬† ¬† this.set(\"random1\", Math.random());\n¬† }\n}).start();\n```\n\n**Note:** _under the hood, a type environment of uniform variables is inferred by parsing your GLSL code._\n\n### [][18]Using arrays\n\n[18]: #using-arrays\n\nHopefully, GLSL also supports arrays. You can actually bind a Javascript array to a GLSL uniform variable.\n\n**Example:**\n\nIn GLSL,\n\n```glsl\nuniform float tenfloats[10];\n```\n\nIn Javascript,\n\n```javascript\nvar glsl = Glsl({\n¬† ...\n¬† variable: {\n¬† ¬† tenfloats: new Float32Array(10)\n¬† },\n¬† update: function () {\n¬† ¬† this.tenfloats[3] = Math.random();\n¬† ¬† this.sync(\"tenfloats\");\n¬† }\n}).start();\n```\n\nAlternatively, you can still use a classical javascript Array (but native Javascript arrays are prefered because more efficient).\n\nUse `Int32Array` for `int[]` and `bool[]`.\n\nVector arrays are also possible. In Javascript, you will have to give a linearized array.  \nFor instance,  \na `vec2[2]` will be `[vec2(1.0, 2.0), vec2(3.0, 4.0)]` if `Float32Array(1.0, 2.0, 3.0, 4.0)` is used.\n\n### [][19]Using objects\n\n[19]: #using-objects\n\nEven more interesting now, you can synchronize a whole object into the GLSL world. This is very interesting for Object-Oriented approach.\n\n**Example:**\n\nIn GLSL,\n\n```glsl\nstruct Circle {\n¬† vec2 center;\n¬† float radius;\n}\nuniform Circle c1;\nbool inCircle (vec2 p, Circle c) {\n¬† vec2 ratio = resolution/resolution.x;\n¬† return distance(p*ratio, c.center*ratio) \u003c c.radius;\n}\nvoid main (void) {\n¬† vec2 p = ( gl_FragCoord.xy / resolution.xy );\n¬† if (inCircle(p, c1))\n¬† ¬† gl_FragColor = vec4(1.0, 0.0, 0.0, 1.0);\n¬† else\n¬† ¬† gl_FragColor = vec4(0.0, 0.0, 0.0, 1.0);\n}\n```\n\nIn Javascript,\n\n```javascript\nfunction Circle (x, y, radius) {\n¬† this.center = { x: x, y: y };\n¬† this.radius = radius;\n¬† this.originalRadius = radius; // not visible by GLSL\n}\nCircle.prototype.update = function () {\n¬† this.radius = this.originalRadius Math.sin(Date.now()/100);\n}\nvar c1 = new Circle(0.5, 0.5, 0.1);\nGlsl({\n¬† ...\n¬† variable: {\n¬† ¬† c1: c1\n¬† },\n¬† update: function (time, delta) {\n¬† ¬† c1.update();\n¬† ¬† this.sync(\"c1\");\n¬† }\n}).start();\n```\n\nstructs inside structs are also supported:\n\n```glsl\nstruct Circle {\n¬† vec2 center;\n¬† float radius;\n}\nstruct Player {\n¬† Circle circle;\n¬† bool visible;\n}\n```\n\n### [][20]Using Arrays of Objects\n\n[20]: #using-arrays-of-objects\n\nThe two previous chapters can be assemble!\n\nYes man, Array of JS object is possible!\n\n```glsl\nuniform Circle circles[2];\n// circles[0].radius\n// ‚Ä¶\n```\n\n```javascript\nGlsl({\n¬† ...\n¬† variable: {\n¬† ¬† circles: [ new Circle(0.1, 0.1, 0.2), new Circle(0.2, 0.3, 0.2) ]\n¬† },\n¬† ...\n}).start();\n```\n\n### [][21]Using images\n\n[21]: #using-images\n\nGLSL:\n\n```glsl\nuniform sampler2D img;\n```\n\nJavascript:\n\n```javascript\nvar image = new Image();\nimg.src = \"foo.png\";\nvar glsl = Glsl({\n¬† ...\n¬† variable: {\n¬† ¬† img: image\n¬† }\n});\nimg.onload = function () {\n¬† glsl.start();\n}\n```\n\nNote: Using an image loader library can be a good idea.\n\nIn GLSL, you will need to use the texture lookup functions to access the image color for a given coordinate. E.g. `texture2D(img, coord)`. (see the [specs][15]).\n\n#### See also\n\n[The mario_sprites example][26]\n\n### [][22]Using another canvas\n\n[22]: #using-another-canvas\n\n[![hello_world_text_glsl_js](/images/2013/02/hello_world_text_glsl_js.png)][24]\n\n### Using\n\n[![glsl_js_video](/images/2013/02/glsl_js_video.png)][25]\n\n## See also\n\n\u003ciframe width=\"480\" height=\"360\" src=\"http://www.youtube.com/embed/kxBkfy_8JEs\" frameborder=\"0\" allowfullscreen=\"\"\u003e\u003c/iframe\u003e\n","data":{"title":"glsl.js, a Javascript + GLSL library = DRY \u0026 efficient","description":"WebGL is super powerful and efficient. This library abuses this power for efficient 2D.","thumbnail":"/images/2013/02/glsl_mario.jpg","author":"Gaetan","layout":"post","permalink":"/2013/02/glsl-js-a-javascript-glsl-library-dry-efficient/","tags":["gamedev","javascript","library","GLSL","WebGL"]}},{"id":"2013-01-30-playcli-play-iteratees-unix-pipe","year":"2013","month":"01","day":"30","slug":"playcli-play-iteratees-unix-pipe","content":"\n [1]: http://scala-lang.org\n [2]: http://www.playframework.org/documentation/2.0/Iteratees\n [3]: http://gre.github.io/playCLI-examples/api\n [4]: http://github.com/gre/playCLI\n [5]: http://github.com/gre/playCLI-examples\n [6]: /2012/08/zound-a-playframework-2-audio-streaming-experiment-using-iteratees/\n [7]: http://mandubian.com/2012/08/27/understanding-play2-iteratees-for-normal-humans/\n [8]: http://gre.github.io/playCLI-examples/api/#enumerate(command:scala.sys.process.ProcessBuilder,chunkSize:Int,terminateTimeout:Long)(implicitec:scala.concurrent.ExecutionContext):play.api.libs.iteratee.Enumerator[Array[Byte]]\n [9]: http://www.playframework.org/documentation/api/2.1-RC1/scala/index.html#play.api.libs.iteratee.Enumerator\n [10]: http://gre.github.io/playCLI-examples/api/#pipe(command:scala.sys.process.ProcessBuilder,chunkSize:Int,terminateTimeout:Long)(implicitec:scala.concurrent.ExecutionContext):play.api.libs.iteratee.Enumeratee[Array[Byte],Array[Byte]]\n [11]: http://www.playframework.org/documentation/api/2.1-RC1/scala/index.html#play.api.libs.iteratee.Enumeratee\n\n [12]: http://gre.github.io/playCLI-examples/api/#consume(command:scala.sys.process.ProcessBuilder,terminateTimeout:Long)(implicitec:scala.concurrent.ExecutionContext):play.api.libs.iteratee.Iteratee[Array[Byte],Int]\n [13]: http://www.playframework.org/documentation/api/2.1-RC1/scala/index.html#play.api.libs.iteratee.Iteratee\n [14]: http://www.scala-lang.org/api/current/index.html#scala.sys.process.package\n [15]: http://www.playframework.org/documentation/api/2.1-RC1/scala/index.html#play.api.libs.iteratee.Done$\n [16]: #pipe(command:scala.sys.process.ProcessBuilder,chunkSize:Int,terminateTimeout:Long)(implicitec:scala.concurrent.ExecutionContext):play.api.libs.iteratee.Enumeratee[Array[Byte],Array[Byte]]\n [17]: http://www.playframework.org/documentation/api/2.1-RC1/scala/index.html#play.api.libs.iteratee.Input$$EOF$\n\n\n\u003e **TL;DR.** PlayCLI is a new [Scala][1] library to work with UNIX commands and [Play-Iteratees][2] (a scala implementation of Iteratees facilitating the handling of data streams reactively). Here‚Äôs an overview:\n\n\u003ciframe src=\"http://gre.github.io/playCLI-examples/embedder.html#index.html\" frameborder=\"0\" width=\"550\" height=\"452\"\u003e\u003c/iframe\u003e\n\n## Links\n\n*   [The scala API][3].\n*   [PlayCLI source code (Github)][4].\n*   [PlayCLI Examples application (Github)][5].\n\n### SBT\n\n```scala\n\"fr.greweb\" %% \"playcli\" % \"0.1\"\n```\n\n\u003c!--more--\u003e\n\n## Why PlayCLI\n\nAfter having made [Zound][6] in a HackDay (an experiment to generate an audio stream with playframework iteratees and through the WAVE format), I figured out this was going to be hard to make it work with multiple audio format: *tell me if I‚Äôm wrong but*, there are not so much audio libraries in Java/Scala, or most of them does not support stream handling (and not reactively), and it was going to be crazy to re-implement everything in Scala (both in term of cost and performance).\n\nBesides, **UNIX has plenty of tools** to do this and:\n\n1.  they are **complete** and provide a lot of options\n2.  they are **easy to use** (see how Bash is powerful as a consequence)\n3.  Most of them **support streams** out of the box (via stdin / stdout)\n4.  They are very **efficient** (written in C / assembly)\n\nSo why not re-use them from our reactive code?\n\n### Similarities with UNIX pipes\n\n\u003e Take the expressivity of UNIX pipes, bring the power of Scala, mix it with Play Framework and you got a powerful framework for handling real-time and web streaming.\n\nPlay Iteratees are an elegant \u0026 powerful way to handle streams reactively, and I‚Äôve actually always understood them like UNIX pipes, you have the same reactive code style: linearized declarative way of handling streams.\n\n**Bash:**\n\n```bash\ncat words.txt | grep $word \u003e result.txt\n```\n\n**Scala:**\n\n```scala\nEnumerator.fromFile(\"words.txt\") \u0026\u003e   \n¬† splitByNl \u0026\u003e // split a stream of Array[Byte] into stream of String (not impl here)  \n¬† Enumeratee.filter(_.containsSlice(word)) ¬†|\u003e\u003e\u003e   \n¬† fileWriter // consume the steam while storing in a file (not impl here)\n```\n\nor if you prefer the ‚Äúwithout symbol‚Äù version:\n\n```scala\nEnumerator.fromFile(\"words.txt\").  \n¬† through splitByNl.  \n¬† through Enumeratee.filter(_.containsSlice(word)).  \n¬† run fileWriter\n```\n\nHowever, It‚Äôs biased to say Iteratees are only UNIX pipes, they are more than that, but I‚Äôm not going to extend on that subject, they are at least statically typed and safe (it‚Äôs more than just a stream of bytes, see [this article][7]).\n\nSo if Iteratees are at least UNIX pipes, why can‚Äôt we use Unix pipes from iteratees?\n\n**PlayCLI provides a bridge to use scala.sys.Process with play-iteratees.**\n\n## More about PlayCLI\n\n*(this is a copy of the API documentation)*\n\n### Overview\n\nDepending on your needs, you can **Enumerate / Pipe / Consume** an UNIX command:\n\n[CLI.enumerate][8] is a way to create a stream from a command which **generates output**  \n(it creates an [Enumerator][9][Array[Byte]] )\n\n[CLI.pipe][10] is a way to pipe a command which **consumes input and generates output**  \n(it creates an [Enumeratee][11][Array[Byte],Array[Byte]])\n\n[CLI.consume][12] creates a process which **consumes a stream** ‚Äì useful for side effect commands  \n(it creates an [Iteratee][13][Array[Byte],Int])\n\n\n#### Examples\n\n```scala\nimport playcli._  \nimport scala.sys.process._  \n  \n// Some CLI use cases  \nval tail = CLI.enumerate(\"tail -f /var/log/nginx/access.log\")  \nval grep = (word: String) =\u003e CLI.pipe(Seq(\"grep\", word))  \nval ffmpeg = CLI.pipe(\"ffmpeg -i pipe:0 ... pipe:1\") // video processing  \nval convert = CLI.pipe(\"convert - -colors 64 png:-\") // color quantization  \n  \n// Some usage examples  \nval sharedTail = Concurrent.broadcast(tail)  \nOk.stream(sharedTail).withHeaders(CONTENT_TYPE -\u003e \"text/plain\") // Play framework  \n  \nval searchResult: Enumerator[String] = dictionaryEnumerator \u0026\u003e grep(\"able\") \u0026\u003e aStringChunker  \n  \nOk.stream(Enumerator.fromFile(\"image.jpg\") \u0026\u003e convert).withHeaders(CONTENT_TYPE -\u003e \"image/png\")  \n  \nEnumerator.fromFile(\"video.avi\") \u0026\u003e ffmpeg \u0026\u003e ...\n```\n\n### Process\n\nCLI uses [scala.sys.process][14]  \nand create a Process instance for each UNIX command.\n\nA CLI process is terminates when:\n\n*   The command has end.\n*   stdin and stdout is terminated.\n*   [Done][15] is reached (for [enumerate][8] and [pipe][16]).\n*   [EOF][17] is sent (for [pipe][10] and [consume][12]).\n\nCLI still waits for the Process to terminate by asking the exit code (via `Process.exitCode()`).  \nIf the process is never ending during this phase, it will be killed when `terminateTimeout` is reached.\n\nPS: Thanks to implicits, you can simply give a String or a Seq to the CLI.* functions a `ProcessBuilder`.\n\n### Mutability\n\n[enumerate][8] and [pipe][10] are **immutable**, in other words, re-usable  \n(each result can be stored in a val and applied multiple times).  \n**A new process is created for each re-use**.\n\n[consume][12] is **mutable**, it should not be used multiple times: it targets side effect command.\n\n### Logs\n\nA ‚ÄúCLI‚Äù logger (logback) is used to log different information in different log levels:\n\n*   **ERROR** would mean a CLI error (not used yet).\n*   **INFO** used for the process‚Äô stdout output of a [CLI.consume][12].\n*   **DEBUG** used for the process life cycle (process creation, process termination, exit code).\n*   **WARN** used for the process‚Äô stderr output.\n*   **TRACE** used for low level information (IO read/write). \n\n## Conclusion\n\nI‚Äôm eager to see what you guys can do with such an API, it enables a lot of possibility, I‚Äôm especially thinking about multimedia purposes (using powerful commands like: ImageMagick, ffmpeg, sox,‚Ä¶).\n","data":{"title":"PlayCLI: Play Iteratees + UNIX pipe","description":"PlayCLI is a new Scala library to work with UNIX commands and Play-Iteratees (a scala implementation of Iteratees facilitating the handling of data streams reactively)","author":"Gaetan","layout":"post","permalink":"/2013/01/playcli-play-iteratees-unix-pipe/","tags":["iteratee","playframework","reactive","unix","library"]}},{"id":"2013-01-10-be-careful-with-js-numbers","year":"2013","month":"01","day":"10","slug":"be-careful-with-js-numbers","content":"\n [3]: http://wtfjs.com/\n [4]: https://en.wikipedia.org/wiki/Floating_point\n [6]: http://news.ycombinator.com/item?id=5051525\n [7]: http://silentmatt.com/biginteger/\n\n\u003cblockquote class=\"twitter-tweet\" lang=\"fr\"\u003e\u003cp\u003e@\u003ca href=\"https://twitter.com/greweb\"\u003egreweb\u003c/a\u003e : Let's do a kickstarter to build the 1st space rocket running on embedded Javascript... I think we can discover new physics rules!\u003c/p\u003e\u0026mdash; mandubian (@mandubian) \u003ca href=\"https://twitter.com/mandubian/status/289422662101504000\"\u003e10 janvier 2013\u003c/a\u003e\u003c/blockquote\u003e\n\nIt is [common][3] in Javascript to have unexpected behaviors, but this one is particulary vicious.\n\n\u003e 10000000000000000 === 10000000000000001\n\n**Javascript doesn‚Äôt have integer type but lets you think it has.** `parseInt` and `parseFloat` built-in functions, the fact that ‚Äú1‚Ä≥ is displayed as ‚Äú1‚Ä≥ and not as ‚Äú1.0‚Ä≥ (like many languages) contribute to the general misunderstood.\n\n**In Javascript, all numbers are floating numbers and are prone to [floating point approximation][4].**\n\nWhen you write `var i = 1;`, and you console.log it, Javascript is nice, you obtain `1` and not `1.0000000000000001`. \n\nBut you can experiment that, in Javascript, `1.0000000000000001 === 1` is true‚Ä¶\n\n\u003e I hear you, telling me that *this sounds OK, floating point approximation rules, right?*\n\nBut the same thing occurs for big numbers:\n\n```javascript\n10000000000000000 === 10000000000000001\n```\n\nOh **F\\*\\*K** !\n\n[edit] where in python:  \n![](https://pbs.twimg.com/media/BAg2wRyCIAAGuXW.png:large)\n\n## Termination of loops\n\nThe following is worse:\n\n\u003cscript src=\"https://gist.github.com/4504986.js\"\u003e\u003c/script\u003e\n\nis logging `10000000000000000` forever!\n\nBecause 10000000000000001 can‚Äôt exist in Javascript with approximations, 10000000000000001 is 10000000000000000, so you can‚Äôt increment this value, and you are stuck in this crazy f\\*\\*king loop. \n\nConclusion, *Program termination proof* sounds hard to reach in Javascript!\n\n\u003c!--more--\u003e\n\n## How many numbers in a 1000 range?\n\nBetween 10000000000000000 and 10000000000001000, there are actually 750 Javascript integers.\n\n\u003cscript src=\"https://gist.github.com/4505510.js\"\u003e\u003c/script\u003e\n\n## Real World Example\n\nThe issue can actually **lead to real web application disaster**. Imagine if your database use Long for id (well like almost every databases in the world, like twitter does), and **if you use the id as number in Javascript and not as string**, you can have strange behaviors like never being able to represent and access a resource from the Javascript or worse!\n\n\u003cscript src=\"https://gist.github.com/4505517.js\"\u003e\u003c/script\u003e\n\n## TL;DR. The lesson\n\nThis is not something new, floating point approximation, but the way Javascript fix values to round the approximations mislead us.\n\nNow, simple thing, **Avoid numbers when approximation is not permitted** like for resource id (especially when you retrieve it from a server).\n\nThis probably impacts your JSON APIs because it‚Äôs the last thing you had think of!\n\nOtherwise, **if you need to manipulate big integers in Javascript use a library for that**.\n\nExample: [http://silentmatt.com/biginteger/][7]\n\n[EDIT]  \n9007199254740993 (which is 2^53 1) is the smallest not representable integer in Javascript. In other words, you can trust Javascript numbers before this integer!\n\n[EDIT 2]  \n[Thanks to 0√ó0 on HackerNews][6] who told me the twitter id issue example really happened in a previous twitter API: \n","data":{"title":"Be careful with JS numbers!","description":"Javascript doesn‚Äôt have integer type but lets you think it has. In Javascript, all numbers are floating numbers and are prone to floating point approximation.","author":"Gaetan","layout":"post","permalink":"/2013/01/be-careful-with-js-numbers/","tags":["float","javascript"]}},{"id":"2012-11-12-play-framework-enumerator-outputstream","year":"2012","month":"11","day":"12","slug":"play-framework-enumerator-outputstream","content":"\n[1]: https://github.com/playframework/Play20/commit/0f1ec1479e490f2c8af4cd79dd0b6a14b0ea9f75\n[2]: http://www.playframework.org/\n[3]: http://mandubian.com/2012/08/27/understanding-play2-iteratees-for-normal-humans/\n\nA few weeks ago, [we‚Äôve introduced][1] a new feature in [Play Framework][2]: the `Enumerator.outputStream` method, allowing you to work with Java API requiring an `OutputStream` to generate content, for instance the `java.util.zip` API.\n\n**Now, let‚Äôs see how easy it is to serve a big Zip generated on-the-fly without memory load with Play Framework.**\n\n\u003c!--more--\u003e\n\n## The Zip generation example\n\n\u003cscript src=\"https://gist.github.com/4058734.js?file=Application.scala\"\u003e\u003c/script\u003e\n\nThis demo shows how to **generate a zip file on-the-fly** and directly **stream it** to an HTTP client **without loading it in memory or storing it in a file**.\n\nIt uses an `Enumerator` created with the `Enumerator.outputStream` method.  \nThe `OutputStream` provided by the method is then plugged to the Java‚Äôs `ZipOutputStream`.\n\nFor the example, we have generated a zip containing 100 text files, and each text files contains 100‚Äô000 random long numbers (yes, 100‚Äô000 !).\n\nThe zip size is approximatively 100 Mb. (and is generated in about 3Mb/s in my machine in localhost, but this can be improved)\n\nThe huge benefit of this is the download starts instantly, it means the Zip is streamed while it is generated.\n\n## Show me the code!\n\nInternally, it is implemented with a `Concurrent.unicast`, and a simple implementation of an `OutputStream` pushing into the unicast‚Äôs channel:\n\n\u003cscript src=\"https://gist.github.com/4058734.js?file=Enumerator.scala\"\u003e\u003c/script\u003e\n\n## About Iteratee and Enumerator\n\nIf you want to learn more about Iteratee concepts in Play Framework, I recommend you [this article][3].\n","data":{"title":"Play Framework ‚Äì Enumerator.outputStream","description":"A few weeks ago, we‚Äôve introduced a new feature in Play Framework: the Enumerator.outputStream method, allowing you to work with Java API requiring an OutputStream to generate content, for instance the java.util.zip API.","author":"Gaetan","layout":"post","permalink":"/2012/11/play-framework-enumerator-outputstream/","tags":["playframework","iteratee"]}},{"id":"2012-08-01-zound-a-playframework-2-audio-streaming-experiment-using-iteratees","year":"2012","month":"08","day":"01","slug":"zound-a-playframework-2-audio-streaming-experiment-using-iteratees","content":"\n![ZOUND](/images/2012/07/ZOUND.png)\n\nLast Friday was HackDay #7 at [Zenexity][2], and we decided to work on a real-time audio [experiment][3] made with [Play Framework][4]. The plan was to use an audio generator ([JSyn][5], an audio synthesizer), encode the output and stream it all using Play Iteratees to pipe everything in real-time.\n\n [2]: http://zenexity.com\n [3]: http://github.com/gre/zound\n [4]: http://playframework.org\n [5]: http://www.softsynth.com/jsyn/\n [7]: https://twitter.com/mrspeaker\n [12]: http://www.playframework.org/\n [13]: scala-lang.org/\n [14]: http://sadache.tumblr.com/post/26784721867/is-socket-push-bytes-all-what-you-need-to-program\n [15]: http://www.infoq.com/presentations/Play-I-ll-See-Your-Async-and-Raise-You-Reactive\n [16]: https://github.com/playframework/Play20/tree/master/framework/src/play/src/main/scala/play/api/libs/iteratee\n [17]: http://en.wikipedia.org/wiki/WAV\n [19]: http://en.wikipedia.org/wiki/Broadcasting_(networking)\n\n\u003ciframe width=\"640\" height=\"360\" src=\"http://www.youtube.com/embed/taDLKTcNHnQ?feature=player_embedded\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\n**First of all, let‚Äôs highlight some interesting part of the project, then get into some of the details.**\n\nThanks to [@Sadache][6] for his Iteratee expertise, we ended up with a simple line of code that does all of the hard work:\n\n [6]: https://twitter.com/Sadache\n\n```scala\nval chunkedAudioStream = rawStream \u0026\u003e chunker \u0026\u003e audioEncoder\n```\n\nYou can think of the `\u0026\u003e` operator as the UNIX pipe `|`. So we simply take the `rawStream`, chunk it with a `chunker` and encode it with an `audioEncoder`.\n\nNow, **rawStream** is the raw stream of audio samples (numbers between -1 and 1) generated by the audio synthesizer. Next, the **chunker** buffers a data stream into chunk of bytes. For instance, if you send data stream at 1Kb/s to a 10Kb chunker, it will output one chunk of size 10Kb every 10 seconds. And finally, the **audioEncoder** takes **audio samples** and outputs encoded bytes implementing an audio format (like WAVE).\n\nWe can then make a broadcast of the stream:\n\n```scala\nval (sharedChunkedAudioStream, _) =   \n¬† Concurrent.broadcast(chunkedAudioStream)\n```\n\nAnd then the **sharedChunkedAudioStream** is now a shared stream for every consumer (clients). All that‚Äôs left to do is to stream it over HTTP:\n\n```scala\ndef stream = Action {  \n¬† Ok.stream(audioHeader \u003e\u003e\u003e sharedChunkedAudioStream).  \n¬† ¬† ¬†withHeaders( (\"Content-Type\", audio.contentType) )  \n}\n```\n\nThe `\u003e\u003e\u003e` operator means ‚Äúconcatenation‚Äù, so here we‚Äôre concatenating the **audio header** (given by the format like WAVE) with the current **chunked audio stream**. We also send the right HTTP **Content-Type** header (like ‚Äúaudio/wav‚Äù for WAVE).\n\nAnother interesting part of the project is the **multi-user web user interface: allowing users to interact with the sound synthesis**.\n\nUsing [@mrspeaker][7]‚Äòs audio synthesis expertise, we started creating a synthesizer generator ‚Äì 3 oscillators, various wave shapes, frequency and volumes, and finally flowing through a high pass filter before entering our ‚ÄúrawStream‚Äù above.\n\n\nThanks to the Play framework goodness, **this audio stream can be both consumed by the web page with an HTML audio tag, and with a stream player such as VLC!** Ok, that‚Äôs the project ‚Äì let‚Äôs have a closer look at some of the concepts‚Ä¶\n\n\u003c!--more--\u003e\n\n\n## What is sound?\n\n\u003e Sound is a mechanical wave that is an oscillation of pressure transmitted through a solid, liquid, or gas, composed of frequencies within the range of hearing and of a level sufficiently strong to be heard, or the sensation stimulated in organs of hearing by such vibrations. ***Wikipedia***\n\nWe can represents the sound like any wave as a graphic of the amplitude (the oscillation pressure) as a function of time. Here you see it in Audacity:\n\n![sound-audacity](/images/2012/07/sound-audacity.png)\n\nElectricity is used to pump these amplitudes to your speakers, over time.\n\n### About primitive wave sounds\n\nThere are some patterns ‚Äì some primitives waves ‚Äì we can easily generate with computers (or before with analog oscillators). Those are well known by mathematicians and physicians: Sine wave, Triangle wave, Square wave,‚Ä¶\n\n[![](/images/2012/07/557px-Waveforms.svg_.png)][10]\n\n [10]: http://en.wikipedia.org/wiki/File:Waveforms.svg\n\nA sine wave produce a smooth tone, whereas Triangle and Square wave are more aggressive sounds. The shorter a wave period is, the lower the note you hear: it‚Äôs called the frequency.\n\n### How is sound represented by computer?\n\nWhereas analog oscillators generate sounds in an *almost** continuous stream of electricity, computers are not able to generate continuous stream of data. This is why with computers the sound is divided in to discrete **samples**, usually **44100 samples per second** for standard CD quality audio. Each sample is a value (amplitude) for a given time position.\n\n[See Sampling (wikipedia)][10].\n\n [10]: http://en.wikipedia.org/wiki/Sampling_(signal_processing)\n\nIf you zoom in Audacity, you can actually see each sample:  \n![audacity-zoom](/images/2012/07/audacity-zoom-ah.png)\nThis is an ‚ÄúAH‚Äù timbre of my voice. A timbre is unique to everyone, it‚Äôs the pattern the sound wave take when you speak. **The amplitude of an audio sample is usually represented as a Real number between -1.0 and 1.0**.\n\n** electricity is not strictly continuous, we have electrons out there!*\n\nOk, Let‚Äôs go back to our experiment now!\n\n## The experiment\n\nOur experiment is using [Play Framework][12] and is written in [Scala language][13]. Specifically, our project takes advantage of Play framework‚Äôs powerful **Iteratee**s.\n\n\n\u003e Take the expressivity of UNIX pipes, bring the power of Scala, mix it with Play Framework and you got a powerful framework for handling real-time and web streaming.\n\nThe iteratee (and related constructs) can take a bit of getting used to. I recommend checking out [this article on Iteratees in Play][14] and/or [this presentation][15] if you are interested in learning more about Play2 and reactive programming with Iteratees. And if you just want to see how it work ‚Äì you can read the source code at [Play20 Github source code][16].\n\n\n### Generating the audio stream\n\n```scala\nval (rawStream, channel) = Concurrent.broadcast[Array[Double]]  \nval zound = new ZoundGenerator(channel).start()\n```\n\nWe create an `Array[Double]` broadcast which return two values: the **rawStream** will be used to read the generated data, and the **channel** used by the generator to push generated audio samples. We give this channel to the **ZoundGenerator**. The `.start()` then starts the audio generation. All of the generation is done using the JSyn library.\n\nHere‚Äôs a snippet from the **ZoundGenerator** class showing the connection between **JSyn** and **Channel**:\n\n```scala\nclass ZoundGenerator(output: Channel[Array[Double]]) {  \n¬† val out = new MonoStreamWriter()  \n  \n¬† val synth = {  \n¬† ¬† val synth = JSyn.createSynthesizer()  \n¬† ¬† synth.add(out)  \n¬† ¬† out.setOutputStream(new AudioOutputStream(){  \n¬† ¬† ¬† def close() {}  \n¬† ¬† ¬† def write(value: Double) {  \n¬† ¬† ¬† ¬† output.push(Array(value))  \n¬† ¬† ¬† }  \n¬† ¬† ¬† def write(buffer: Array[Double]) {  \n¬† ¬† ¬† ¬† write(buffer, , buffer.length)  \n¬† ¬† ¬† }  \n¬† ¬† ¬† def write(buffer: Array[Double], start: Int, count: Int) {  \n¬† ¬† ¬† ¬† output.push(buffer.slice(start, start count))  \n¬† ¬† ¬† }  \n¬† ¬† })  \n¬† ¬† synth  \n¬† }  \n¬† // ...\n```\n\nWe have to implement the methods of AudioOutputStream ‚Äì but it‚Äôs just a matter of pushing each audio sample to the channel. It‚Äôs that simple!\n\n### Encoding the raw audio stream\n\nFor now, we have only implemented the [WAVE format][17]. Basically, WAVE has 2 parts; the WAVE header which describes important information (like the framerate and the bits per sample), and the data.  \nThe data is encoded in a simple manner I won‚Äôt describe here but you can look to the encoder I made here: \n\n\nNow more interesting, let‚Äôs wrap it with Play Iteratees:\n\n```scala\nval audio = MonoWaveEncoder() // instanciate the WAV encoder  \nval audioHeader = Enumerator(audio.header)  \nval audioEncoder = Enumeratee.map[Array[Double]](audio.encodeData)\n```\n\n***N. B.***: *Remember that Scala is a typed language but where the type declaration is optional because the compiler can infer the type.*\n\n**audioHeader** is an `Enumerator` which means it can *produce* data, and here the data is the audio header. More precisely it‚Äôs an `Enumerator[Array[Byte]]` because audio.header is an `Array[Byte]`. Note that contained data is not ‚Äúconsumed‚Äù like it would be for an `InputStream`. Each time you use this enumerator, it gives you its entire content.\n\n**audioEncoder** is an `Enumeratee[Array[Double], Array[Byte]]`. It takes an `Array[Double]` from input and returns an `Array[Byte]` as output. The input is a raw array of audio samples (double numbers between -1.0 and 1.0). The output is the encoded array of bytes.\n\nMore formally, an `Enumeratee[A, B]` is an *adapter* which maps some data of type A to new data of type B. You can implement the way the data is transformed with the map function. Here we just give it the `audio.encodeData` function.\n\n### Streaming it\n\nWe can basically stream the audio stream with Play2 like so:\n\n```scala\ndef stream = Action {  \n¬† val audioStream = rawStream \u0026\u003e audioEncoder  \n¬† Ok.stream(audioHeader \u003e\u003e\u003e audioStream).  \n¬† ¬† ¬†withHeaders( (CONTENT_TYPE, audio.contentType) )  \n}\n```\n\nThe `rawStream \u0026\u003e audioEncoder` takes the raw stream and **pipes** it into the encoder which results in the encoded audio stream. `audioHeader \u003e\u003e\u003e audioStream` will **concatenate** `audioHeader` with `audioStream`. Hence, the first thing the server will do is start sending the audio header to the client **and then** stream the audio in real-time.\n\n**A client can connect at any time and will hear current stream**, so it should simultaneously hear the same thing as any other client (with some delay depending on the client buffer). If the generator stops emitting audio samples, the http client will stop receiving audio data ‚Äì but it will still be waiting for the server, so the audio play will pause until the server re-sends new audio samples. **That is pretty cool!** ‚Äì because of the way iteratees work, the stream doesn‚Äôt just die when all of the input is consumed.\n\n#### A chunker to reduce HTTP packet numbers\n\nUp to now we‚Äôve been streaming the audio in **very small chunks** because by default JSyn writes out arrays of just 8 audio samples and the `.stream()` function consumes all data as it comes. This means *a lot* of HTTP chunks per second are sent ‚Äì which is less efficient and take more bandwidth.\n\nIn order to fix this, we need to use a **buffer on the server side**. In other words, instead of sending audio samples as they come we need to **group audio samples**. We have currently grouped audio samples in arrays of 5000 which is quite reasonable (it‚Äôs about 10 chunks per second using 44100 samples/s). We can easily change this later. This logic is implemented in an `Enumeratee` we called ‚Äúchunker‚Äù. In that sense, it is reusable and modular:\n\n```scala\nval chunker = Enumeratee.grouped(  \n¬† Traversable.take[Array[Double]](5000) \u0026\u003e\u003e Iteratee.consume()  \n)\n```\n\nAnd now, we can easily plug it in like this:\n\n```scala\ndef stream = Action {  \n¬† val chunkedAudioStream = rawStream \u0026\u003e chunker \u0026\u003e audioEncoder  \n¬† Ok.stream(audioHeader \u003e\u003e\u003e chunkedAudioStream).  \n¬† ¬† ¬†withHeaders( (CONTENT_TYPE, audio.contentType) )  \n}\n```\n\n### Broadcast\n\nNow, another improvement we made was to **factorize this chunking and encoding part: avoiding having this computing tasks done for every stream consumer**.\n\nBasically, we move it out of the stream function:\n\n```scala\nval chunkedAudioStream = rawStream \u0026\u003e chunker \u0026\u003e audioEncoder  \ndef stream = Action {  \n¬† Ok.stream(audioHeader \u003e\u003e\u003e chunkedAudioStream).  \n¬† ¬† ¬†withHeaders( (CONTENT_TYPE, audio.contentType) )  \n}\n```\n\nBut to allow broadcasting, we have to use a broadcast:\n\n```scala\nval chunkedAudioStream = rawStream \u0026\u003e chunker \u0026\u003e audioEncoder  \nval (sharedChunkedAudioStream, _) = ¬†=   \n¬† Concurrent.broadcast(chunkedAudioStream)  \ndef stream = Action {  \n¬† Ok.stream(audioHeader \u003e\u003e\u003e sharedChunkedAudioStream).  \n¬† ¬† ¬†withHeaders( (CONTENT_TYPE, audio.contentType) )  \n}\n```\n\nHere we only care about the enumerator (the left argument in the Tuple2), we put the wildcard \"_\" to ignore the return value.\n\n[![](/images/2012/07/320px-Broadcast.svg_.png)][19]\n\nUsing a broadcast, generated audio samples pushed by the audio generator can be simultaneously spread to multiple consumers. This is perfect for our needs, multiple players can connect to this web radio!\n\n### Avoiding the server load\n\nThe last important fix we made was to **avoid the server load**:\n\n```scala\n¬† def stream = Action {  \n¬† ¬† Ok.stream(audioHeader \u003e\u003e\u003e sharedChunkedAudioStream   \n¬† ¬† ¬† \u0026\u003e Concurrent.dropInputIfNotReady(50)).  \n¬† ¬† ¬† ¬†withHeaders( (CONTENT_TYPE, audio.contentType) )  \n¬† }\n```\n\nIf a client is opening the stream connection but doesn‚Äôt consume enough or doesn‚Äôt consume it at all (download is paused), the server will fill in memory the chunks to send to the client and the server can reach an *out of memory* exception. To avoid that **we have to drops chunks if the consumer is not ready**. Then the client will just lose messages if it is not ready (in our case, we give them 50 milliseconds).\n\nAnd this is what `Concurrent.dropInputIfNotReady(50)` is actually doing ‚Äì with yet another **Enumeratee**! **Dropping old chunks is really what we want in an audio streaming application**: We want the consumer to subscribe to the current audio stream and not to continue from where they stopped.\n\n### Client consumers\n\n#### HTML5 Audio tag\n\nIn HTML5, we have the Audio tag ‚Äì and we can just consume our stream like this:\n\n```html\n\u003caudio src=\"/stream.wav\"\u003e\u003c/audio\u003e\n```\n\nOr if we want to make it auto loading:\n\n```html\n\u003caudio src=\"/stream.wav\" preload autoplay controls\u003e\u003c/audio\u003e\n```\n\nIt may be a bit ‚Äúwrong‚Äù to use `` for streaming, but it works because we are using it as if the server was hosting a static audio file. The only disputable hack is to have to set the max ChunkSize in the WAVE header which is 2147483647 (it‚Äôs about 6 hours 45mn!), so the browser believes the audio is not finished.\n\nThe issue we are currently facing is this crazy latency (a few seconds) between user actions and the produced sound. This problem is due to the browser audio cache buffer: if we were able to minimize it we would have an almost real-time audio player.\n\n#### Playing it with VLC\n\nThis stream is spread through HTTP so we need a HTTP client to consume it. But a HTTP client doesn‚Äôt mean only browsers! We can also use VLC for this, as if it was a web radio! One advantage of using VLC is it suffers far less latency (presumably because the cache buffer is smaller than the audio tag).\n\n![vlc](/images/2012/07/vlc.png)\n\n## Making the real-time control UI\n\nOur experiment **mixes different oscillators to generator one sound**. The web user interface allows a user to control the parameters of those. Two knobs control the **volume** and the **pitch** (tuned to a dorian mode scale) and you can select the oscillator **wave primitive** (sine, sawtooth, square, noise). It‚Äôs not fancy at the moment ‚Äì but JSYN offers a lot of features for expanding our simple demo.\n\n![](/images/2012/07/Capture-d‚Äô√©cran-2012-07-31-√†-16.42.15.png)\n\nThis interface is **multi-users**, so if you use it with other people, **the interface will stay synchronized over multiple browsers** (turn the knobs, change the wave primitive, ‚Ä¶). All this is done with WebSockets, and on the server-side it‚Äôs using, again, **Iteratees**!\n\nThe workflow is simple: When someone does some action on the user interface, events are sent to the server. These events are interpreted by the *ZoundGenerator* resulting in updates to the audio synthesis configuration. These events are then broadcast to each client, and some Javascript handlers are called in order to keep the interface synchronized.\n\n## Source code\n\n**[Fork me on Github][3]**\n\n## What‚Äôs next?\n\nThis was just a simple demo to show the power and flexibility of Play2‚Ä≤s Iteratee concept. Because of the modular nature, extending the demo is easy. For example, we could plug a new audio encoder such an OGG encoder. The code would be simple and we could even choose on a request-by-request basis which encoder to use:\n\n```scala\nimport Concurrent.broadcast  \nval (chunkedWaveStream, _) =   \n¬† broadcast(rawStream \u0026\u003e chunker \u0026\u003e waveEncoder)  \nval (chunkedOggStream, _) =   \n¬† broadcast(rawStream \u0026\u003e chunker \u0026\u003e oggEncoder)  \n  \ndef stream(format: String) = Action {  \n¬† val stream = format match {  \n¬† ¬† case \"wav\" =\u003e waveHeader \u003e\u003e\u003e chunkedWaveStream  \n¬† ¬† case \"ogg\" =\u003e oggHeader \u003e\u003e\u003e chunkedOffStream  \n¬† }  \n¬† Ok.stream(stream).  \n¬† ¬† ¬†withHeaders( (CONTENT_TYPE, audio.contentType) )  \n}\n```\n\n**Now it‚Äôs up to you!**\n\nHopefully you get a feel for the possibilities of stream processing and piping with Play. You can now reuse these concepts and make your own stuff: Maybe you don‚Äôt need to generate sounds on the fly, but instead you simply want to play a collection of audio files and stream them like radio? **Well you can make a web radio engine now!**. \n\nBut that‚Äôs just the beginning ‚Äì I would love to see someone taking the concept, and running even further‚Ä¶ Do you know that in Youtube, during the time you are uploading a video, Youtube is already re-encoding it and can start streaming it *before* the file has finished uploading? Hmm, that‚Äôs starting to sound almost simple‚Ä¶\n","data":{"title":"Zound, a PlayFramework 2 audio streaming experiment using Iteratees","description":"Zound uses an audio generator (JSyn, an audio synthesizer), encode the output and stream it all using Play Iteratees to pipe everything in real-time.","thumbnail":"/images/2012/07/ZOUND.png","author":"Gaetan","layout":"post","permalink":"/2012/08/zound-a-playframework-2-audio-streaming-experiment-using-iteratees/","tags":["audio","iteratee","playframework","hackday"]}},{"id":"2012-07-03-how-i-learned-backbone-js-three-js-glsl-in-one-week","year":"2012","month":"07","day":"03","slug":"how-i-learned-backbone-js-three-js-glsl-in-one-week","content":"\n[1]: http://youcanmakevideogames.com/\n[2]: http://7dfps.org/\n[3]: http://www.ludumdare.com/\n[4]: http://gre.github.io/dta\n[8]: http://caniuse.com/#search=webgl\n[12]: http://www.egonelbre.com/js/jsfx/\n[16]: http://www.opengl.org/documentation/glsl/\n[17]: http://glsl.heroku.com/\n[19]: https://github.com/gre/dta/blob/master/app/models/models.scala\n[20]: http://playframework.org/\n\n![](/images/2012/07/dta1.png)\n\nLast week was the [7dfps challenge][2], an open challenge where participants had to make a FPS in only one week.\nSuch contest are very very interesting for those who want to experiment with things. Challenging yourself is IMO the best way to learn new things. You may also know the famous [‚ÄúLudum Dare‚Äù contest][3].\n\nI learned to use Backbone.js and Three.js (a famous library on top of WebGL) in only one week, so you have no excuse to not be able to do the same!\n\n[-\u003e You can make games!][1]\n\n\u003e ‚ÄúIf Lawnmower Man f\\*\\*\\*\\*d Tron on the bonnet of a tank‚Äù  \n\u003e **_YouBigNugget_**\n\nI‚Äôve only used web technologies, no need of any plugin, just a recent browser like Chrome / Firefox.\n\nThis is the result:\n\n\u003ciframe width=\"640\" height=\"360\" src=\"http://www.youtube.com/embed/g9CldBI9C6E?feature=player_embedded\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\nand you can play it here:\n\n**[Play the game][4]**\n\n\u003c!--more--\u003e\n\n## Overview of a one week game development\n\n### Backbone.js, for the model, events and class inherence\n\nI‚Äôm a fan of the ‚Äúdo it yourself‚Äù idea, not using any library or only using small 140byt.es codes. But the frequent issue with that is (1) always doing the same thing again and again, (2) taking a lot of time on the architecture, and not finishing anything. When you have a due date, relying on libraries and frameworks could save you a lot of time.  \nI‚Äôve choose Backbone.js for its Model architecture with class inherence, a get/set system, and also its event system bound to model instances, the destroy function with the ‚Äúdestroy‚Äù event to bind on,‚Ä¶ It was also a new library to learn for me.\n\n#### Models\n\nHere is the class diagram of the game:\n\n![not available](/images/2012/07/f5dea341.jpg)\n\n### Learning Three.js, a 3D library on top of WebGL.\n\nWebGL is [more and more supported by browsers][8].  \nWebGL means OpenGL in the web. It allows to make efficient and hardware accelerated 3D computation.\n\nHere is the same, not using any library but using pure WebGL wasn‚Äôt possible in one week! This is why I‚Äôve chosen to use Three.js, probably today the most popular WebGL library.  \nI was impressed how Three.js is finally not so hard to use, knowing some basic 3D concepts from my old Blender days.\n\nYou create a `Scene`, add a `Camera`, add Meshes to the scene, A `Mesh` has a `Material` and a `Geometry`, ‚Ä¶ everything very straighforward.\n\nOne challenging part was when trying to compute global world position relative to a given object position, for instance to compute the Bullet initial position and orientation from the Tank position and orientation.\n\nFortunately I‚Äôve got some help ![:)][4] and got the answer: multiply your vector with the worldMatrix of the mesh.\n\n\u003cblockquote class=\"twitter-tweet\" data-in-reply-to=\"212609230891524096\" width=\"550\" lang=\"fr\"\u003e\u003cp\u003e@\u003ca href=\"https://twitter.com/greweb\"\u003egreweb\u003c/a\u003e Should be calculatable quite easily. Multiply the position vector by the object‚Äôs matrixWorld property.\u003c/p\u003e\n\u003cp\u003e\u0026mdash; Paul Lewis (@aerotwist) \u003ca href=\"https://twitter.com/aerotwist/status/212617930024816641\" data-datetime=\"2012-06-12T18:50:39+00:00\"\u003eJuin 12, 2012\u003c/a\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\n\u003cblockquote class=\"twitter-tweet\" data-in-reply-to=\"212609230891524096\" width=\"550\" lang=\"fr\"\u003e\u003cp\u003e@\u003ca href=\"https://twitter.com/greweb\"\u003egreweb\u003c/a\u003e var position = new THREE.Vector3().getPositionFromMatrix( object.matrixWorld );\u003c/p\u003e\n\u003cp\u003e\u0026mdash; Mr.doob (@mrdoob) \u003ca href=\"https://twitter.com/mrdoob/status/212648943673290752\" data-datetime=\"2012-06-12T20:53:53+00:00\"\u003eJuin 12, 2012\u003c/a\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\n### Playing with AI\n\nLike TankKeyboardControls, I‚Äôve created a new ‚ÄúTankControls‚Äù for computer tanks: **TankRandomControls** was the first dumb one, it just does random things:\n\n```javascript\nfunction TankRandomControls () {\nvar self = this;\nself.moveForward = false;\nself.moveBackward = false;\nself.moveLeft = false;\nself.moveRight = false;\nself.fire = false;\nself.fireMissile = false;\nvar i = ;\nsetInterval (function () { // take random decisions every 0.5 second\n  i;\nself.moveForward = i%3== \u0026\u0026 Math.random() \u003e 0.2;\nself.moveBackward = !self.moveForward \u0026\u0026 Math.random() \u003e 0.5;\nself.moveLeft = Math.random() \u003c 0.1;\nself.moveRight = Math.random() \u003c 0.1;\nself.fire = Math.random() \u003e 0.2;\nself.fireMissile = Math.random() \u003c 0.2;\n}, 500);\n}\n```\n\n**TankRemoteControls** was the second one using some simple AI rules:\n\n- Try to avoid walls and objects\n- Either do random things or target a tank (more likely)\n- Target the closest tank, update target every 5 seconds\n- Turn the tank to the target tank, shoot bullets and missiles\n- Move forward if the target is far away / Move backward if the target is too close\n\nSee the source code here: .\n\n### Sounds\n\nI‚Äôve used [JSFX][12], an experimental library, were you can generate sounds based on a few parameters.  \nIt‚Äôs a 8-bit sound generator perfect for generate old-school sounds.\n\nSo you can create each sound in Javascript like this:\n\n```javascript\nSOUNDS = {\n  bullet: jsfxlib.createWave([\n    \"noise\",\n    7.0,\n    0.18,\n    0.0,\n    0.082,\n    0.0,\n    0.222,\n    20.0,\n    800,\n    2400.0,\n    -0.428,\n    0.0,\n    0.0,\n    0.01,\n    0.0003,\n    0.0,\n    0.0,\n    0.0,\n    0.0,\n    0.0,\n    0.0,\n    0.0,\n    0.0,\n    0.992,\n    0.0,\n    0.0,\n    0.22,\n    0.0,\n  ]),\n  missile: jsfxlib.createWave([\n    \"noise\",\n    0.0,\n    0.12,\n    0.0,\n    0.546,\n    0.0,\n    0.856,\n    64.0,\n    250,\n    1063.0,\n    0.28,\n    0.086,\n    0.024,\n    5.4329,\n    0.3565,\n    0.466,\n    -0.638,\n    0.058,\n    0.008,\n    0.0,\n    0.0,\n    -0.114,\n    0.216,\n    0.98,\n    -0.984,\n    1.0,\n    0.307,\n    0.988,\n  ]),\n  explosion: jsfxlib.createWave([\n    \"noise\",\n    1.0,\n    0.4,\n    0.02,\n    0.682,\n    1.746,\n    1.97,\n    100.0,\n    378.0,\n    2242.0,\n    -0.55,\n    -0.372,\n    0.024,\n    0.4899,\n    -0.1622,\n    0.262,\n    0.34,\n    0.724,\n    0.0205,\n    -0.102,\n    0.0416,\n    -0.098,\n    0.1,\n    0.805,\n    0.094,\n    0.428,\n    0.0,\n    -0.262,\n  ]),\n  collideWall: jsfxlib.createWave([\n    \"noise\",\n    0.0,\n    0.4,\n    0.0,\n    0.056,\n    0.0,\n    0.296,\n    20.0,\n    560.0,\n    2400.0,\n    -0.482,\n    0.0,\n    0.0,\n    0.01,\n    0.0003,\n    0.0,\n    0.0,\n    0.0,\n    0.0,\n    0.0,\n    0.0,\n    0.0,\n    0.0,\n    1.0,\n    0.0,\n    0.0,\n    0.0,\n    0.0,\n  ]),\n};\n// ...\n// SOUNDS.bullet.play()\n```\n\nThe jsfxlib.createWave function returns a HTML5 Audio element and you can play with its API (.play(), .pause(), ‚Ä¶).  \nBut doing this way, you will not be able to play a sound at the same time so I‚Äôve made a buffer system which duplicate N times the sound in different audio elements.  \nI have also try to generate different sounds to randomize it a little.\n\nSee the full source code here: .\n\n### Post Processing Effects with GLSL shaders\n\nI was very impressed by the power of GLSL for its possibilities and efficiency.  \nGLSL are definitely the indispensable thing you need to add a better atmosphere in your games.\n\n#### Before\n\n![](/images/2012/06/before.png)\n\n#### After\n\n![](/images/2012/06/after_.png)\n\nand when getting shot:\n\n![](/images/2012/06/after.png)\n\nThese effects are done by combining these two GLSL effects.  \nHere is the GLSL code of these effects.\n\n#### The radio noise effect\n\n\u003cscript src=\"https://gist.github.com/2950478.js?file=radionoise.frag\"\u003e\u003c/script\u003e\n\n#### The shot interference\n\n\u003cscript src=\"https://gist.github.com/2950478.js?file=perturbation.frag\"\u003e\u003c/script\u003e\n\n### Wait! what is this crazy ‚ÄúGLSL‚Äù language?\n\nGLSL is an OpenGL language close to the C syntax design to have a direct control of the graphic pipeline. You can directly add passes to the rendering process with GLSL shaders.  \nGLSL gives you a lot of very useful types (like vectors, matrix, ‚Ä¶) and functions (like smoothstep, ‚Ä¶). [Read the spec][16] to know more about it.  \nI also recommend you to experiment the [GLSL Sandbox][17]. You can see awesome demos and their codes shared by people made with GLSL. It also have an online IDE to easily code and instantly test your shaders.\n\n### WebGL and Three.js integration\n\nYou can use GLSL in different ways with Three.js. You can add a GLSL shader to an object material, and you can also add GLSL shaders on top of the Canvas (post-processing). We will only see how to add GLSL shaders as a post processing render on the entire Canvas.\n\nFortunately Three.js have some utils classes to make the GLSL shaders integration easier.\n\nI have been inspired from the awesome work done here: .\n\nThere is a lot of code out there, so I‚Äôve try to extract the minimum required for mixing the 2 GLSL shaders for a post-processing on the entire canvas of a Three.js scene:\n\n\u003ciframe style=\"width: 100%; height: 400px;\" src=\"http://jsfiddle.net/jggvJ/7/embedded/\" frameborder=\"0\" width=\"320\" height=\"240\"\u003e\u003c/iframe\u003e\n\nAs you can see, you can easily inject your own Javascript variables in a GLSL shader to make a bridge between the JS and the GLSL code and so having quite generic and configurable shaders.  \nQuite cool!\n\n### Adding game UI\n\nWeb, with HTML and CSS, allows to have different containers, layers, positioning systems,‚Ä¶  \nPerfect! For the game UI, we used different elements:\n\n![](/images/2012/06/ui.png)\n\nRadar and Damage are implemented with an independent Canvas while Level is a simple text div.\n\nFor instance, this is Damage (I called LifeIndicator):\n\n```javascript\n(function(){\n\nvar LifeIndicator = function (nodeId) {\nthis.canvas = document.getElementById(nodeId);\nthis.ctx = this.canvas.getContext(\"2d\");\nthis.life = 1;\nthis.render();\n}\n\nLifeIndicator.prototype.setLife = function (life) {\nthis.life = life;\nthis.render();\n}\n\nLifeIndicator.prototype.render = function () {\nvar c = this.ctx;\nvar w = c.canvas.width, h = c.canvas.height;\nc.clearRect(, , w, h);\n// TODO\nvar g = Math.floor(255*this.life);\nvar r = 255-g;\nc.fillStyle = \"rgb(\" r \",\" g \",0)\";\nvar wt = 6;\nvar ht = 12;\nc.fillRect((w-wt)/2, , wt, ht);\nc.fillRect(, ht, w, h);\n}\n\nwindow.LifeIndicator = LifeIndicator;\n\n}());\n```\n\n## Play Framework integration\n\nI was planning to make a multiplayer game but I couldn‚Äôt find the time for this.  \nI use Play Framework 2 and its power and concepts for handling streams (even if I don‚Äôt use it yet).  \nThe only part I can show you for now is the game map generation. For making a multiplayer game, I needed to have the game state on server side to be able to submit game infos (map, players, ‚Ä¶) to new players.\n\nThis is a few scala code to generate a nice distribution of random objects in the map:\n\n```scala\nobject Game {\n¬† def createRandom: Game = {\n¬† ¬† val random = new Random()\n¬† ¬† val half = 5000\n¬† ¬† val size = 2*half\n¬† ¬† val split = 4\n¬† ¬† val objects =\n¬† ¬† Range(, split).map { xi =\u003e\n¬† ¬† ¬† Range(, split).map { yi =\u003e\n¬† ¬† ¬† ¬† val s = size.toDouble / split.toDouble\n¬† ¬† ¬† ¬† val x = -half.toDouble math.round(s*(xi random.nextDouble));\n¬† ¬† ¬† ¬† val y = -half.toDouble math.round(s*(yi random.nextDouble));\n¬† ¬† ¬† ¬† var sizeRandom = random.nextDouble\n¬† ¬† ¬† ¬† val w = 200.*math.ceil((1-sizeRandom)*8.);\n¬† ¬† ¬† ¬† val h = 200.*math.ceil(sizeRandom*8.);\n¬† ¬† ¬† ¬† RectObj(Vec3(x,y,), w, h)\n¬† ¬† ¬† }.toList\n¬† ¬† }.toList.flatten\n¬† ¬† Game( (Vec3(-half, -half, ), Vec3(half, half, )), List(), objects, List() )\n¬† }\n}\n\ncase class Game (bounds: Tuple2[Vec3, Vec3], chars: List[Char], objects: List[GameObj], dynamics: List[GameDyn])\n...\n```\n\n[See the full source][19] (some code may not be used).\n\n## Still an unfinished game\n\nThere is more things to do now:\n\n- More visual effects\n- More sound effects\n- A better collision system (using a physic engine?)\n- Multiplayer\n- A better gameplay with some goals, different kind of games, ‚Ä¶\n\n### Initial multiplayer objective rescheduled\n\nMy initial game release was focus on the game core, graphisms and gameplay.\n\nI was unfortunately not ready enough to make the multi-player real-time part for the first week sprint but I have some though on the subject.  \nI was asking myself what are the best way to make the game synchronisation between clients while trying to keep as much client-side code as possible. I wanted a scalable decentralized game. I had some though on how to solve this issue. For instance, when a client shoots, he sends a ‚Äúshoot‚Äù event with the timestamp, server just streams events (with a tick frequency) sent by clients and clients replay the game exactly the same way everywhere (using all these time-based events) with some interpolation with the current time.  \nI need to think more about this now, and try to use [PlayFramework][20].\n\n## Source code\n\n[http://github.com/gre/dta](http://github.com/gre/dta)\n\n**[EDIT] You may be mainly interested by the [main.js][21]. It shows how powerful the even-driven programming is, for plugging components together.  \n**\n\n[21]: https://github.com/gre/dta/blob/master/app/assets/javascripts/main.js\n\n## Special thanks\n\n[@mrspeaker][22] for helping me with Three.js \u0026 also collision system,  \n[@mrdoob][23] \u0026 [@aerotwist][24] for replying to my newbies questions,  \nguys on IRC (#three.js),  \nand of-course 7dfps guys.\n\n[22]: http://twitter.com/mrspeaker\n[23]: http://twitter.com/mrdoob\n[24]: http://twitter.com/aerotwist\n","data":{"title":"How I learned Backbone.js, Three.js, GLSL in one week","author":"Gaetan","description":"I learned to use Backbone.js and Three.js (a famous library on top of WebGL) to make a FPS in only one week.","thumbnail":"/images/2012/07/dta1.png","layout":"post","permalink":"/2012/07/how-i-learned-backbone-js-three-js-glsl-in-one-week/","tags":["gamedev","GLSL","WebGL"]}},{"id":"2012-05-12-minimize-your-javascript-files-with-curl","year":"2012","month":"05","day":"12","slug":"minimize-your-javascript-files-with-curl","content":"\nI‚Äôve always been fascinated by the power of **using existing web applications as external tools**: you don‚Äôt need to install anything on your computer but you can **rely on the web**.\n\nWe can **externalize the intelligence of applications in servers** and **easily make updates**, while having any terminal consuming them with a **minimal OS environment**.  \n*Cloud* or whatever you call it, it‚Äôs awesome.\n\nWOA is our common architecture for making applications. Clients of web servers can be anything you want, not only desktop browsers, but also mobiles, tablets, other web services, and‚Ä¶ **command-line**!\n\nAnd today, as an example, we will use [Google Closure Compiler web service][1] to **minimize a Javascript file with only cURL**.\n\n [1]: https://developers.google.com/closure/compiler/docs/api-ref\n\n\u003c!--more--\u003e\n\n**cURL** is a CLI swiss army knife of transferring data and it is perfect for testing, debugging and consuming web services.\n\n**Google Closure Compiler** check and ‚Äúcompile‚Äù your Javascript file. By compile, it means optimizing its size by renaming variables and removing spaces and comments. Javascript compilation has because an essential phase of major javascript libraries.\n\n## Bash script examples\n\n### Download and minimize the last version of Illuminated.js\n\n```bash\nURL=https://raw.github.com/gre/illuminated.js/master/src/illuminated.js  \nOUTPUT=illuminated.min.js  \ncurl -d compilation_level=SIMPLE_OPTIMIZATIONS -d output_format=text -d output_info=compiled_code -d code_url=$URL http://closure-compiler.appspot.com/compile \u003e $OUTPUT\n```\n\n### Minimize a local JS file\n\n```bash\nLOCAL_FILE=./mysuperlib.js  \nOUTPUT=mysuperlib.min.js  \ncurl -d compilation_level=SIMPLE_OPTIMIZATIONS -d output_format=text -d output_info=compiled_code --data-urlencode \"js_code@${LOCAL_FILE}\" http://closure-compiler.appspot.com/compile \u003e $OUTPUT\n```\n","data":{"title":"Minimize your Javascript files with cURL","description":"use Google Closure Compiler web service to minimize a Javascript file with only cURL.","author":"Gaetan","layout":"post","permalink":"/2012/05/minimize-your-javascript-files-with-curl/","tags":["javascript","linux"]}},{"id":"2012-05-10-illuminated-js-2d-lights-and-shadows-rendering-engine-for-html5-applications","year":"2012","month":"05","day":"10","slug":"illuminated-js-2d-lights-and-shadows-rendering-engine-for-html5-applications","content":"\n[1]: http://bit.ly/LZ2dq1\n[2]: http://gre.github.io/illuminated.js\n[3]: http://github.com/gre/illuminated.js\n[4]: /2012/05/illuminated-js-2d-lights-and-shadows-rendering-engine-for-html5-applications/#gettingstarted\n[5]: /2012/05/illuminated-js-2d-lights-and-shadows-rendering-engine-for-html5-applications/#underthehood\n[6]: http://en.wikipedia.org/wiki/Canvas_element\n[7]: http://gre.github.io/illuminated.js/gettingstarted.html\n[8]: http://en.wikipedia.org/wiki/Ray_tracing_(graphics)\n[9]: /images/2012/05/step11.jpg\n[10]: /images/2012/05/step21.jpg\n[11]: /images/2012/05/step31.jpg\n[12]: /images/2012/05/step4.jpg\n[13]: http://blog.marmakoide.org/?p=1\n[14]: /images/2012/05/sampling.jpg\n[15]: http://en.wikipedia.org/wiki/Tangent_lines_to_circles\n\n[![](/images/2012/05/illuminatedjs.jpg)][1]\n\n[Click on the image to open it!][1]\n\n## Wow! what‚Äôs this?\n\nIt‚Äôs a **2D scene** containing 2 **lights** and 13 different **objects** rendered in **real-time** by a **Javascript library** I made called **Illuminated.js**.\n\nThe library is designed to add some **awesome effects to your existing applications**. Adding **a cool atmosphere for your applications and games** can make the difference!\n\n**[Try the editor][2]** and **[Get the source code][3]**.\n\nIn this article, we will introduce the basic usages of _Illuminated.js_ and APIs, and then explain how the engine works step-by-step.\n\n- [API ‚Äì Getting started][4]\n- [Technical notes ‚Äì how does it work?][5]\n\n\u003c!--more--\u003e\n\n## How can I use it?\n\nThe library uses [HTML5 Canvas][6] to draw lights and shadows ‚Äì so you can simply drop it straight into your existing Canvas applications: you just need to add some code in your render function and maintaining a binding between your application logic and the _Illuminated.js_ objects.  \nNot using canvas? No worries! In theory, if you have an existing application or game made in full DOM, you could use _Illuminated.js_ behind this, playing with z-index.\n\n## \u003ca id=\"gettingstarted\"\u003e\u003c/a\u003e Getting started\n\n### Basic concepts\n\nAll the classes of the package live in `window.illuminated`.\n\nA **Light** describes a light emit source.  \nAn **OpaqueObject** specifies an 2D object used by a Lighting.  \nA **Lighting** defines the lighting of a light through a set of opaque objects, each object stops the light and casts shadows.  \nA **DarkMask** defines a dark layer which hides dark area not lighted by a set of lights. It should be drown on the top-layer to hide objects which are far from the light. This effect produces a better atmosphere and is perfect for game where light are essential (where hiding invisible area is part of the difficulty).\n\n### Example of a basic scene rendering\n\n[  \nClick here to open this example.  \n![](/images/2012/05/gettingstarted.jpg)\n][7]\n\n## Lights and Objects\n\n### Vec2\n\n```javascript\nnew Vec2(x, y);\n```\n\nVec2 represents a 2d position or a 2d vector. It is used everywhere in _Illuminated.js_.\n\nVec2 is inspired from Box2d‚Äôs Vec2 except that in _Illuminated.js_ a Vec2 vector is immutable. It means every methods create a new Vec2 instance and you can safely use a same Vec2 instance everywhere because the immutability guarantees the non-modification of properties.\n\n### Lights\n\nFor now, we have only implemented one kind of light: a **Lamp** which is basically a radial gradient. A Lamp can also be ‚Äúoriented‚Äù, it means lighting more far in a given direction.\n\n#### Lamp\n\n```javascript\nnew Lamp();\n\nnew Lamp({ position: new Vec2(12, 34) });\n```\n\nevery parameters:\n\n```javascript\nnew Lamp({\n¬† position: new Vec2(12, 34),\n¬† distance: 100,\n¬† diffuse: 0.8,\n¬† color: 'rgba(250,220,150,0.8)',\n¬† radius: ,\n¬† samples: 1,\n¬† angle: ,\n¬† roughness:\n})\n```\n\nIt defines a **Lamp** placed at a **position**, with a maximum emiting **distance**, a **diffuse** parameters to define the light penetration in objects.  \nThe **radius** defines the size of the light. Bigger the size is, Higher shadows are smoothed. The **samples** is an important parameters to define the quality of this smooth.  \nThe **angle** and **roughness** parameters are used for oriented lamp: angle defines the orientation while roughness defines the roughness of the effect.\n\n### Light methods\n\nYou can easily create your own Light type by implementing its methods.\n\n#### .mask(ctx)\n\nRender a mask representing the visibility (used by DarkMask).\n\n#### .render(ctx)\n\nRender the light (without any shadows).\n\n#### .bounds()\n\nReturn the Rectangle bound of the light representing where the light emission limit. `{ topleft: vec2, bottomright: vec2 }`\n\n#### .forEachSample(fn)\n\nApply a function fn for each light sample position. By default it‚Äôs called once with the light position.\n\n### Opaque Objects\n\nIn _Illuminated.js_, an object which cast shadows is called an opaque object. That‚Äôs why every types inherits OpaqueObject.\n\nDiscObject and PolygonObject are the two available primitive objects.\n\n#### DiscObject\n\nA ‚ÄúDiscObject‚Äù is basically a 2D circlar object. You must define its center **position** and its **radius**:\n\n```javascript\nnew DiscObject({ position: new Vec2(80, 50), radius: 20 });\n```\n\n#### PolygonObject\n\nPolygonObject also has some derivated classes you can use: **RectangleObject**, **LineObject**.\n\nYou can instanciate these different objects like this:\n\n```javascript\nnew PolygonObject([ new Vec2(, ), new Vec2(10, 10), ... ]) // an array of points\nnew RectangleObject(topleft, bottomright) // topleft and bottomright positions of the rectangle\nnew LineObject(a, b) // an object defined by the line from a to b.\n```\n\n### OpaqueObject methods\n\nYou can easily create your own object type by implementing OpaqueObject methods.\n\n#### .bounds()\n\nReturn the Rectangle bound of the object. `{ topleft: vec2, bottomright: vec2 }`\n\n#### .contains(point)\n\nReturn `true` if the object contains a **point**.\n\n#### .path(ctx)\n\nBuild the path of the object shape in a 2d context **ctx**.\n\n#### .cast(ctx, origin, bounds)\n\nFill every shadows with **ctx** projected by the **origin** point in the object and in a given **bounds**.\n\n## Lighting and DarkMask\n\nPrevious defined classes was representing datas we will now use to perform lightings and masks.\n\n### Lighting\n\nA Lighting defines the lighting of one light through a set of opaque objects.\n\n```javascript\nnew Lighting({ light: light, objects: [ object1, object2, ... ] })\n```\n\n#### .compute(width, height)\n\nwill compute shadows casting.\n\n#### .cast(ctx)\n\nwill draw black shadows on the **ctx** canvas 2d context.  \nYou usually don‚Äôt have to use it if you use `render()`.\n\n#### .render(ctx)\n\nwill draw the light with its shadows on **ctx** canvas 2d context.\n\n### DarkMask\n\nA DarkMask defines a dark layer which hides dark area not lighted by a set of lights.\n\n```javascript\nnew DarkMask({ lights: [light1, light2, ...], color: 'rgba(0,0,0,0.9)' })\n```\n\n#### .compute(width, height)\n\nwill compute the dark mask.\n\n#### .render(ctx)\n\nwill draw the computed dark mask on **ctx** canvas 2d context.\n\n### about compute and render\n\nBoth Lighting and DarkMask objects have `compute()` and `render()` methods.\n\nWe think that **you** know the best when to recompute the lights because it‚Äôs closely link to the application you are making (we will not check at each time if something has changed, you know it).  \nCall the `compute()` method when something has changed in your scene so we can recompute lights and shadows.\n\n## \u003ca id=\"underthehood\"\u003e\u003c/a\u003e How does it work under the hood?\n\n_Illuminated.js_ divides its work into several layers.\n\n### Real-time example\n\n\u003ciframe src=\"http://gre.github.io/illuminated.js/howdoesitwork.html\" border=\"0\" height=\"2700\" width=\"450\"\u003e\u003c/iframe\u003e\n\n### The art of composing layers\n\nThe layers are all stored in a Canvas which allows us to cache it. The light is drawn using a Canvas Radial Gradient in a cache canvas only once. This is interesting because canvas gradient are processor intensive  \nAt the end, layers are combine on the global canvas with `drawImage`.  \nBut the library lets you reuse these layers to combine them the way you want.\n\nCanvas‚Äô `globalCompositeOperation` is very useful to compose layers together.  \nFor instance, in the following example, the ‚ÄúLight shadow casting‚Äù layer is combined with the ‚ÄúLight rendering‚Äù layer to generate the ‚ÄúLight rendering with shadows‚Äù layer. The composition mode used is ‚Äúdestination-out‚Äù which remove the color of the destination image where the source image has color.\n\n```javascript\nlight.render(ctx);\nctx.globalCompositeOperation = \"destination-out\";\nthis.cast(ctx);\n```\n\nAnother very useful composite operation is `\"lighter\"` which adds color values. It is used to combine two lightings.\n\n### How shadows are projected\n\nSome rendering engine use [ray tracing][8] to render a scene, a concept very close to physics which trace from a light source a lot of rays with different paths which will collide with object and will be subject of absorption/diffraction/reflexion in accordance with the object properties‚Ä¶  \nRay casting is a very **realistic** rendering solution **but consuming** (you need a lot of rays to avoid noises in the result image).  \n_Illuminated.js_ doesn‚Äôt use ray tracing because it aims to be efficient for a real-time usage. It uses some heuristics for casting shadows.\n\n#### Let‚Äôs see how shadows are projected for a polygon object.\n\nWe have a scene with a light and a triangle.\n\n![][9]\n\nWe select each edge of the polygon which is visible by the light (and in the light bounds).\n\n![][10]\n\nFor every selected edge, we project it to generate a polygon area.\n\n\u003e **N.B.** In the current implementation, we generate an hexagon projection to ensure it goes outside of the light bounds because a quadrilateral didn‚Äôt garantee it, if a light is very close to it. The projecting vector used is enough big to work for most case, but it‚Äôs still an heuristic.\n\n![][11]\n\nWe draw black color in this polygon area. Some improvments can be made by not drawing black in the shape / ajusting the opacity of the color.\n\n![][12]\n\nFor casting blured shadows, we repeat this algorithm for each ‚Äúsamples‚Äù of the light. Samples are distribute around the light with a [‚Äúspiral algorithm‚Äù][13].\n\n![][14]\n\n```javascript\nvar GOLDEN_ANGLE = Math.PI * (3 - Math.sqrt(5));\nLamp.prototype.forEachSample = function (f) {\n  for (var s = 0; s \u003c this.samples; ++s) {\n    var a = s * GOLDEN_ANGLE;\n    var r = Math.sqrt(s / this.samples) * this.radius;\n    var delta = new Vec2(Math.cos(a) * r, Math.sin(a) * r);\n    f(this.position.add(delta));\n  }\n};\n```\n\n## To be continued‚Ä¶\n\nThe current version of _Illuminated.js_ needs more work, I‚Äôm aware of some bugs and some parts I need to improve:\n\n- Implementing new kinds of lights like ‚ÄúSpot‚Äù, ‚ÄúNeon‚Äù, ‚Ä¶\n- The dark mask doesn‚Äôt follow the Lamp orientation.\n- The shadow casting of Circle objects are not projected nicely, I need to compute [tangent lines to the circle][15].\n- Shadows go sometimes wrong especially when having objects behind objects\n- The shadow sampling implementation is a bit hacky and wrong (changing the samples parameter changes the shadow opacity‚Ä¶)\n\n## Get involved\n\n[Try the editor][2] and [Get the source code][3].\n\nThis article is translated to [Serbo-Croatian](http://science.webhostinggeeks.com/masina-za-renderovanje) language by Jovana Milutinovich from Webhostinggeeks.com.\n","data":{"title":"Illuminated.js ‚Äì 2D lights and shadows rendering engine for HTML5 applications","description":"Illuminated.js is designed to add some awesome effects to your existing applications. Adding a cool atmosphere for your applications and games can make the difference!","thumbnail":"/images/2012/05/illuminatedjs.jpg","author":"Gaetan","layout":"post","permalink":"/2012/05/illuminated-js-2d-lights-and-shadows-rendering-engine-for-html5-applications/","tags":["gamedev","canvas","javascript","library"]}},{"id":"2012-05-03-html5-canvas-as-a-color-converter","year":"2012","month":"05","day":"03","slug":"html5-canvas-as-a-color-converter","content":"\n [2]: http://gre.github.io/illuminated.js/\n [3]: https://github.com/bgrins/spectrum\n [4]: http://www.w3.org/TR/css3-color/\n\n\u003cimg src=\"/images/2012/05/color-alpha-options.png\" alt=\"\" class=\"thumbnail-left\" /\u003e\n\nI‚Äôm currently working on the User Interface of a scene editor for my [Illuminated.js library][2] with some color and alpha picker.\n\nHTML5 now have the `\u003cinput type=\"color\" /\u003e` and `\u003cinput type=\"range\" /\u003e` which is nice. It works on Chrome and there are some [polyfills][3] to make it working on older browsers.\n\nWe will now see how we can easily **retrieve a rgba color from such an UI**, regardless of the color format given by the color picker and **combine the alpha component from the alpha range picker**.\n\n\u003e We can implement an **anythingToRGBA converter** in 10 lines of Javascript!\n\n## What?\n\nBasically, for instance, you have this: `\"#ff6432\"` and `0.8`\n\nand you want this: `\"rgba(255,100,50,0.8)\"`\n\nwhich is this color: \u003cspan style=\"background: #ff6432; display: inline-block; width: 50px\"\u003e\u0026nbsp;\u003c/span\u003e.\n\n\u003e Well, of course, we could use a library with regexp parsers!\n\nBut there is a lot of different formats available especially if you want to convert a color from [CSS][4]!\n\nOnly for the \u003cspan style=\"color:blue\"\u003eblue\u003c/span\u003e color, you have at least 7 different representations: `#00F`, `#0000FF`, `rgb(0,0,255)`, `rgba(0,0,255,1)`, `hsl(255,100%,50%)`, `hsla(255,100%,50%,1)`,  \nand‚Ä¶ `blue`!\n\n\u003e Ouch, so let‚Äôs make a huge converter library!\n\nNope! \n\nAll of these are color formats are supported by CSS and also Canvas.  \n**So, why not just re-using what the browser can do?**\n\n\u003c!--more--\u003e\n\n\n## How?\n\nBecause we have access to Canvas in Javascript, **we can implement an anythingToRGBA converter in a few line of Javascript**:\n\n```javascript\nvar getRGBA = (function(){  \n¬† var canvas = document.createElement(\"canvas\");  \n¬† canvas.width = canvas.height = 1;  \n¬† var ctx = canvas.getContext(\"2d\");  \n¬† return function (color, alpha) {  \n¬† ¬† ctx.clearRect(,,1,1);  \n¬† ¬† ctx.fillStyle = color;  \n¬† ¬† ctx.fillRect(,,1,1);  \n¬† ¬† var d = ctx.getImageData(,,1,1).data;  \n¬† ¬† return 'rgba(' [ d[], d[1], d[2], alpha ] ')';  \n¬† }  \n}());\n```\n\nYou have now a ready to use Javascript color library! \n\n`getRGBA(\"#ff6432\", 0.8)` will returns `\"rgba(255,100,50,0.8)\"`.  \n`getRGBA(\"red\", 0.5)` will returns `\"rgba(255,0,0,0.5)\"`.\n\nYou can ‚Äústandardize‚Äù your color and use it anywhere!\n\n**Feel free to adapt the code to any other desired format.**\n\nWe can easily make the reverse (give a rgba color and get the #RRGGBB and alpha values):\n\n```javascript\nvar extractColorAndAlpha = (function(){  \n¬† var canvas = document.createElement(\"canvas\");  \n¬† canvas.width = canvas.height = 1;  \n¬† var ctx = canvas.getContext(\"2d\");  \n  \n¬† function toHex (value) {   \n¬† ¬† var s = value.toString(16);   \n¬† ¬† if(s.length==1) s = \"0\" s;  \n¬† ¬† return s;  \n¬† }  \n  \n¬† return function (color) {  \n¬† ¬† ctx.clearRect(,,1,1);  \n¬† ¬† ctx.fillStyle = color;  \n¬† ¬† ctx.fillRect(,,1,1);  \n¬† ¬† var d = ctx.getImageData(,,1,1).data;  \n¬† ¬† return {  \n¬† ¬† ¬† color: \"#\" toHex(d[]) toHex(d[1]) toHex(d[2]),  \n¬† ¬† ¬† alpha: Math.round(1000*d[3]/255)/1000  \n¬† ¬† };  \n¬† }  \n}());\n```\n","data":{"title":"HTML5 Canvas as a color converter","author":"Gaetan","description":"We can implement an anythingToRGBA converter in 10 lines of Javascript!","thumbnail":"/images/2012/05/color-alpha-options.png","layout":"post","permalink":"/2012/05/html5-canvas-as-a-color-converter/","tags":["canvas","javascript","color"]}},{"id":"2012-04-26-work-in-progress","year":"2012","month":"04","day":"26","slug":"work-in-progress","content":"\n**Did you know browsers now have a built-in HTML tag for making progress bar?**\n\n\u003cprogress style=\"width: 50%\"\u003e(progress is not supported)\u003c/progress\u003e\n\nHow cool is that!\n\nIt is perfect for making web applications loading bar in just one line of HTML and a few Javascript code.\n\nA progress tag will be displayed on recent browsers with a OS-native progress bar representing a loading. Like many HTML tag, if it is not supported, it fallbacks nicely by displaying its inner content. This fallback content should either be your own designed progress bar or simply display a percentage.\n\nIt is today supported by Firefox 9 , Chrome, Opera and IE10.\n\n\u003c!--more--\u003e\n\n## Example\n\n```html\n\u003cprogress value=\"23\" max=\"100\"\u003e23 %\u003c/progress\u003e\n```\n\n### On your browser:\n\n\u003cprogress value=\"23\" max=\"100\"\u003e23 %\u003c/progress\u003e\n\n### On Linux / Firefox (with GNOME)\n\n![](/images/2012/04/progress.png)\n\n### On Mac OS / Chrome:\n\n![](/images/2012/04/progress_mac.png)\n\n### On IE 6:\n\n![](/images/2012/04/progress_ie.png)\n\n## Let‚Äôs see some cases:\n\n### waiting\n\n\u003cprogress max=\"1000\"\u003e\u003c/progress\u003e\n\n```html\n\u003cprogress max=\"1000\"\u003e\u003c/progress\u003e\n```\n\n### starting\n\n\u003cprogress value=\"0\" max=\"1000\"\u003e\u003c/progress\u003e\n\n```html\n\u003cprogress value=\"0\" max=\"1000\"\u003e\u003c/progress\u003e\n```\n\n### in progress:\n\n\u003cprogress value=\"500\" max=\"1000\"\u003e\u003c/progress\u003e\n\n```html\n\u003cprogress value=\"500\" max=\"1000\"\u003e\u003c/progress\u003e\n```\n\n### finished:\n\n\u003cprogress value=\"1000\" max=\"1000\"\u003e\u003c/progress\u003e\n\n```html\n\u003cprogress value=\"1000\" max=\"1000\"\u003e\u003c/progress\u003e\n```\n\n## Making a download bar\n\nWhen you need to load big resource like images, videos, or 3D materials, you usually want to display the progress of the download.  \nYou could still do it using some divs and CSS Javascript, but this is now much simpler to use a :\n\n### One line of HTML:\n\n```html\n\u003cprogress id=\"download\"\u003e\u003c/progress\u003e\n```\n\n### And the Javascript:\n\n(for more convenience, we are using jQuery)\n\n```javascript\nvar totalBytes = 10000000; // CHANGE ME WITH THE SIZE OF THE RESOURCE\nvar req = new XMLHttpRequest();\nvar progress = $('#download');\nprogress.attr(\"max\", totalBytes);\nreq.addEventListener(\"progress\", function (e) {\n¬† progress.attr(\"value\", e.loaded).text(Math.floor(100*e.loaded/totalBytes) \" %\");\n}, false); ¬†\nreq.addEventListener(\"load\", function (e) {\n¬† // THE RESOURCE IS LOADED\n¬† progress.replaceWith(\"Downloaded!\");\n});\nreq.open(\"GET\",\"resource.dat\",true);\nreq.send();\n```\n\nIt is quite easy to extend my code to support multiple files to download.\n\nIt is also easy to use this progress bar for anything else, but remember it represents a progress. If you want to represent some kind of stats, refer to the dedicated tag.\n","data":{"title":"Work in \u003cprogress /\u003e","description":"A progress tag will be displayed on recent browsers with a OS-native progress bar representing a loading.","thumbnail":"/images/2012/04/progress_mac.png","author":"Gaetan","layout":"post","permalink":"/2012/04/work-in-progress/","tags":["html","javascript"]}},{"id":"2012-04-09-blender-as-a-2d-game-map-editor-proof-of-concept","year":"2012","month":"04","day":"09","slug":"blender-as-a-2d-game-map-editor-proof-of-concept","content":"\n#\n\nA long time ago, video games were only two-dimensional. Of-course this was due to our poor hardware capabilities, but when computers became faster and faster 3D games appeared in mass.  \n**Did it kill 2D games? Nope.** They continue to exist because it offer a different gameplay and are easier to make. Maybe also a bit because we are nostalgic of old-school games!\n\nWe can distinguish two kinds of 2D games:\n\n\u003cimg src=\"/images/2012/04/bomberman93.jpg\" alt=\"\" class=\"thumbnail-left\" /\u003e\n[**Tile based games**][2] where the game world is simplified with a big grid ‚Äì each grid position has some properties.  \nA map editor is not always needed for tile based games, because the map can be straighforward to represent and maintain like in a *Bomberman* or in a *Pacman*. A simple editor is generally used to make graphism with sprites.\n\n[2]: http://www.tonypa.pri.ee/tbw/tut00.html\n[4]: http://higherorderfun.com/blog/2012/05/20/the-guide-to-implementing-2d-platformers/\n[5]: http://www.masswerk.at/JavaPac/JS-PacMan2.html\n[6]: http://impactjs.com/documentation/weltmeister\n[7]: http://gre.github.io/blazing-race\n[12]: http://gre.github.io/blazing-race/maps/converter/\n\n\u003cbr style=\"clear:both\" /\u003e\n\n\u003cimg src=\"/images/2012/04/woarpc001.jpg\" alt=\"\" class=\"thumbnail-left\" /\u003e\n**Non-tile based games**, which can be called ‚Äúpolygon based games‚Äù are more complex.  \nIn such game, like a *Worms* or a *Sonic*, it‚Äôs totally crazy to write the map by hand (objects positions, polygons coordinates, ‚Ä¶). The alternative, is not to use predefined maps, but on-the-fly generated maps which doesn‚Äôt fit every games.\n\n\u003cbr style=\"clear:both\" /\u003e\n  \n[Here are more detailed work on these different game designs][4].\n\n**Making the game engine** is one thing, but **designing the game levels** can be one big work too and **we need tools to make it easier**.\n\n\u003c!--more--\u003e\n\n## Tile based game maps\n\nIn tile based games, maps are usually quite simple to represent.\n\nFor instance, here is how we can code the maze of [Pacman][5]:\n\n```javascript\n[\n  \"ahhhhhgxbhhdxehhhhhc\",\n  \"vp....o......o....pv\",\n  \"v.lhm...lhhm...lhm.v\",\n  \"v.....n......n.....v\",\n  \"v.n.n.v.ahhc.v.n.n.v\",\n  \"d.v.o.v.vxxq.v.o.v.b\",\n  \"x.v...v.vxxt.v...v.x\",\n  \"c.bhm.o.bhhr.o.lhd.a\",\n  \"v........x.........v\",\n  \"em.lc.am.lm.lc.am.lg\",\n  \"v...v.v......v.v...v\",\n  \"v.k.o.o.lhhm.o.o.k.v\",\n  \"vp................pv\",\n  \"bhhhhhcxahhcxahhhhhd\",\n];\n```\n\nwhere every character is a tile and has a given meaning.\n\nFor more complex games, we can also represent the map with a set of objects, and each object has position and size properties (x, y, width, height) and other properties for the game logic.\n\nFor instance, see the _ImpactJS_ tile based games editor:\n\n[![](/images/2012/04/weltmeister-tutorial-entities.png)][6]\n\n## But what about polygons based game?\n\nWell, some have tried to make dedicated 2D game map editor like shown in this video:\n\n\u003ciframe width=\"640\" height=\"360\" src=\"http://www.youtube.com/embed/kvvEmm2Vyoc?feature=player_embedded\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\nbut it sounds a bit unfinished and specific.\n\n### Do it yourself, but don‚Äôt reinvent the wheel.\n\n**But finally, isn‚Äôt it what a 3D editor is doing?**\n\nIsn‚Äôt it the most generic tool we can find?\n\nThey have done a lot of awesome work in term of user interface, polygon modeling, textures (procedural / bitmap), ‚Ä¶let‚Äôs profit of all this work to generate awesome texture map while exporting polygons.\n\nRelying on such tools, you don‚Äôt have to learn a brand new map editor, you can relax on what you know if you have the chance to know Blender or Maya or anything.\n\n### The Z magic\n\nLet‚Äôs ignore the Z dimension, or rather, let‚Äôs **use the Z-dimension as a way to represent the semantics of the game map!**\n\nThis is the map I made for [Blazing Race][7], a HTML5 against-the-clock platform game where you control a fireball:\n\n![](/images/2012/04/zs.png)\n\nFor my game needs, I used **different Z layers to represent different kind of materials and game objects**:\n\n- z=1 : candles‚Äô position ‚Äì the objective of the game is to light them all\n- z=0 : the game grounds ‚Äì where collision occurs\n- z=-1 : the water areas ‚Äì where your flame dies\n- z=-2 : special areas where you miss oxgyen ‚Äì your flame dies in a few seconds\n\nBut I also used **objects ids** as an another way to distinguish objects:  \na ‚Äústart‚Äù object to define the game start position and two ‚Äútopleft‚Äù and ‚Äúbottomright‚Äù objects to define the game bound.\n\n### Maintain your map source in one file\n\nAnother powerful feature of this, is you can maintain your map polygons AND your map textures in a single way. Use your 3D editor as a polygon editor and use its render engine to generate textures:\n\n![](/images/2012/04/map1.png)\n\nTake benefits from what your 3D editor can do.\n\n### Export polygons to the Javascript game\n\n![](/images/2012/04/path4850.png)\n\nI‚Äôve made a transformer which take a COLLADA file in input (the most commonly supported standard format to describe a 3D scene, you can export it from any 3D editor like Blender, Maya, 3DS‚Ä¶) which extract and transform relevant informations from it and give you a json map for your game in output.\n\n_It was quite simple to implement, thanks to the Three.js COLLADA importer!_\n\nHere is the current (unfinished) interface for this:\n\n[![](/images/2012/04/demo_screenshot.png)][12]\n\nAs a proof of usability of the output JSON map, the preview was only made in a few lines of Javascript code.\n\nExtract:\n\n```javascript\nfunction draw(map) {\n  var container = $(\"#viewport\").empty();\n  $(\"#legend\").empty();\n  a = 0;\n  var w = 500;\n  var h = Math.floor((w * map.height) / map.width);\n  var CROSS_SIZE = 3;\n  var canvas = $('\u003ccanvas width=\"' + w + '\" height=\"' + h + '\"\u003e\u003c/canvas\u003e');\n  var ctx = canvas[0].getContext(\"2d\");\n  for (var name in map) {\n    var objs = map[name];\n    if (objs[0] \u0026\u0026 objs[0].faces) {\n      var color = randomColor(70, 0.8);\n      ctx.fillStyle = color;\n      for (var i = 0; i \u003c objs.length; ++i) {\n        var obj = objs[i];\n        for (var f = 0; f \u003c obj.faces.length; ++f) {\n          var face = obj.faces[f];\n          ctx.beginPath();\n          for (var v = 0; v \u003c face.length; ++v) {\n            var vertice = obj.vertices[face[v]];\n            var x = (ctx.canvas.width * vertice.x) / map.width;\n            var y = ctx.canvas.height * (1 - vertice.y / map.height);\n            if (v == 0) ctx.moveTo(x, y);\n            else ctx.lineTo(x, y);\n          }\n          ctx.fill();\n        }\n      }\n      addLegend(color, name, true);\n    }\n  }\n  for (var name in map) {\n    var objs = map[name];\n    if (objs[0] \u0026\u0026 objs[0].x) {\n      var color = randomColor(50);\n      ctx.strokeStyle = color;\n      ctx.lineWidth = 2;\n      for (var i = 0; i \u003c objs.length; ++i) {\n        var p = objs[i];\n        var x = (ctx.canvas.width * p.x) / map.width;\n        var y = ctx.canvas.height * (1 - p.y / map.height);\n        ctx.beginPath();\n        ctx.moveTo(x - CROSS_SIZE, y);\n        ctx.lineTo(x + CROSS_SIZE, y);\n        ctx.moveTo(x, y - CROSS_SIZE);\n        ctx.lineTo(x, y + CROSS_SIZE);\n        ctx.stroke();\n      }\n      addLegend(color, name, false);\n    }\n  }\n  container.append(canvas);\n}\n```\n\n## What is next?\n\nBlazing Race, is not finished yet, I need to improve a lot of things.\n\nI‚Äôll try to release a standalone version of this converter soon with tutorials and examples.\n","data":{"title":"Blender as a 2D game level editor ‚Äì Proof Of Concept","description":"Here is how you can design and export your 2D game map with Blender (both the logic and the graphics).","thumbnail":"/images/2012/04/map1.png","author":"Gaetan","layout":"post","permalink":"/2012/04/blender-as-a-2d-game-map-editor-proof-of-concept/","tags":["gamedev","blender","javascript"]}},{"id":"2012-03-17-play-painter-how-ive-improved-the-30-minutes-prototyped-version","year":"2012","month":"03","day":"17","slug":"play-painter-how-ive-improved-the-30-minutes-prototyped-version","content":"\nOne week ago, I‚Äôve released a [technical web experiment][1] featuring **a collaborative real-time Paint-like application I‚Äôve called Play Painter**. It has been made with [Play Framework 2][2] and rely on WebSocket and HTML5 Canvas Javascript APIs.\n\n[1]: /2012/03/30-minutes-to-make-a-multi-user-real-time-paint-with-play-2-framework-canvas-and-websocket/\n[2]: http://playframework.org/\n[4]: https://github.com/playframework/Play20/wiki/Iteratees\n[5]: https://github.com/gre/playpainter/blob/master/scala/app/controllers/Application.scala\n[6]: http://github.com/gre/playpainter\n[7]: http://playpainter.greweb.fr/\n[8]: https://github.com/gre/playpainter/issues/1\n[9]: https://twitter.com/dbathily\n\nThanks to everyone having tested my Play Painter experiment, you helped me figure out bugs and bottlenecks and to benchmark the application running on my tiny server.  \nThe first version of Play Painter has been improved with some optimizations.\n\nExplanation‚Ä¶\n\n\u003cblockquote class=\"twitter-tweet\" lang=\"fr\"\u003e\u003cp\u003eThanks guys for testing playpainter! but you are breaking my server :D \u003ca href=\"http://t.co/F62qwk1i\" title=\"http://twitter.com/greweb/status/179194592481116160/photo/1\"\u003etwitter.com/greweb/status/‚Ä¶\u003c/a\u003e\u003c/p\u003e\u0026mdash; Ga√´tan Renaudeau (@greweb) \u003ca href=\"https://twitter.com/greweb/status/179194592481116160\"\u003e12 mars 2012\u003c/a\u003e\u003c/blockquote\u003e\n\n\u003c!--more--\u003e\n\n## In brief\n\n- **80 twitts**, **3500 unique visitors** in a few days.\n- a peak of about **80 simultaneous painters**.\n- about **200 WebSocket messages per second** when 3-4 users are drawing =\u003e bottleneck found.\n- when it occurs, 100% CPU and about **1500 system interrupts per second** on my poor Atom 1.2 Ghz server.\n\n## Some reasons\n\nThe initial version of Play Painter was a basic fast-prototyped version:\n\nFirst, It was **spreading every mouse events** (down, up, move) to all clients **as fast as it comes**. It means that, depending on the computer and browser performance, a huge number of events could have been triggered and spread to all connected users.\n\nWe solved this by **Chunking draw events**.\n\nSecond, **a lot of informations was repeated in WebSocket messages.** No datas were stored on the server-side so to be sure a new user see the right draws, player name, brush color and size was sent in every message. Multiply this by the number of mouse events and you get a lot of useless information!\n\nWe are now **Storing painters information**.\n\n## Chunking draw events\n\nWhen an user starts drawing, mouse events give the brush positions (x, y). But instead of sending a websocket message for each of these new positions, they are stored, and every **X** milliseconds, are sent in a websocket message. Such message contains all points of the draw from the last sent draw message.\n\nThe **X** value has currently been fixed to **50 milliseconds** because it‚Äôs enough for the human eye: It means about 20 messages per second for one painter. In movies we usually have a 24 frame rate.\n\nThe same principle has been applied on the painter brush positions.\n\n### Example\n\n**Before**  \n13 WebSocket messages:\n\n```javascript\n{\"type\":\"lineTo\",\"x\":181,\"y\":259,\"pid\":19}\n{\"type\":\"lineTo\",\"x\":183,\"y\":259,\"pid\":19}\n{\"type\":\"lineTo\",\"x\":184,\"y\":257,\"pid\":19}\n{\"type\":\"lineTo\",\"x\":187,\"y\":257,\"pid\":19}\n{\"type\":\"lineTo\",\"x\":188,\"y\":257,\"pid\":19}\n{\"type\":\"lineTo\",\"x\":191,\"y\":256,\"pid\":19}\n{\"type\":\"lineTo\",\"x\":192,\"y\":255,\"pid\":19}\n{\"type\":\"lineTo\",\"x\":192,\"y\":255,\"pid\":19}\n{\"type\":\"lineTo\",\"x\":192,\"y\":254,\"pid\":19}\n{\"type\":\"lineTo\",\"x\":193,\"y\":254,\"pid\":19}\n{\"type\":\"lineTo\",\"x\":195,\"y\":254,\"pid\":19}\n{\"type\":\"lineTo\",\"x\":196,\"y\":253,\"pid\":19}\n{\"type\":\"lineTo\",\"x\":196,\"y\":253,\"pid\":19}\n```\n\n**After**  \n2 WebSocket messages: (50 ms apart)\n\n```javascript\n{\"type\":\"trace\",\"points\":[{\"x\":181,\"y\":259},{\"x\":183,\"y\":259},{\"x\":184,\"y\":257},{\"x\":187,\"y\":257},{\"x\":188,\"y\":257},{\"x\":191,\"y\":256},{\"x\":192,\"y\":255}],\"pid\":19}\n{\"type\":\"trace\",\"points\":[{\"x\":192,\"y\":255},{\"x\":192,\"y\":254},{\"x\":193,\"y\":254},{\"x\":195,\"y\":254},{\"x\":195,\"y\":253},{\"x\":196,\"y\":253},{\"x\":196,\"y\":253}],\"pid\":19}\n```\n\n### To a variable frame rate?\n\nI am also thinking about a variable rate depending of the number of active painters. In fact, the more we have painters, the more we will have messages, and the more the server will have system interrupts, we could then decrease the frame rate per second to reduce this load.\n\nThe problem of this extreme approach is the degradation of the feeling of real-time.\n\nFor now, I‚Äôm keeping the constant frame rate version, we will see how far it goes.\n\n## Storing painters information\n\nAs I said, **a lot of informations was repeated in WebSocket messages.** In every draw events, painter name, brush size and brush color was sent from the client to the server, and the spread into all connected clients.\n\nThis was ok for prototyping but we have now optimize this by storing these painter generic informations in the server and sending them when a new WebSocket connection is opened.\n\n### Example\n\nThis is what a client can receive when a websocket is connected:\n\n```javascript\n{\"type\":\"youAre\",\"pid\":24}\n{\"name\":\"john\",\"color\":\"red\",\"size\":5,\"type\":\"painter\",\"pid\":21}\n{\"name\":\"gre\",\"color\":\"red\",\"size\":5,\"type\":\"painter\",\"pid\":24}\n{\"name\":\"peter\",\"color\":\"red\",\"size\":5,\"type\":\"painter\",\"pid\":4}\n{\"name\":\"paul\",\"color\":\"red\",\"size\":5,\"type\":\"painter\",\"pid\":6}\n{\"name\":\"jack\",\"color\":\"red\",\"size\":5,\"type\":\"painter\",\"pid\":2}\n```\n\nand then‚Ä¶\n\n```\n{\"type\":\"trace\",\"points\":[{\"x\":181,\"y\":259},{\"x\":183,\"y\":259},{\"x\":184,\"y\":257}],\"pid\":19}\n...\n```\n\nBy knowing all painter properties, when someone will draw something, he will not have to repeat which color and size its brush has.\n\n### Server side\n\nIt was quite interesting to implement the server part with [Play2‚Ä≤s Iteratees][4], a new way of handling I/O ‚Äì not so new in fact because it is directly related to Haskell Iteratee concepts.\n\nTo implement a WebSocket connection, you will provide an **Iteratee** for consuming the **input** and an **Enumerator** for producing the **output**.\n\nEnumerator are chainable, this is how I firstly send the painter id and painters informations:\n\n```scala\n// out: handle messages to send to the painter\nval out =\n¬† // Inform the painter who he is (which pid, he can them identify himself)\n¬† Enumerator(JsObject(Seq(\"type\" -\u003e JsString(\"youAre\"), \"pid\" -\u003e JsNumber(pid))).as[JsValue]) \u003e\u003e\u003e\n¬† // Inform the list of other painters\n¬† Enumerator(painters.map { case (id, painter) =\u003e\n¬† ¬† (painter.toJson JsObject(Seq(\"type\" -\u003e JsString(\"painter\"), \"pid\" -\u003e JsNumber(id)))).as[JsValue]\n¬† } toList : _*) \u003e\u003e\u003e\n¬† // Stream the hub\n¬† hub.getPatchCord()\n```\n\nThe **\u003e\u003e\u003e** operator is a shortcut to the **andThen** method which is the way to chain enumerators.\n\nFor more details, [see the scala code of the controller][5].\n\n## Other features\n\nThe application has been improved in many other ways.\n\n- **A ‚Äúbuffering‚Äù Canvas** in the foreground has been add **for the user draws**. It brings client-side reactivity and helps to avoid unpleasant lag feeling when drawing. When the user draw events are coming from the server and no other user events has been sent since, it‚Äôs synchronized and we can clean this buffer.\n- **Painter positions are show with their names**.\n- It should now work properly on **smartphones and tablets**. Try on iPad and iPhone, and maybe on recent version of Android (WebSocket support required).\n- **Keyboard shortcut**: using arrows to change brush size and color.\n- The **[source code][6] has been polished and commented** especially the server side part (it‚Äôs probably the hardest part if you don‚Äôt know Play framework).\n- Error message displayed when a technology is not supported and when the WebSocket connection goes down (with a reconnecting try loop).\n\n## The demo is still online!\n\n**[playpainter.greweb.fr][7]**\n\n## Future\n\nWith these two optimizations, I‚Äôve reduce the global **number of socket messages** and also the **size of each message**.\n\nThe first benchmark sounds good, 3 painters was simultaneously crazily painting while the server application was only using less than 10% of CPU.\n\nNow, the most challenging part would be to scale the application to a huge number of connections, but having maybe solved this bottleneck, it‚Äôs maybe now more a matter of system architecture than the application itself.\n\nThis experiment gave me a lot of interest in **WebSocket** and also in **the powerful way WebSockets are handled in Play framework**.\n\nIf anyone want to start a Java version of the application, please go on! [(this was requested on Github)][8]\n\nThanks to [@dbathily][9], we know have both Scala and Java version!\n\n### Next experiment\n\nI am thinking about making a multiplayer game on the web.  \nIt would be something like a shooter survival game (like Counter Strike Zombie Mod) a multi-plateform 2D side view game (like Mario) !\n\nYou will know more about this soon!\n","data":{"title":"Play Painter ‚Äì how i've improved the 30 minutes prototyped version","description":"One week ago, I‚Äôve released a web experiment featuring a collaborative Paint-like application made with Play Framework 2 and relying on WebSocket and HTML5 Canvas. Here is how I've improved it.","thumbnail":"/images/2012/03/twitt_playpainter.png","author":"Gaetan","layout":"post","permalink":"/2012/03/play-painter-how-ive-improved-the-30-minutes-prototyped-version/","tags":["canvas","playframework","websocket","javascript"]}},{"id":"2012-03-12-30-minutes-to-make-a-multi-user-real-time-paint-with-play-2-framework-canvas-and-websocket","year":"2012","month":"03","day":"12","slug":"30-minutes-to-make-a-multi-user-real-time-paint-with-play-2-framework-canvas-and-websocket","content":"\n [1]: http://playpainter.greweb.fr/\n [2]: http://playframework.com\n [3]: http://www.whatwg.org/specs/web-apps/current-work/multipage/the-canvas-element.html\n [4]: http://dev.w3.org/html5/websockets/\n [5]: /2012/03/play-painter-how-ive-improved-the-30-minutes-prototyped-version/\n [6]: https://github.com/gre/playpainter\n\n[  \n\u003cimg src=\"/images/2012/03/playpainter_teaser2.png\" alt=\"\" class=\"thumbnail-left\" /\u003e\n][1]\n\n\u003e I will show you how to **implement a multi user paint** using latest web technologies like [Play framework][2] (version 2), [HTML5 Canvas][3] and [WebSocket][4].\n\n\nLet‚Äôs see how to implement this from scratch in about 30 minutes‚Ä¶\n\n\u003ciframe width=\"640\" height=\"360\" src=\"http://www.youtube.com/embed/NHEbm-WEbRw?feature=player_embedded\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\u003c!--more--\u003e\n\n[and read how I‚Äôve improved this initial version‚Ä¶][5]\n\n[or try the demo below‚Ä¶][1]\n\n\u003ciframe frameborder=\"0\" src=\"http://playpainter.greweb.fr/\" width=\"550\" height=\"501\" style=\"overflow: hidden; max-width: 550px\"\u003e\u003c/iframe\u003e\n\n[Fork me on Github.][6]\n","data":{"title":"30 minutes to make a multi user real time paint with Play 2 framework, Canvas and WebSocket.","description":"[VIDEO] I will show you how to implement a multi user paint using latest web technologies like Play framework (version 2), HTML5 Canvas and WebSocket.","thumbnail":"/images/2012/03/playpainter_teaser2.png","author":"Gaetan","layout":"post","permalink":"/2012/03/30-minutes-to-make-a-multi-user-real-time-paint-with-play-2-framework-canvas-and-websocket/","tags":["canvas","javascript","playframework","websocket"]}},{"id":"2012-03-06-chart-libraries-headaches-finding-the-best-grid-step","year":"2012","month":"03","day":"06","slug":"chart-libraries-headaches-finding-the-best-grid-step","content":"\n#\n\n\u003cimg src=\"/images/2012/02/wrong-chart-scale.png\" class=\"thumbnail-left\" /\u003e\n\nIf you have ever made a chart library in your life, you‚Äôve probably asked yourself how to find the best scale for the grid in order to have **nice values to display in the axis**.\n\nMost of the time, **data ranges are unknown**, hence we need to **adapt the grid step** to provide the best display.\n\n## Check this out\n\n\u003ciframe src=\"/demo/grid-utils/\" frameborder=\"0\" width=\"525\" height=\"140\"\u003e\u003c/iframe\u003e\n\nLet‚Äôs explain the algorithm‚Ä¶\n\n\u003c!--more--\u003e\n\n## About scientific notation\n\nAny number can be formatted in scientific notation. It is written in the form of **A x 10N** and is noted **AeN**.\n\nFor instance, 2300 becomes **2.3e3** (because 2300 = 2.3 x 103), 12 becomes **1.2e1**, and 0.23 becomes **2.3e-1**.\n\nScientific notation is exactly made for **displaying huge or tiny values in a few characters**.  \nWe can use the same principle for finding good values for the step scale, we can just **keep the pow of 10** part (**N**) and **round the value part** (**A**).\n\n## Magic numbers\n\nBut **rounding is not enough**, I have found that good pattern numbers of step range is those divisible by 2, 5 and 10.\n\nIn math term, we need to find a step range _sr_, where\n\n```\n‚àÄ n ‚àà |N, ‚àÄ a ‚àà {1, 2, 5}, ‚àÉ sr, sr = a x 10^n\n```\n\nThis is basically because 2 x 5 = 10 : using a step of 5 we have a 10 modularity every 2 step, and, using a step of 2 we have a 10 modularity every 5 step.\n\n** 2 step:** 0 2 4 6 8 **10** 12 14 16 18 **20** ‚Ä¶  \n** 5 step:** 0 5 **10** 15 **20** 25 **30** 35 **40** 45 ‚Ä¶  \n**10 step:** **0 10 20 30 40 50 60 70 80 90** ‚Ä¶\n\nFor any dataset, we need to fallback on the closest step range in all of possible step ranges: ‚Ä¶ 0.002, 0.02, 0.2, 2, 20, 200, ‚Ä¶, ‚Ä¶ 0.005, 0.05, 0.5, 5, 50, 500, ‚Ä¶, and ‚Ä¶ 0.001, 0.01, 0.1, 1, 10, 100, ‚Ä¶,\n\n### Calculate the pow of 10\n\nTo get the **N** value of the **A x 10N** form, we can use the log of 10:\n\n```javascript\nN = Math.log(number) / Math.log(10);\n```\n\n### Calculate the value modulo 10\n\nTo get the **A** value of the **A x 10N** form, we can just divide the number by **10N**:\n\n```javascript\nA = number / Math.pow(10, N);\n```\n\n### ‚ÄòRounding‚Äô the number\n\nWe know just need to change the value of **A** and make it more ‚Äúreadable‚Äù.  \nWe can map the value as follow:\n\n```\nif A ‚àà [0, 1.5[ then A becomes 1\nif A ‚àà [1.5, 3.5[ then A becomes 2\nif A ‚àà [3.5, 7.5[ then A becomes 5\nif A ‚àà [7.5, 10[ then A becomes 10\n```\n\nNote that these rules may probably be improved, I would love if someone could improve this (because I use a arithmetic mean approach and it should probably be arithmetic).\n\n## Implementation\n\n### Scala\n\n\u003cscript src=\"https://gist.github.com/1987311.js?file=GridUtils.scala\"\u003e\u003c/script\u003e\n\n### Javascript\n\n\u003cscript src=\"https://gist.github.com/1987311.js?file=GridUtils.js\"\u003e\u003c/script\u003e\n\n**Usage example:**\n\n```javascript\nGridUtils.findNiceRoundStep(xMax, 10);\n```\n\nwhere _xMax_ is the scale of the axis, and _10_ is the desired number of graduation split.\n\n## Conclusion\n\nFinding the best grid step is finally a simple thing to implement but is an essential feature every chart libraries should have.\n","data":{"title":"Chart libraries headaches ‚Äì finding the best grid step","description":"If you have ever made a chart library in your life, you‚Äôve probably asked yourself how to find the best scale for the grid in order to have nice values to display in the axis.","thumbnail":"/images/2012/02/wrong-chart-scale.png","author":"Gaetan","layout":"post","permalink":"/2012/03/chart-libraries-headaches-finding-the-best-grid-step/","tags":["javascript","math"]}},{"id":"2012-02-29-bezier-curve-based-easing-functions-from-concept-to-implementation","year":"2012","month":"02","day":"29","slug":"bezier-curve-based-easing-functions-from-concept-to-implementation","content":"\n[1]: /images/2012/02/Capture-d‚Äô√©cran-2012-02-29-√†-11.26.01.png \"Bezier example\"\n[2]: /images/2012/02/TimingFunction.png\n[3]: http://13thparallel.com/archive/bezier-curves/\n[4]: http://en.wikipedia.org/wiki/Newton%27s_method\n[5]: http://en.wikipedia.org/wiki/Dichotomic_search\n[6]: http://sliderjs.org/\n[7]: http://en.wikipedia.org/wiki/Inventor's_paradox\n[8]: /2012/02/bezier-curve-based-easing-functions-from-concept-to-implementation/ \"Bezier Curve based easing functions ‚Äì from concept to implementation\"\n\n\u003e **EDIT 2014:** This article ends up in an updated library available on [NPM](http://npmjs.org/package/bezier-easing) (`bezier-easing`) and available on [Github](https://github.com/gre/bezier-easing). It has been used by Apple for the [mac-pro page](http://www.apple.com/mac-pro/) and by [Velocity.js](http://velocityjs.org/). You can also find its usage in the [glsl-transition examples](http://greweb.me/glsl-transition/example/).\n\n\u003cimg src=\"/images/2012/02/bezier_transition_editor.png\" class=\"thumbnail-left\" /\u003e\n\nMany animation libraries are today using **easing functions** ‚Äì functions of time returning a progression percentage value. This is required to perform such cool effects:\n\n\u003ciframe src=\"/demo/simple-easing-animation/\" height=\"50\" width=\"50%\"\u003e\u003c/iframe\u003e\n\nBut most of these libraries implement a huge collection of functions. We will see how we can generalize them with bezier curves.\n\n\u003c!--more--\u003e\n\nFor instance, we use to do this:\n\n```javascript\nEasingFunctions = {\n  linear: function (t) {\n    return t;\n  },\n  easeInQuad: function (t) {\n    return t * t;\n  },\n  easeOutQuad: function (t) {\n    return t * (2 - t);\n  },\n  easeInOutQuad: function (t) {\n    return t \u003c 0.5 ? 2 * t * t : -1 + (4 - 2 * t) * t;\n  },\n  easeInCubic: function (t) {\n    return t * t * t;\n  },\n  easeOutCubic: function (t) {\n    return --t * t * t + 1;\n  },\n  easeInOutCubic: function (t) {\n    return t \u003c 0.5 ? 4 * t * t * t : (t - 1) * (2 * t - 2) * (2 * t - 2) + 1;\n  },\n  easeInQuart: function (t) {\n    return t * t * t * t;\n  },\n  easeOutQuart: function (t) {\n    return 1 - --t * t * t * t;\n  },\n  easeInOutQuart: function (t) {\n    return t \u003c 0.5 ? 8 * t * t * t * t : 1 - 8 * --t * t * t * t;\n  },\n  easeInQuint: function (t) {\n    return t * t * t * t * t;\n  },\n  easeOutQuint: function (t) {\n    return 1 + --t * t * t * t * t;\n  },\n  easeInOutQuint: function (t) {\n    return t \u003c 0.5 ? 16 * t * t * t * t * t : 1 + 16 * --t * t * t * t * t;\n  },\n};\n```\n\nDefining such functions is lot of math fun but it is very **specific** and not really customizable. Hopefully, we can generalize these easing functions. With **Bezier curves**.\n\nIn fact, this work has already been done in CSS Transitions and CSS Animations specifications! You can use `transition-timing-function` CSS property and give a `cubic-bezier(x1, y1, x2, y2)` value (all **ease, linear, ease-in, ease-out, ease-in-out** values are just fallbacking on this cubic-bezier usage).\n\n![][2]\n\nIn a bezier curve based easing function, the X axis is the **time axis** whereas the Y axis represents the **percentage of progress** of the animation.  \nThe two points P1 and P2 are called **handles** and you can (exclusively) control their X and Y positions to generate every possible cubic timing function.\n\n### Live demo\n\nTry to interact with the handles:\n\n\u003ciframe src=\"/demo/bezier-easing/\" frameborder=\"0\" width=\"560\" height=\"400\"\u003e\u003c/iframe\u003e\n\n## Implementation\n\nOk, so, this bezier curve concept is great but how can I implement it?\n\nI‚Äôve read [here][3] how simple is it to **compute many points of a Bezier curve** and potentially draw them:\n\n```javascript\nfunction B1(t) {\n  return t * t * t;\n}\nfunction B2(t) {\n  return 3 * t * t * (1 - t);\n}\nfunction B3(t) {\n  return 3 * t * (1 - t) * (1 - t);\n}\nfunction B4(t) {\n  return (1 - t) * (1 - t) * (1 - t);\n}\nfunction getBezier(percent, C1, C2, C3, C4) {\n  var pos = new coord();\n  pos.x =\n    C1.x * B1(percent) +\n    C2.x * B2(percent) +\n    C3.x * B3(percent) +\n    C4.x * B4(percent);\n  pos.y =\n    C1.y * B1(percent) +\n    C2.y * B2(percent) +\n    C3.y * B3(percent) +\n    C4.y * B4(percent);\n  return pos;\n}\n```\n\nBut it‚Äôs not enough. We need to project a point to the Bezier curve, in other words, we need to get the Y of a given X in the bezier curve, and we can‚Äôt just get it with the `percent` parameter of the Bezier computation.  \n**We need an interpolation.**\n\n### Deep into Firefox implementation\n\nIn Mozilla Firefox, The bezier curve interpolation is implemented in nsSMILKeySpline.cpp : .\n\nWhat we can learn from it is:\n\n- A first optimization store **sample values of the bezier curve** in a small table used to roughly find a initial X guess.\n- Then, it use two different implementation strategies: One use the [Newton‚Äôs method][4] and the other is just a [dichotomic search][5] (binary subdivision).\n- A **criteria** based on the **slope** give the best strategy to take.\n\nThese sub-optimizations probably make the difference for the C++ version but are not really relevant for the JavaScript implementation. Moreover, I have only used the Newton‚Äôs method algorithm.  \nAnd this is the code:\n\n\u003cscript src=\"https://gist.github.com/1926947.js?file=KeySpline.js\"\u003e\u003c/script\u003e\n\nNow we can just alias some classic easing function ‚Äì like CSS does.\n\n\u003cscript src=\"https://gist.github.com/1926947.js?file=EasingFunctions.json\"\u003e\u003c/script\u003e\n\nI‚Äôm working on the next version of [Slider.JS][6] which relies on 3 different technologies for image transitions: **CSS Transitions**, **Canvas** and **GLSL shaders (from WebGL)**.\n\n---\n\nI have now found **a common way to describe easing functions for both CSS-based and Javascript-based animations**!\n\nThis example has shown that sometimes, finding a larger solution for a problem is more interesting than having specific solutions.  \n**This is called the [Inventor‚Äôs paradox][7].**\n","data":{"title":"Bezier Curve based easing functions ‚Äì from concept to implementation","description":"Many animation libraries are today using easing functions ‚Äì functions of time returning a progression percentage value. We will see how we can generalize them with bezier curves.","thumbnail":"/images/2012/02/bezier_transition_editor.png","author":"Gaetan","layout":"post","permalink":"/2012/02/bezier-curve-based-easing-functions-from-concept-to-implementation/","tags":["animation","bezier","css","javascript"]}},{"id":"2012-02-04-css-selector-based-templating-example-with-javascript","year":"2012","month":"02","day":"04","slug":"css-selector-based-templating-example-with-javascript","content":"\n [2]: http://sliderjs.org/\n [3]: http://www.ubelly.com/2011/11/scalablejs/\n\n\u003cimg src=\"/images/2012/02/218px-Mir_diagram-fr.svg_.png\" alt=\"\" class=\"thumbnail-left\" /\u003e\n\nIn this article, we will focus on the power of **CSS as a descriptive language**, current template system approach and their problems with **modularity** and **extensibility**, and try to mix both features from the **concept** to a **concrete implementation**.\n\n\u003c!--more--\u003e\n\n\u003cbr style=\"clear: both\" /\u003e\n\n## What is CSS ?\n\nCSS is an extremely powerful descriptive language.  \nIt helps to define **how to display a document** (e.g. a web page).\n\nA style sheet contains a set of **CSS rules**.  \nEach CSS rule has a **CSS selector** associated with a set of **declarations**.  \nYou can see a CSS selector as a selection filter applied on every HTML element. A few element can match a CSS selector if they fit the structure describes in this selector.  \nEach **declaration** is composed of a couple (**property** : **value**).  \nThe **CSS property** is a predefined property related to a display or layout behavior.  \nThe **value** will apply a custom value for the property on all elements matching the CSS selector.\n\nLet‚Äôs focus on some advantages of this descriptive language:\n\n### A CSS rule is independant from others.\n\nThe order of CSS rules *(selector declarations)* **really** does not matter.  \nThe priority between CSS rules is based on the selector itself and not on their arrangement.\n\n### You can ‚Äúmix‚Äù CSS rule \n2 CSS rules can have the same CSS selector. An element can be matched with multiple CSS rules. CSS rules are merged, it‚Äôs called the cascading.  \nThis is the most important feature of CSS.  \nIt implies a very modular and extensible language.\n\n#### Example\n\n```css\na {   \n¬† color: #33CC00;  \n¬† text-decoration: none;  \n}  \na:hover { text-decoration: underline; }  \n#articles {  \n¬† font-size: 12px;  \n}  \n#articles a {  \n¬† color: red; /* overriding the generic color of a */  \n}\n```\n\n## Some limitations of today‚Äôs template system\n\nMost of template system are based on inherence between templates.\n\n*   You have usually an ‚Äúinclusion‚Äù approach: a template will ‚Äúinclude‚Äù multiple external template. *(Many template into Many template)*\n*   And an ‚Äúextension‚Äù approach: You define in a main template an area where you can append a template. Others templates ‚Äúextend‚Äù your main template. *(One main template for Many template)*\n\nThese approaches aims to factorize template codes and that‚Äôs great.\n\nBut it doesn‚Äôt fit my needs:\n\n*   It brings **dependencies between templates**. \n*   If you add a new template, you have to modify existing templates.  \n    If your application tend to go modules based, this is going to be unmaintainable. \n\n\u003e **web application module (n)**  \n\u003e 1 : an independent unit of functionality that is part of the total structure of a web application \n\n### A solution for scalable applications and libraries\n\nI‚Äôve recently started to rewrite my [SliderJS][2] library and I needed to split it into very modular features and having loose coupling between each component.\n\nI followed this [Scalable JavaScript Application Architecture][3] article.\n\nSo, **how to bring loose coupling in templating?**.  \nEach module have its own template and know where to append. Bringing this logic in a main template would break the independency and if I need to add new modules soon, it will not working without modifiyng it.  \nHow to keep the scalability of the main template without modifying it?\n\nThe best solution I found is to combine **CSS selectors** concepts with **template system** approaches.\n\n## CSS concepts applied on templating\n\nI‚Äôve decided to inspire from CSS: **attaching a CSS selector with a template**.  \nIt benefits from some CSS advantages explained before.\n\n### Simple twitter widget example\n\n```html\n\u003cdiv class=\"twitter\"\u003e\n  \u003ch1\u003eTwitter\u003c/h1\u003e\n  \u003cul class=\"twitts\"\u003e\n    \u003cli class=\"twitt\"\u003eHello world!\u003c/li\u003e\n  \u003c/ul\u003e\n  \u003cfooter\u003e\u003ca href=\"http://twitter.com/greweb\"\u003eFollow me on twitter\u003c/a\u003e\u003c/footer\u003e\n\u003c/div\u003e\n```\n\n#### Classical approach\n\nThe way to template it with the classical approach would be:\n\n```html\n\u003cdiv class=\"twitter\"\u003e\n  \u003ch1\u003eTwitter\u003c/h1\u003e\n  \u003cul class=\"twitts\"\u003e\n    {for twitt in twitts}\n    \u003cli class=\"twitt\"\u003e{twitt}\u003c/li\u003e\n    {/for}\n  \u003c/ul\u003e\n  \u003cfooter\u003e\u003ca href=\"http://twitter.com/{me}\"\u003eFollow me on twitter\u003c/a\u003e\u003c/footer\u003e\n\u003c/div\u003e\n```\n\n#### CSS selector based approach\n\nBut we can also split the original ‚Äútemplate‚Äù in small fragments and mix all of them.  \nIt helps to define each templates independently.\n\n**We can identify:**\n\n- A **root template fragment**:\n\n```html\n\u003cdiv class=\"twitter\"\u003e\u003c/div\u003e\n```\n\n- A **header fragment** appended with `.twitter` selector and with an **high priority**:\n\n```html\n\u003ch1\u003eTwitter\u003c/h1\u003e\n```\n\n- A **twitts list fragment** appended with `.twitter` selector:\n\n```html\n\u003cul class=\"twitts\"\u003e\n  {for twitt in twitts}\n  \u003cli class=\"twitt\"\u003e{twitt}\u003c/li\u003e\n  {/for}\n\u003c/ul\u003e\n```\n\nwith **parameters** `twitts = [ \"Hello world!\" ]`\n\n- An empty **footer fragment** appended with `.twitter` selector and with a **low** priority:\n\n```html\n\u003cfooter\u003e\u003c/footer\u003e\n```\n\n- A ‚Äúfollow me‚Äù **link fragment** appended with `.twitter footer` selector:\n\n```html\n\u003ca href=\"http://twitter.com/{me}\"\u003eFollow me on twitter\u003c/a\u003e\n```\n\nwith **parameters** `me = \"greweb\"`\n\n### Advanced example\n\nThis is another example with a slider. \n\nLet‚Äôs conceptually imagine the following template language:\n\n```html\n@root { html: \u003cdiv class=\"slider\"\u003e\u003c/div\u003e }\n\n.slider {\n  html: \u003cdiv class=\"slides\"\u003e\u003c/div\u003e\n}\n\n.slider div.slides {\n  html: \u003ccanvas class=\"slides\"\u003e\u003c/canvas\u003e\n}\n\n.slider div.slides {\n  html:\n  \u003cdiv class=\"slide\"\u003e\n    \u003ca href=\"\u003c%= link %\u003e\"\u003e\n      \u003cimg src=\"\u003c%= img %\u003e\" /\u003e\n      \u003cspan class=\"caption\"\u003e\u003c%= title %\u003e\u003c/span\u003e\n    \u003c/a\u003e\n  \u003c/div\u003e\n}\n\n.slider {\n  priority: -10\n  html: \u003cdiv class=\"pager\"\u003e\u003c/div\u003e\n}\n\n.slider div.pager {\n  priority: 10\n  html: \u003ca class=\"prevSlide\" href=\"javascript:;\"\u003eprev\u003c/a\u003e\n}\n\n.slider div.pager {\n  priority: -10\n  html: \u003ca class=\"nextSlide\" href=\"javascript:;\"\u003enext\u003c/a\u003e\n}\n\n.slider div.pager {\n  html: \u003cspan class=\"pages\"\u003e\u003c/span\u003e\n}\n\n.slider div.pager {\n  html: \u003cspan class=\"pages\"\u003e\n    \u003c% for (var i=0; i\u003cslides.length; ++i) { %\u003e\n    \u003ca class=\"page\" href=\"javascript:;\"\u003e\u003c%= i+1 %\u003e\u003c/a\u003e\n    \u003c% } %\u003e\n  \u003c/span\u003e\n}\n```\n\ncombined with some parameters, it will result\n\n```html\n\u003cdiv class=\"slider\"\u003e\n  \u003cdiv class=\"slides\"\u003e\n    \u003cdiv class=\"slide\"\u003e\u003ca href=\"..\"\u003e\u003cimg src=\"..\"/\u003e\u003cspan class=\"caption\"\u003e...\u003c/span\u003e\u003c/a\u003e\u003c/div\u003e\n    \u003cdiv class=\"slide\"\u003e...\u003c/div\u003e\n    \u003cdiv class=\"slide\"\u003e...\u003c/div\u003e\n    \u003ccanvas class=\"slides\"\u003e\u003c/canvas\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"pager\"\u003e\n    \u003ca href=\"javascript:;\" class=\"prevSlide\"\u003eprev\u003c/a\u003e\n    \u003cdiv class=\"pages\"\u003e\n      \u003ca href=\"javascript:;\" class=\"page\"\u003e1\u003c/a\u003e\n      \u003ca href=\"javascript:;\" class=\"page\"\u003e2\u003c/a\u003e\n      \u003ca href=\"javascript:;\" class=\"page\"\u003e3\u003c/a\u003e\n    \u003c/div\u003e                                                                      \n    \u003ca href=\"javascript:;\" class=\"nextSlide\"\u003enext\u003c/a\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n```\n\nOf-course we could also do this programmatically with DOM. But see the benefit of such a descriptive way to define things?\n\nYou should keep in mind that **the order of rules definition does not matter**. In that‚Äôs sense, it is **a mixable, extensible, modular and loosely-coupled template system**.\n\n\n### More about this POC\n\nUnlike CSS, **two same rules aren‚Äôt merged but are appended**.\n\nThe ***priority*** governs the order of append. The higher the value is, the sooner it is appended to the containers selected by the CSS selector.\n\nAs you can see, there is a **micro-templating** inside each rule. For this example, it looks like the John Resig ‚Äòs Micro Templating.\n\nNote also that a rule must be aware of its **parameters** to work properly. But this only concerns the implementation: You have to find a way to give a dynamic reference of these parameters when you add a rule.\n\n### Concrete implementation\n\nThe code above was a conceptual proof of concept, but I implement a subset of these features in Javascript and made ‚ÄúSelectorTemplating.js‚Äù available here : \u003chttps://gist.github.com/1731611\u003e\n\nThis is how it can be used for (almost) the same example. You will see different style of usage:\n\n```javascript\nvar node = document.getElementById(\"slider\");\nvar t = new SelectorTemplating(node);\nvar tmpl; // defined somewhere, the John Resig 's Micro Templating.\n\n// root module\nfunction root () { return '\u003cdiv class=\"slider\"\u003e\u003c/div\u003e' }\nt.add(null, root);\n\n// slides module\nvar slidesTmpl = tmpl('\u003cdiv class=\"slides\"\u003e \u003c% if(obj.slides) { for(var i=0; i\u003cslides.length; ++i) { var s = slides[i]; %\u003e \u003cdiv class=\"slide\"\u003e \u003ca href=\"\u003c%= s.link %\u003e\"\u003e \u003cimg src=\"\u003c%= s.img %\u003e\" /\u003e \u003cspan class=\"caption\"\u003e\u003c%= s.title %\u003e\u003c/span\u003e \u003c/a\u003e \u003c/div\u003e \u003c% }} %\u003e \u003c/div\u003e')\nvar slides = [ ... ]; // mutable\nt.add(\".slider\", function () { return slidesTmpl(slides: slides) });\n\n// canvas module\nt.add(\".slider div.slides\", function () { return '\u003ccanvas class=\"slides\"\u003e\u003c/canvas\u003e' });\n\n// pager module\nvar pagesTmpl = tmpl('\u003cdiv class=\"pager\"\u003e \u003cspan class=\"pages\"\u003e \u003c% if(obj.slides) { for(var i=0; i\u003cslides.length; ++i) { %\u003e \u003ca href=\"javascript:;\" class=\"page\"\u003e\u003c%= i+1 %\u003e\u003c/a\u003e \u003c% }} %\u003e \u003c/span\u003e \u003c/div\u003e');\nvar slides = [...]; // synchronised with the slides module\nvar prevButton, nextButton; // DOM element init when templated\nvar pages = function () {\n  return pagesTmpl(slides: slides);\n}\nt.add(\".sliderjs\", pages, null, -10);\nt.add(\".sliderjs .options\", function () { return '\u003ca class=\"prevSlide\" href=\"javascript:;\"\u003eprev\u003c/a\u003e' }, function (n) { prevButton = n[0] }, 10 }); // prepend first in options\nt.add(\".sliderjs .options\", function () { return '\u003ca class=\"nextSlide\" href=\"javascript:;\"\u003enext\u003c/a\u003e' }, function (n) { nextButton = n[0] }, -10 }); // append at the end of options\n\n// when all modules are init :\nt.init();\n```\n\n```\nt.add (selector, templateFunction, callback, priority)\n * selector is the selector function. if null, append to root.\n * a template function is an identifier in the template.\n * the callback is called at the end of the templating with 2 arguments : the appended nodes and the global container.\n```\n\n#### Algorithm of the template process\n\n```\ncontainer := the container element\nrules := an array containing all rules.\nsort rules by priority.\n(1) take one rule from rules\n  - elements := []\n  - if the selector is @root, elements := [container]\n  - otherwise, elements := all elements which matches the selector\n  - if the elements is empty, back to (1) by taking the next rule.\n  - (2) if not, templatize the html and append it into all of these elements. remove the rule from rules. back to (1) by starting from the first rules. \n\nthe loop (1) must end when :\n  - there is no rules anymore\n  - you have covered all the rules array without finding a match (without passing by (2) for this loop). In that case, it means some rules are not used.\n```\n\nThere is a known limitation of the algorithm I intend to fix soon:\nOnce we found matching elements for a rule, we append the template in these elements once, and we remove the rule. It‚Äôs a simple way to avoid recursion. But this approach doesn‚Äôt work if a selector can potentially matches elements defined in different rules. **I know how to fix this but it‚Äôs not yet implemented.**\n\n## What's next?\n\nWe are working hard for the next version (v2) of [SliderJS](http://sliderjs.org) by trying to make a revolutionary IDE platform for SliderJS. It requires a modulification of every components of SliderJS, we try to keep things simple (no external library required, the core system is only 4k sized). You will have more information soon!\n\nThis templating system should benefits of this work.\n\nKeep in touch!\n","data":{"title":"CSS-selector-based templating system for scalable JavaScript applications","description":"In this article, we will focus on the power of CSS as a descriptive language, current template system approach and their problems with modularity and extensibility, and try to mix both features from the concept to a concrete implementation.","thumbnail":"/images/2012/02/218px-Mir_diagram-fr.svg_.png","author":"Gaetan","layout":"post","permalink":"/2012/02/css-selector-based-templating-example-with-javascript/","tags":["css","javascript","templating"]}},{"id":"2011-10-15-how-to-deploy-your-play-applications-on-archlinux-with-daemons","year":"2011","month":"10","day":"15","slug":"how-to-deploy-your-play-applications-on-archlinux-with-daemons","content":"\n [1]: http://playframework.org\n [2]: http://aur.archlinux.org/packages.php?ID=45541\n [3]: http://nginx.org\n [4]: https://wiki.archlinux.org/index.php/Play_framework\n\nThis video shows how to run different instances of [Play framework][1] server in the most Linux friendly way: using daemons. Example with ArchLinux, using yaourt, the [playframework AUR package][2] and [nginx][3].\n\n\u003ciframe src=\"http://player.vimeo.com/video/30603225?title=0\u0026amp;byline=0\u0026amp;portrait=0\" width=\"400\" height=\"300\" frameborder=\"0\" webkitAllowFullScreen mozallowfullscreen allowFullScreen\u003e\u003c/iframe\u003e\n\n# Some links\n\n*   [Play framework website][1]\n*   [Play framework Archlinux documentation][4]\n*   [Play framework AUR package][2]\n\n\u003c!--more--\u003e\n\n# Abstract summary\n\n## Introduction\n\nExisting Platform as a Service: [Playapps.net][5], [Heroku.com][6].\n\n [5]: http://playapps.net\n [6]: http://heroku.com\n\nOur needs are quite different: you sometimes need to have your own server on your own infrastructure and not depending on third party web services.\n\nLet‚Äôs see how to deploy some play applications from scratch with ArchLinux.\n\n## Requirement\n\n[Install ArchLinux on your server][7]\n\n [7]: http://archlinux.org\n\n[Install yaourt][8]\n\n [8]: http://archlinux.fr/yaourt-en\n\n## Installation\n\n```bash\nyaourt -S playframework\n```\n\n## Creating 2 play framework applications\n\n```bash\nmkdir sites \u0026\u0026 cd sites  \nplay new app1  \nplay new app2 # editing app/views/\n```\n\n## Configuring daemons\n\n```bash\ncd /etc/rc.d  \nln -s skeleton_playapp app1  \nln -s skeleton_playapp app2  \ncd /etc/conf.d  \ncp playapp_sample app1  \ncp playapp_sample app2  \nvim app1 # configure variables  \nvim app2 # configure variables\n```bash\n\nFor app1 :\n\n```bash\nPLAY_APP=/home/gre/sites/app1  \nPLAY_USER=gre  \nPLAY_ARGS=\"--%prod --http.port=9001\" \n```\n\nFor app2 :\n\n```bash\nPLAY_APP=/home/gre/sites/app2  \nPLAY_USER=gre  \nPLAY_ARGS=\"--%prod --http.port=9002\" \n```\n\n## Starting daemons\n\n```bash\nrc.d start app1  \nrc.d start app2\n```\n\nMake it permanent in /etc/rc.conf by adding them in the DAEMONS variable.\n\n```bash\n...\nDAEMONS=(... app1 app2) \n```\n\n## Nginx, as a front end proxy server\n\nInstall nginx using pacman.\n\nEdit `/etc/nginx/conf/nginx.conf`\n\n```nginx\n...  \n¬† ¬† server {  \n¬† ¬† ¬† ¬† listen 80;  \n¬† ¬† ¬† ¬† server_name app2.archdemo;  \n¬† ¬† ¬† ¬† location / {  \n¬† ¬† ¬† ¬† ¬† proxy_pass http://127.0.0.1:9002;  \n¬† ¬† ¬† ¬† ¬† proxy\\_set\\_header Host $host;  \n¬† ¬† ¬† ¬† ¬† proxy\\_set\\_header X-Forwarded-For $proxy\\_add\\_x\\_forwarded\\_for;  \n¬† ¬† ¬† ¬† ¬† proxy\\_set\\_header X-Forwarded-Host $host;  \n¬† ¬† ¬† ¬† ¬† proxy\\_set\\_header X-Forwarded-Port $server_port;  \n¬† ¬† ¬† ¬† ¬† proxy\\_set\\_header X-Forwarded-Proto https;  \n¬† ¬† ¬† ¬† }  \n¬† ¬† }  \n¬† ¬† server {  \n¬† ¬† ¬† ¬† listen 80;  \n¬† ¬† ¬† ¬† server_name app1.archdemo;  \n¬† ¬† ¬† ¬† location / {  \n¬† ¬† ¬† ¬† ¬† proxy_pass http://127.0.0.1:9001;  \n¬† ¬† ¬† ¬† ¬† proxy\\_set\\_header Host $host;  \n¬† ¬† ¬† ¬† ¬† proxy\\_set\\_header X-Forwarded-For $proxy\\_add\\_x\\_forwarded\\_for;  \n¬† ¬† ¬† ¬† ¬† proxy\\_set\\_header X-Forwarded-Host $host;  \n¬† ¬† ¬† ¬† ¬† proxy\\_set\\_header X-Forwarded-Port $server_port;  \n¬† ¬† ¬† ¬† ¬† proxy\\_set\\_header X-Forwarded-Proto https;  \n¬† ¬† ¬† ¬† }  \n¬† ¬† }  \n...\n```\n","data":{"title":"How to deploy your play applications on ArchLinux with daemons","description":"This video shows how to run different instances of Play framework server in the most Linux friendly way: using daemons. Example with ArchLinux, using yaourt, the playframework AUR package and nginx.","author":"Gaetan","layout":"post","permalink":"/2011/10/how-to-deploy-your-play-applications-on-archlinux-with-daemons/","tags":["sysadmin","linux","playframework"]}},{"id":"2011-07-30-improve-your-web-navigation-experience-flexible-nav-jquery-library","year":"2011","month":"07","day":"30","slug":"improve-your-web-navigation-experience-flexible-nav-jquery-library","content":"\n [1]: http://gre.github.io/flexible-nav/demo\n [3]: https://github.com/gre/flexible-nav\n [4]: javascript:(function(){window.flexibleNavBase='http://gre.github.io/flexible-nav/lib/';var%20a=document.getElementsByTagName('head')[0],b=document.createElement('script');b.type='text/javascript';b.src=flexibleNavBase+'bookmarklet.min.js';a.appendChild(b);})();%20void%200\n\n## [Demo][1]\n\n\u003e Flexible Nav is a small jQuery library which add a smart navigation bar on the right of the page. It improves a web page navigation and helps to visualize different sections of a document, an article,.. any web page.\n\u003e \n\u003e Nav links are distributed proportionally to the page sections. See how your scrollbar ‚Äúweds‚Äù these links.\n\nFlexible Nav is both a **library** and a **bookmarklet** which can be used easily in any website.\n\nThe bookmarklet demonstrate the **hackability of the web** by using the semantic of web headings (*h1, h2, h3, ..*).\n\n### Links\n\n* [Documentation and Demos][1]\n* [fork me on Github][3]\n* [FlexibleNav bookmarklet][4] (drag me in your bookmarks bar)\n\n","data":{"title":"Improve your web navigation experience ‚Äì Flexible Nav jQuery library","description":"Flexible Nav is a small jQuery library which add a smart navigation bar on the right of the page. It improves a web page navigation and helps to visualize different sections of a document, an article,.. any web page.","author":"Gaetan","layout":"post","permalink":"/2011/07/improve-your-web-navigation-experience-flexible-nav-jquery-library/","tags":["javascript","library","navigation"]}},{"id":"2011-07-12-same-game-gravity-technical-notes","year":"2011","month":"07","day":"12","slug":"same-game-gravity-technical-notes","content":"\n [1]: /2011/07/same-game-gravity-for-ipad-iphone-android-facebook-chrome-and-web/\n [2]: http://gre.github.io/same-game-gravity\n [4]: /2011/06/automating-web-app-development-for-multiple-platforms/\n [5]: https://github.com/gre/same-game-gravity\n [6]: https://github.com/gre/same-game-gravity/blob/master/game.html\n [7]: https://github.com/gre/same-game-gravity/blob/master/game.css\n [8]: https://github.com/gre/same-game-gravity/blob/master/game.js\n [9]: https://github.com/gre/same-game-gravity/blob/master/game.js#L324\n [10]: https://github.com/gre/same-game-gravity/blob/master/game.js#L687\n [11]: https://github.com/gre/same-game-gravity/blob/master/game.js#L850\n [12]: https://github.com/gre/same-game-gravity/blob/master/game.desktop.js\n [13]: https://github.com/gre/same-game-gravity/blob/master/game.desktop.js#L137\n [14]: http://docs.phonegap.com/phonegap_accelerometer_accelerometer.md.html\n [15]: http://dev.w3.org/geo/api/spec-source-orientation.html\n [16]: http://twitter.com/42loops\n [17]: https://github.com/peutetre/test-mobile-safari/blob/master/devicemotionevent.html\n [18]: /images/2011/07/c-rotation.png\n [19]: https://github.com/gre/same-game-gravity/blob/master/game.desktop.js#L31\n [20]: https://github.com/gre/same-game-gravity/blob/master/index.css#L35\n [21]: http://playframework.org/\n [22]: /images/2011/07/same_game_gravity_schema.jpg\n [23]: http://same.greweb.fr/public/javascripts/same.scores.js\n\nsee also [Same Game Gravity presentation][1].\n\n2 years ago, I started to developed the [Same Game][2] as an HTML Canvas experiment. I‚Äôve enjoyed developing this game, mostly because playing with HTML5 Canvas is so easy. Recently I‚Äôve seen a nice increase in the user base (now around 250 visitors a day) ‚Äì despite it being perhaps the simplest games I‚Äôve ever developed. Simplicity is good, but my increase in users is thanks to the power of HTML5: The Same Game is available for 6 different platforms. And I can pump out new builds for them all in around 15 minutes. Here‚Äôs how‚Ä¶\n\n\n**It‚Äôs often the simplest games which work. Too much complexity is not good.**\n\n[  \n![](/images/2011/07/gravity_exemple.png)\n][2]\n\n\u003c!--more--\u003e\n\nIn 2010, I learned how to make mobile web applications. It was also the year of the iPad. Out of interest I tried my same game canvas experiment on the iPad, and was surprised to find that it worked pretty well out of the box! Seeing it run on multiple devices was exciting ‚Äì and the touch screens offered a new dimension for creating highly intuitive interactions. I mean, today, **even my mum can play Same Game Gravity without any help!** (That‚Äôs unfortunately not the case for her desktop)\n\nThat‚Äôs why I wanted to make Same Game for mobile. I started out developing and testing it as an Android application ‚Äì because I have an Android phone. I created my own micro framework with some MVC concept (views, controllers, a router, etc.). The goal was to create **a simple and light web app that look like a native application**. For views? Portions of HTML. For transitions between views? CSS transitions. Supporting the ‚Äúback‚Äù button of Android devices as a native application? I played with the hash (onhashchange event). \n\nIn short, the web is wide and worldly enough to do pretty much everything you want with‚Ä¶\n\nSo I implemented the Same Game on Android. But (naturally) the game already existed on Android! I had to find something new! I was itching to fully exploit the possibilities of a new technology. Mobile has great potential ‚Äì so it would be bad not to make use of new APIs. I discovered **the Accelerometer**. My idea was gravity: change the balls position by rotating the device.\n\nBut, many of my friends don‚Äôt have Android phones!\n\nThe Same Game Gravity is now available for iPad, iPhone, Android, Facebook, Chrome Store and desktop browsers. That‚Äôs a lot of platforms, with a lot of APIs to learn ‚Äì and potentially a LOT of work in maintenance. But thankfully I didn‚Äôt have to go off learning Objective-C and Java Android, or keep track of arm-fulls of repositories! All the platforms are supported from **a single codebase**: thanks to the power and awesomeness of JavaScript, HTML, and CSS ‚Äì combined with a nifty tool I developed [WebAppBuilder][4] to easily build each instance.\n\n\n**I added a cool scoring system that spreads via multiple social networks simultaneously and easily ‚Äì and now I have a truly cross-platform game!**\n\n### The code\n\nDesktop version source code is available on [Github][5].\n\n\n#### The HTML\n\n(see [game.html][6])\n\n\nThe HTML code is pretty simple.  \nBasically, there is a **container which contains different**. Each section is a view of the game.\n\nFor instance here is the game view :  \n\n```html\n\u003csection id=\"game\"\u003e\n  \u003cdiv class=\"turnleft\"\u003e\u003c/div\u003e\n  \u003cdiv class=\"turnright\"\u003e\u003c/div\u003e\n  \u003cdiv class=\"gameStatus\"\u003e\n    \u003ca class=\"i18n-back back\" href=\"#!/\"\u003eBack\u003c/a\u003e\n    \u003cspan class=\"timeline\"\u003e\n      \u003cspan class=\"remainingSeconds\"\u003e\u003cspan class=\"remainingSecondsTime\"\u003e\u003c/span\u003e s\u003c/span\u003e\n    \u003c/span\u003e\n  \u003c/div\u003e\n  \u003cdiv id=\"gameContainer\"\u003e\n    \u003ccanvas class=\"highlight\" width=\"400\" height=\"300\"\u003e\u003c/canvas\u003e\n    \u003ccanvas class=\"main\" width=\"400\" height=\"300\"\u003e\u003c/canvas\u003e\n  \u003c/div\u003e\n\u003c/section\u003e\n```\n\nIn the desktop version, **game.html** is wrapped into **index.html** in an iframe to keep the game independent of the context.\n\n#### The CSS\n\n(see [game.css][7])\n\n\nCSS 3 is very rich.\n\nCSS Transitions and CSS Transforms has been used to do view change.  \n\n```css\n/* Pre conditions :\n * With Javascript: \n   - A class \"current\" is setted for the current section view. \n   - All section after \"current\" must take a \"after\" class. \n */\n#main.enabletransition \u003e section { /* #main must take \"enabletransition\" after the DOM load to avoid a first transition */\n  transition-duration: 1s, 0s;  /* transform takes 1s duration, opacity doesn't have transition */\n}\n#main \u003e section {\n  z-index: -1;\n  opacity: 0;\n  transition-delay: 0s, 1s; /* opacity go to 0 in 1s */\n  transition-property: transform, opacity;\n  transform: translateY(-100%); /* go above the page */\n}\n#main \u003e section.current {\n  z-index: 0;\n  opacity: 1;\n  transition-delay: 0s, 0s;\n  transition-transform: translateY(0%); /* go to the bottom */\n}\n#main \u003e section.after { /* same as \"#main \u003e section.current ~ section\" but without bugs */\n  opacity: 0;\n  transition-delay: 0s, 1s;\n  transform: translateY(100%); /* go below the page */\n}\n \n/* Note that this is a part of the css code \n * (you need to add -webkit-, -moz-, ... in some properties)\n */\n```\n\n#### The game core\n\n(see [game.js][8])\n\n\nCode is organized in different javascript ‚Äúclasses‚Äù.\n\nThe main components are :\n\n*   [game.Grid][9] contains all the algorithm of the game.\n*   [game.GameCanvasRenderer][10] is a game renderer (graphic part of the game) based on HTML Canvas element. It contains different functions called by **game.Game**.\n*   [game.Game][11] contains all the game logic, the game loop and bind DOM events (touch, click, ‚Ä¶).\n\n\n#### game.desktop.js: a game instance for the desktop\n\n(see [game.desktop.js][12])\n\n\nThis file contains all the specific code for the desktop version (it overrides existing classes). But it mainly contains the [game controller][13] handling different views and using all game classes.\n\n\n##### Some significant code\n\n```javascript\n// Colors.get(nb) : pick nb random colors \nvar Colors = function() {\n  var clrs = [ new Color('#D34040'), new Color('#82D340'), new Color('#40C2D3'), new Color('#8B40D3'), new Color('#D3C840') ];\n  return {\n      get: function(nb) {\n      return clrs\n      .sort(function(){ return Math.random() - 0.5; })\n      .slice(0, nb);\n    }\n  }\n}();\n \n/** Game instanciation **/\n \nvar gridSizeByDifficulty = [ // Size of grid for each difficulty\n  {w:8, h:8},\n  {w:12, h: 12},\n  {w: 16, h: 16}\n];\nvar colorNumberByDifficulty = [3, 4, 5]; // Nb of colors for each difficulty\n \nvar difficulty; // can be 0 (easy), 1 (normal) or 2 (hard)\ncurrentGame = new game.Game({\n  gridSize: gridSizeByDifficulty[difficulty],\n  colors: colors=Colors.get(colorNumberByDifficulty[difficulty]),\n  container: '#game', // container selector\n  rendererClass: 'GameCanvasRenderer', // The class to use for rendering the game\n  difficulty: difficulty,\n  drawHover: true,\n  globalTimer: new Timer().pause(),\n  keepSquare: true // Keep a square ratio\n});\n```\n\n### The gravity\n\nThe game gravity was maybe the hardest part of the game development. \n\n#### Using device Accelerometer for mobile/tablet version\n\nI needed to find ways to access to the device accelerometer. For Android I used [PhoneGap Accelerometer][14]. But on iPhone I wasn‚Äôt able to get PhoneGap‚Äôs accelerometer.getCurrentAcceleration to work properly, so I used DeviceMotion event supported by iOS 4.2 . (see [DeviceOrientation spec][15]).\n\n(A big thanks to [@42loops][16] for that: [devicemotionevent.html][17])\n\n\n![Device orientation schema][18]\n\n\n#### CSS Transforms and Transitions for the desktop version\n\nComputers don‚Äôt have an Accelerometer. *Except maybe some macbook but I‚Äôm not sure people would like to turn macbook in 360¬∞!* but the gravity concept is crucial to the game. I ended up implementing ‚Äúgravity‚Äù via the arrow keys.  \nThe game is entirely rotated with [CSS Transforms][19] and animated with [CSS Transitions][20].\n\n\n### The score system\n\nI‚Äôve written a web service with [Play! framework][21] to receive scores or retrieve them from Twitter, validate them and spread them with a json API and widgets.\n\n\n![tweet example][22]\n\n\n**This web service will be available soon for game developers.**\n\nThe power of this web service is the usage of **social networks**. It will retrieve peoples names, avatars, and their social links without needing to prompt the user.  \nFor game developers, social scores sharing is a nice way of **advertising your game**: someone shares his scores to his friends: so your game can spread virally.\n\nSee that little hash ‚Äú$4f005‚Ä≥? That‚Äôs a way to check if sent scores are valid.  \nIn fact the web service allows you to handle your own security, via your own ‚Äútwitter to scores‚Äù transformer. You can add a small Javascript function that is executed by the server when transforming a twit to scores ‚Äì to ensure there hasn‚Äôt been any cheating.\n\nThe web service also provides **generic widgets** to easily embed game scores in websites.  \nIf a player has played a few different games using this web scores service, we can provide a ‚Äútransversal‚Äù widget contains all scores of a player.\n\n#### The Same Game Gravity Widget\n\nSame Game Gravity use its own widget ([source code here][23]).\n\nThis widget is very customizable. Here‚Äôs an example of the code used to embed the widget anywhere (like in this blog post) :\n\n```html\n\u003cscript type=\"text/javascript\" src=\"http://same.greweb.fr/public/javascripts/same.scores.js\"\u003e\u003c/script\u003e\n\u003cscript type=\"text/javascript\"\u003e\n  new same.Scores({\n    width: '250px',\n    height: '400px',\n    items: 3,\n    period: 'all',\n    platform: ['web', 'mobile', 'tablet'],\n    type: ['hs_hard', 'hs_normal', 'hs_easy'],\n    title: 'Best highscores ever',\n    theme: {\n      bg: 'rgb(218, 236, 244)',\n      color: '#000',\n      scores_bg: 'rgba(255, 255, 255, 0.5)',\n      scores_color: 'rgba(0, 0, 0, 0.8)',\n      link: '#1F98C7'\n    }\n  }).init().fetch();\n\u003c/script\u003e\n```\n\n### Conclusion\n\nAnd here we are! 6 months to develop a game and release it on different platforms! I learn a lot about mobile development and I‚Äôm now more capable to develop other games.  \nI learned that you should avoid using Canvas if you can use DOM instead because performance are bad on some mobile device whereas CSS Transitions / Animations are hardware accelerated.\n\nFinally, I learned that game development is not only about programming! The marketing and the graphical parts are so important too.\n\nWant to checkout the code or contribute to the game i18n? Just fork the [game repository][5].\n\n\n\n#### Thanks\n\nBig thanks to all game testers. Friends and colleagues, thank you very much!  \nSpecial thanks to @mrspeaker for English help !\n\n","data":{"title":"Same Game Gravity: 6 platforms, 1 codebase","thumbnail":"/images/2011/07/gravity_exemple.png","description":"The Same Game is available for 6 different platforms. And I can pump out new builds for them all in around 15 minutes. Here‚Äôs how‚Ä¶","author":"Gaetan","layout":"post","permalink":"/2011/07/same-game-gravity-technical-notes/","tags":["mobile","gamedev","canvas"]}},{"id":"2011-07-12-same-game-gravity-for-ipad-iphone-android-facebook-chrome-and-web","year":"2011","month":"07","day":"12","slug":"same-game-gravity-for-ipad-iphone-android-facebook-chrome-and-web","content":"\n [1]: /2011/07/same-game-gravity-for-ipad-iphone-android-facebook-chrome-and-web/\n [4]: http://gre.github.io/same-game-gravity\n [5]: http://itunes.apple.com/us/app/same-game-gravity-for-ipad/id446790701\n [6]: http://itunes.apple.com/us/app/same-game-gravity/id445606743\n [7]: http://market.android.com/details?id=fr.gaetanrenaudeau.samegame.free\n [8]: http://gre.github.io/same-game-gravity\n [9]: http://apps.facebook.com/samegamegravity/\n [10]: https://chrome.google.com/webstore/detail/eibjpmiiheipmgfhffjpdmojoagccijb\n\nsee also [Same Game Gravity Technical Notes][1].\n \n# [Click to play][4]\n\n![](/images/2011/07/promo.png)\n\n\n[Same Game Gravity][4] is today available on **[iPad][5], [iPhone][6], [Android][7], [Web][8], [Facebook Apps][9] and [Chrome Web Store][10]**.\n\n![](/images/2011/07/same_platform.png)\n\n\u003e Un principe simple remis au go√ªt du jour. Tr√©s bon!! *(bobylito)*\n\n\u003e Great twist (excuse the pun) on the ‚Äúsame‚Äù game! *(mrspeaker)*\n\n\u003e A fresh spin on Same Game the use of gravity is pretty clever. *(erwan)*\n\nSame Game Gravity is a mind-bending ball removal puzzle game where you try to remove all of the balls from the board. Balls can only be removed when they are grouped with more balls of the same color. The balls are influenced by gravity. On mobile, you change gravity by rotating your mobile, on desktop you have arrows.\n\n\u003c!--more--\u003e\n\n## The game gravity\n\nRotate your device to apply gravity and change balls arrangement!\n\n![gravity example](/images/2011/07/gravity_exemple.png)\n\n\n\n### A pioneering scores system based on social networks\n\n  \n\n\nPlayers share their scores through **Twitter** or directly **from the game** (for the Facebook version). The **user name** and the **avatar** are directly taken from these social networks. \nYou can easily compare your scores and contact other players with social networks!\n\n","data":{"title":"Same Game Gravity for iPad, iPhone, Android, Facebook, Chrome, and Web!","description":"Same Game Gravity is today available on iPad, iPhone, Android, Web, Facebook Apps and Chrome Web Store.","thumbnail":"/images/2011/07/promo.png","author":"Gaetan","layout":"post","permalink":"/2011/07/same-game-gravity-for-ipad-iphone-android-facebook-chrome-and-web/","tags":["mobile","gamedev","canvas"]}},{"id":"2011-06-01-automating-web-app-development-for-multiple-platforms","year":"2011","month":"06","day":"01","slug":"automating-web-app-development-for-multiple-platforms","content":"\n [1]: /2011/06/automating-web-app-development-for-multiple-platforms/#webappbuilder\n [2]: http://gre.github.io/same-game-gravity\n [3]: https://github.com/gre/WebAppBuilder\n [4]: http://diveintohtml5.org/\n [6]: http://www.phonegap.com/\n [8]: http://mustache.github.com/\n [9]: http://sass-lang.com\n [10]: http://compass-style.org\n [12]: http://mrspeaker.net/\n [13]: https://github.com/jquery/jquery/tree/master/build\n\n\nIn this article, we will explain why we‚Äôd choose web technologies to make applications and introduce [**WebAppBuilder**][1], a tool to easily build different instances of an application. We‚Äôll examine the [Same Game Gravity][2] as an example.\n\n\nUsing web to develop mobile applications is very **productive** and web technologies are **rich**.\n\n[Fork WebAppBuilder on Github.][3]\n\n\u003c!--more--\u003e\n\n## Rich?\n\nNew web technologies have become rich with CSS3, HTML5 and new Javascript APIs are now being supported on most of smartphones. CSS3 animations, Web Service usage, local storage, Geolocation, drawing shapes (Canvas),.. are some example of new web features.\n\nI won‚Äôt expand more on this topic but invite you to [visit this link][4] for more details.\n\n## Productive?\n\nCompared to a native application, the web application **paradigm is reversed**.  \nThe Web provide a common way to make applications.  \nTo develop a native application, you must adapt yourself for each device, each new API, or each new language,‚Ä¶ but with Web, the device fits to you by proving bridges (accessible via JavaScript) to access device features!  \nI mean, you don‚Äôt need to dive into the Java Android API or Objective-C language (for iPhone/iPad), or any other API for other devices‚Ä¶ You just have to learn **web technologies**.\n\nWe are in 2011, the ‚Äúonly desktop application‚Äù model is over now, and mobile and tablet are two new platforms you should be aware of. So it changes everything about the technology to use.\n\n### Having a common language for all instances\n\nInstances of a single application can be numerous.  \nIn fact, an application can be projected in at least 3 axis of instance : The **platform** (mobile, tablet, desktop, ‚Ä¶), the **Operating System** (Android, iPhone, webOS) and the **application version** (free version, full version, ‚Ä¶).\n\n![](/images/2011/application-axis3.png)\n\nThat‚Äôs pretty expensive to develop X instances of an application. This is a problem for developing the first version and mainly for maintainability : You want to fix bugs and add features once, and only once.\n\n**So, the point is we need a common language to describe an application with multiple instances.**\n\n### Web is great for that!\n\nComputers have browsers, mobiles and tablets device have recent browsers.\n\nTo make your application development fully independent from the device, firstly you need a great **framework** to bridge your application and the device (like [PhoneGap][6]), secondly you need a great tool to easily **build** all applications from your common source code.\n\n\nFirst of all, let‚Äôs see how to organize a web project.\n\n### Good practice\n\nThis is how I‚Äôve organize my project :\n\n#### The source code directory\n\nThis directory contains all your web app source code. You should keep your application source code (with HTML, CSS, Javascripts, images, sounds, ‚Ä¶) in one directory (like */src* ).  \nYou should **avoid specific code**, but sometimes you still need some specific behaviors for different devices. If so, I recommend you to put these differences inside different files (for exemple: *mobile.html*, *tablet.html*, *computer.html*,‚Ä¶).\n\n#### Project skeletons\n\nKeep one skeleton directory for each instance of your application.  \nA skeleton directory will contains all the specific code related to the platform/device/version.  \n**Frameworks like PhoneGap bring you these skeletons.**\n\n* * *\n\n## WebAppBuilder\n\n![](/images/2011/webappmaker.png)\n\nI created **WebAppBuilder : a lightweight Makefile to build your project**. This is a mashup of existing cool stuff like : a small template system (Mustache), SASS with Compass, Javascript minimizer, ‚Ä¶\n\n### Features of WebAppBuilder\n\n*   Template easily your HTML files with [Mustache][8].\n*   Copy, concatenate, minimize Javascripts however you want.\n*   Retrieve Javascript files from URLs (useful for libraries).\n*   Compile SASS files into CSS files (if you use [this awesome stylesheets language][9])\n*   Support [Compass][10] if installed (you don‚Äôt need to provide it in your source, only an import works)\n*   Merge your CSS files.\n*   Copy and optionally rename resources you want to include (images, fonts, sounds,‚Ä¶).\n*   Error handling and atomicity : if one operation fail, the make fail (javascript syntax error, sass syntax error, ‚Ä¶)\n\nYou must have one Makefile per project skeleton, so you can easily define what to do with the */src* for the related platform/device/OS.\n\n### Download or Contribute\n\n[Fork me on Github][3]\n\n### Example with my Same Game Gravity game\n\nI developed these tools during the [Same Game Gravity][2] game development. \n\nA **make** inside my android/ skeleton gives me :\n\n![](/images/2011/webappmaker-term.png)\n\nAnd here is the Makefile I use :\n\n#### Android Makefile\n\n```makefile\n# Same Game Gravity - Android full version #\n        \n        ###             ~ Web App Builder ~               ###\n        #       a Makefile to compile a web project.        #\n        #  designed for web project with different devices  #\n        #  (mobile, tablet, desktop) but with common code.  #\n        ###        by @greweb  -  http://greweb.fr/       ###\n \n# BUILD_DIR : PATH to Web App Builder /build directory (the directory containing all build tools)\nBUILD_DIR = ../build\n \n# SRC_DIR : the source directory\nSRC_DIR = ../app\n \n# DIST_DIR : the dist directory (ex: assets for android, www for iphone)\nDIST_DIR = assets\n \n# RESOURCES : Your assets (images, sounds, fonts... and other static files)\n# You can rename dist file by prefix newname= ( ex: index.html=iphone_version.html )\nRESOURCES = Chewy.ttf logo.png background.jpg pop.mp3 swosh.mp3 gravity_exemple.png\n \n# VIEWS : Views will be interpreted by Mustache.js\n# You can pass arguments with JSON format.\n# Example: index.html:\"{key1:value1,key2:value2,...}\"  \u003c= no spaces!\nVIEWS = index.html=mobile.html:\"{versionType:'',version:'1.0',platform:'mobile',android:true,free:false}\"\n \n### SCRIPTS : all javascripts\n# - You can pass an URL to retrieve\n# - if you want to minimize the JS, prefix with '!'\n# - to mix scripts, concat them with a comma ','\n# - to set the destination name, you can prefix scripts with 'myname.js=' else the first script name is used ( exemple: all.js=util.js,ui.js,main.js ).\nSCRIPTS = game.min.js=!game.js,!game.mobile.js,!md5.js \\\n          phonegap.min.js=!phonegap.js,!phonegap.webintent.js \\\n          jquery.min.js=http://ajax.googleapis.com/ajax/libs/jquery/1/jquery.min.js,jquery.ba-hashchange.min.js,jquery.tmpl.min.js\n \n### STYLES : all styles : CSS or SASS\n# - For .sass files, we compile them to css\n# - Like before, you can mix styles with ',' and you can name your target by prefixing 'name='\nSTYLES = game.css=mobile.sass\n \n########################################################################\n \n \nall: welcome clean assets_views assets_scripts assets_styles assets_files\n \nwelcome:\n\t@@${BUILD_DIR}/welcome.sh\n \nassets_base: \n\t@@mkdir -p ${DIST_DIR}\n \nassets_views: assets_base\n\t@@${BUILD_DIR}/compile_views.sh ${SRC_DIR} ${DIST_DIR} ${VIEWS}\n \nassets_scripts: assets_base\n\t@@${BUILD_DIR}/compile_scripts.sh ${SRC_DIR} ${DIST_DIR} ${SCRIPTS}\n \nassets_styles: assets_base\n\t@@${BUILD_DIR}/compile_styles.sh ${SRC_DIR} ${DIST_DIR} ${STYLES}\n \nassets_files: assets_base\n\t@@${BUILD_DIR}/copy_resources.sh ${SRC_DIR} ${DIST_DIR} ${RESOURCES}\n \nclean: \n\t@@rm -rf ${DIST_DIR}\n \n.PHONY: welcome clean assets_views assets_scripts assets_styles assets_files\n```\n\n### Configuring your IDE\n\nI use mainly komodo and geany as an IDE. They both have a build system. I recommand you to configure your IDE to make \u0026\u0026 open the page just by pressing a shortcut key.\n\n### Features planned\n\n*   make should build the .apk for Android app\n\n## Special thanks\n\n*   to [mrspeaker][12] for English review.\n*   to [jQuery build system][13] (js minifier)\n\n","data":{"title":"Automating Web App development for multiple platforms","description":"In this article, we will explain why we‚Äôd choose web technologies to make applications and introduce WebAppBuilder, a tool to easily build different instances of an application. We‚Äôll examine the Same Game Gravity as an example.","thumbnail":"/images/2011/webappmaker.png","author":"Gaetan","layout":"post","permalink":"/2011/06/automating-web-app-development-for-multiple-platforms/","tags":["linux"]}},{"id":"2011-04-24-releasing-same-game-gravity-android","year":"2011","month":"04","day":"24","slug":"releasing-same-game-gravity-android","content":"# \n\nFollowing on from the success of my Same game made in HTML5 Canvas, I‚Äôve decided to extend it to a (web) mobile game.\n\nIt‚Äôs now available on Android market.\n\nMore infos on \u003chttp://gre.github.io/same-game-gravity\u003e.\n\n\u003ciframe width=\"640\" height=\"360\" src=\"http://www.youtube.com/embed/Qyd69g9hmIY?feature=player_embedded\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\nThis ball removal puzzle game is made with HTML5 technologies and PhoneGap. Source code will be published within a few months.\n","data":{"title":"Releasing Same Game Gravity (Android)","description":"Following on from the success of my Same game made in HTML5 Canvas, I‚Äôve decided to extend it to a (web) mobile game.","author":"Gaetan","layout":"post","permalink":"/2011/04/releasing-same-game-gravity-android/","tags":["mobile","gamedev","canvas"]}},{"id":"2011-03-12-html-canvas-pour-les-neophytes","year":"2011","month":"03","day":"12","slug":"html-canvas-pour-les-neophytes","content":"\n [1]: http://whatwg.org/html\n [2]: http://www.whatwg.org/specs/web-apps/current-work/multipage/the-canvas-element.html\n [3]: http://www.mrspeaker.net/dev/parcycle/\n [4]: http://en.inforapid.org/\n [5]: http://gre.github.io/same-game-gravity\n [6]: http://fizz.bloom.io/ \n [7]: http://easeljs.com/\n [8]: http://processingjs.org/ \n\nCette vid√©o de 20 minutes pr√©sente les possibilit√©s du Canvas √† travers quelques d√©mos et l‚Äôimpl√©mentation pas √† pas d‚Äôun exemple basique.  \nElle est destin√©e √† des d√©veloppeurs d√©butant dans l‚Äôutilisation de Canvas.\n\n\u003ciframe src=\"http://player.vimeo.com/video/20957255?portrait=0\" width=\"550\" height=\"410\" frameborder=\"0\" webkitAllowFullScreen mozallowfullscreen allowFullScreen\u003e\u003c/iframe\u003e\n\n*Dans la suite de l‚Äôarticle : les liens et codes de la vid√©o ‚Ä¶*\n\n\u003c!--more--\u003e\n\n## Liens\n\n### Specs\n\n*   [whatwg.org/html][1]\n*   [Canvas][2]\n\n### Exemples\n\n*   [parcycle][3]\n*   [Visualisation des relations entre sujets (wikip√©dia)][4]\n*   [Same Game][5]\n*   [fizz : visualisation des tweets][6]\n\n### Biblioth√®ques graphiques\n\n*   [EaselJS][7]\n*   [Processing JS][8]\n\n\n## Exemple de l‚Äôimpl√©mentation\n\n```html\n\u003chtml\u003e\n  \u003chead\u003e\n    \u003cstyle\u003e\n      body {\n        background: #ddd;\n      }\n      canvas {\n        background: #fff;\n      }\n    \u003c/style\u003e\n  \u003c/head\u003e\n  \u003cbody\u003e\n    \u003ccanvas id=\"sketch\" width=\"300\" height=\"300\"\u003e\u003c/canvas\u003e\n    \u003cimg id=\"image\" src=\"http://www.whatwg.org/images/logo\" style=\"display: none;\" /\u003e\n    \n    \u003cscript type=\"text/javascript\"\u003e\n    (function(){\n      \n      var canvas = document.getElementById('sketch');\n      var ctx = canvas.getContext('2d');\n      var img = document.getElementById('image');\n      \n      var i = 0;\n      setInterval(function() {\n        ctx.clearRect(0, 0, 300, 300);\n        \n        ctx.drawImage(img, 0, 0, 300, 300);\n        \n        ctx.fillStyle = 'rgba(255, 0, '+Math.floor(Math.sin(i/50)*255)+', 0.8)';\n        ctx.fillRect(100, i % 300, 100, 100);\n        \n        /*\n        \n        ctx.strokeStyle = '#09F';\n        ctx.lineWidth = 5;\n        \n        ctx.beginPath();               // commence √† tracer un chemin\n        ctx.moveTo(0, 20);             // d√©fini le premier point de tracage √† la position (0, 20)\n        ctx.lineTo(canvas.width-100, 30);  // Tracer une ligne jusqu'√† la position (canvas.width, 30). canvas.width d√©signe la largeur du canvas (500 dans notre exemple).\n        ctx.bezierCurveTo(100, 200, 0, 100, 300, 300);\n        ctx.stroke();                  // Indique au canvas de dessiner le chemin trac√© depuis le beginPath\n        \n        */\n        \n        ++ i;\n      }, 30);\n      \n    }());\n    \u003c/script\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n```\n","data":{"title":"HTML Canvas pour les n√©ophytes","description":"Cette vid√©o de 20 minutes pr√©sente les possibilit√©s du Canvas √† travers quelques d√©mos et l‚Äôimpl√©mentation pas √† pas d‚Äôun exemple basique.","author":"Gaetan","layout":"post","permalink":"/2011/03/html-canvas-pour-les-neophytes/","tags":["canvas","html","javascript"]}},{"id":"2011-01-05-how-to-make-dlink-dwa-140-perfectly-work-on-linux","year":"2011","month":"01","day":"05","slug":"how-to-make-dlink-dwa-140-perfectly-work-on-linux","content":"\n**[EDIT] Note: the DWA-140 B2 is now supported by recent Linux version.**\n\nI‚Äôm using ArchLinux and I finally make my DWA-140 B2 Wifi USB adaptor work !\n\nIf you have the same problem, you can read this article to fix it !\n\n\u003c!--more--\u003e\n\n**My conf**:\n\n* Linux Kernel: *2.6.36*\n* Architecture: *x64*\n* Device: *ID 07d1:3c0a D-Link System DWA-140 RangeBooster N Adapter(rev.B2) [Ralink RT2870]*\n\n\n## What to use ?\n\nSo the only way I found to make it work nicely is to use **ndiswrapper**, a Windows XP driver wrapper, and **wpa_supplicant**, a WLAN tool.\n\nFirst, I‚Äôve tried some RaLink drivers but this was not really great, I‚Äôve succeed to make `ra2870` work only one time but there was some lags each 20 seconds (like 1000ms ping frequently).\n\nMoreover, I recommend not using NetworkManager with this method, it seems ndiswrapper and networkmanager produced awful results for this bundle (like waiting 30s for wifi to connect (or not connecting!), or freezing my Linux, ‚Ä¶). Use wpa_supplicant utils like netcfg instead of ! \n\n## Procedure\n\nAlternatives rejected, here is the solution step by step :\n\n### Pre-conditions\n\nWith a wired connection, install these packages (with your package manager) : **ndiswrapper**, **wpa_supplicant** and (optional) **netcfg**.\n\nUnfortunately, I had some freeze issues with the current ndiswrapper repository version so I checkout ndiswrapper SVN source code on  and recompile it. **Maybe you have to do the same.**\n\n1.  Don‚Äôt plug your bundle yet. Insert the CD of the product. It contains the driver we need for ndiswrapper. You need to find the `/Drivers/WinXPx64/` (it may depend on your arch) **.inf** file. You can also download the last driver on the [D-Link website][2].\n2.  Go on commandline in root mode and type:\n\n```bash\nndiswrapper -i {path of the .inf file}\n```\n    \nExample:\n    \n```bash\nndiswrapper -i /media/cdrom0/Drivers/WinXPx64/Drt2870.inf\n```\n   \n* Plug your bundle and check if the command `ndiswrapper -l` say something like: \n\n```\ndrt2870 : driver installed  \ndevice (07D1:3C0A) present\n```   \n\nIf not you maybe need to `ndiswrapper -r drt2870` to remove the driver and return to the second step trying another `/Drivers/*/*.inf` file. \n\n*   Next, you need to configure your `wpa_supplicant.conf` configuration file for  your wifi access point. Refer to the [documentation][3].\n*   Try to run `wpa_supplicant` : \n\n```bash\nwpa_supplicant -i wlan0 -c /etc/wpa_supplicant.conf\n```\n\nOnce this working, you maybe need to run dhcpd :\n      \n```bash\ndhcpd wlan0\n```\n      \n*   Now you can try your internet connection, by pinging google.com or try to browse the web.\n*   If it works, you can save your configuration, refer to the [documentation][3]. \n        \n**For any problem, feel free to comment this article.**\n\n [2]: http://www.dlink.com/products/?tab=3\u0026pid=DWA-140\u0026rev=DWA-140_revB\n [3]: https://wiki.archlinux.org/index.php/WPA_supplicant\n","data":{"title":"How to make DLink DWA-140 B2 perfectly work on Linux","author":"Gaetan","layout":"post","permalink":"/2011/01/how-to-make-dlink-dwa-140-perfectly-work-on-linux/","tags":["linux"]}},{"id":"2010-05-05-css3-transitions-available-on-firefox-3-7","year":"2010","month":"05","day":"05","slug":"css3-transitions-available-on-firefox-3-7","content":"\n [1]: http://www.w3.org/TR/css3-transitions/\n\n[CSS3 transitions][1] are now available on Firefox, Chrome, Safari, Opera and ‚Ä¶ IE9!, and it‚Äôs awesome.\n\n\u003e CSS Transitions allows property changes in CSS values to occur smoothly over a specified duration.\n\n**Javascript is not anymore required for simple animation.**\n\nSpecifically, we don‚Äôt need Javascript to manage animation with **setInterval** or with any library like **jQuery.animate** : Forget the animation management, stay focused on the real work.\n\nIn this article, I will try to explain why and how using css3-transition with some examples.\n\n\u003c!--more--\u003e\n\n## Why ?\n\n*   It‚Äôs **simple** and **smart** : adding one line of css code.\n*   It **let the browser rendering animation instead of using javascript complex code**. You don‚Äôt have to worry about animation and performance. So you can stay focused on real part of your project.\n*   It‚Äôs **cleaner** than some javascript animation implementation because you don‚Äôt modify style attribute during animations. So, it‚Äôs also probably more efficient.\n*   **Degradation is great.** If your browser doesn‚Äôt support CSS Transition, it‚Äôs not really bad, only the animation is not available. **Other behaviors aren‚Äôt alterate**. For example, imagine a photo slide-show with zoom effect when changing photo. With an old browser, photos are just instantly zoomed without animation. That‚Äôs not bad.\n*   And off course, It‚Äôs **standard**.\n\n## How ?\n\nCSS Transition are **extremely simply to use**.\n\n### transition-duration\n\nBasically, you set a **css time properties** in a css selector *like `-moz-transition-duration: 1s;` for mozilla*. This time define the animation duration. Browser will determine the transition between this selector and a descendant selector.\n\nNot that css3 transition is currently in draft mode, so there are multiple property for each browser (the prefix change). \n\nFor Firefox (3.7 ), Chrome (and other webkit browser) and Opera, you have to use : \n\n```css\n-moz-transition-duration: 1s;  \n-webkit-transition-duration: 1s;  \n-o-transition-duration: 1s;\n```\n\nDon‚Äôt panic, in the future (on CSS3 release), only one property will be used.\n\n#### Example\n\n```css\n.box {  \n¬† -moz-transition-duration: 1s;  \n¬† -webkit-transition-duration: 1s;  \n¬† -o-transition-duration: 1s;  \n  \n¬† margin: 10px;  \n¬† background-color: red;  \n}  \n.box:hover {  \n¬† margin: 50px;  \n¬† background-color: green;  \n}\n```\n\nOn mouse over the **.box**, during one second : margin will move from **10px** to **50px** and background-color will move from **red** to **green**. That‚Äôs all!\n\n### transition-property\n\nYou can also specify the name of the CSS property to which the transition is applied.\n\nFor instance, **color**, **width**, **opacity**, ‚Ä¶\n\n#### Like this\n\n```css\n-moz-transition-property: margin, background-color;  \n-webkit-transition-property: margin, background-color;  \n-o-transition-property: margin, background-color;\n```\n\n### Others properties\n\nMore properties are available to specify more deeply transition effects. Retrieve them on [CSS Transitions Working Draft][1].\n\n*   transition-timing-function\n*   transition-delay\n\n## Examples\n\nHere are some CSS transition examples.\n\n* [A box with color, text, shapes transformation.](/demo/css3/transition/box1/)\n* [Letters animation.](/demo/css3/transition/letters/)\n* [Image slider.](http://sliderjs.org/)\n* [Navigation bar.](/demo/css3/transition/navbar/)\n\n","data":{"title":"CSS3 Transitions","author":"Gaetan","layout":"post","permalink":"/2010/05/css3-transitions-available-on-firefox-3-7/","tags":["css","animation","transition"]}},{"id":"2010-04-04-automatiser-lexportation-dun-site-statique-avec-wget","year":"2010","month":"04","day":"04","slug":"automatiser-lexportation-dun-site-statique-avec-wget","content":"\nIl est pr√©f√©rable d‚Äô**utiliser un framework web** m√™me si l‚Äôon veut r√©aliser un **site vitrine simple (statique)**, car l‚Äôon b√©n√©ficie des avantages du framework notamment de l‚Äôh√©ritage (entre autre) des templates, de l‚Äôinternationalisation, de la configuration des _routes_, du _SASS_, etc.\n\nDe plus, cela permet de rendre la maintenabilit√© moins longue et couteuse.\n\nN√©anmoins, son h√©bergeur ne permet pas toujours de faire tourner son site avec le framework utilis√© (par exemple _ruby on rails_ ou _play! framework_).\n\nPour rem√©dier √† ce probl√®me, il suffit d‚Äô**exporter son site statique en simples pages HTML**.\n\nCet article est donc destin√© aux particuliers ne voulant pas investir dans un serveur d√©di√©, fans de framework avanc√©s qui ne sont pas support√©s par les offres d‚Äôentr√©e de gamme des h√©bergeurs (qui n‚Äôoffrent g√©n√©ralement que le support du php).\n\n**D‚Äôailleurs, pour ce type de site, pourquoi utiliser un framework MVC si l‚Äôon utilise que des vues ?**\n\n## La d√©marche\n\n### Le site web\n\n**R√©alisez le site** avec votre framework pr√©f√©r√© et faites en sorte que la page principale (**/**) **permette l‚Äôacc√®s √† toutes les pages du site** par des liens (pas forc√©ment directs).\n\n### Exportation\n\nAvec **wget** r√©cup√©rez votre site √† la ligne de commande :\n\n```bash\nwget -r -k -np \"http://localhost:9000/\"\n```\n\nA l‚Äôinstar d‚Äôun moteur de recherche, c‚Äôest cette commande qui va s‚Äôoccuper de **retracer toutes les pages web de votre site**, mais aussi toutes ses ressources (images, css, js, ..). En plus de cela, elle va **pr√©server le fonctionnement des liens** entre pages.\n\nAdaptez _http://localhost:9000/_ √† l‚Äôurl de votre site.\n\nCela va cr√©er le dossier _localhost:9000_, il ne vous restera plus qu‚Äô√† l‚Äôenvoyer sur votre serveur http.\n\n## Internationalisation\n\nVoici une solution pour exporter son internationalisation.\n\nElle consiste √† rediriger l‚Äôutilisateur, depuis la page principale, sur la bonne page internationalis√©e en fonction de sa langue (indiqu√©e par le navigateur), tout **en gardant les fichiers publics en commun** (css, images).\n\n### Pr√©parer ses routes\n\npr√©parez les uri pour qu‚Äôelles soient de la forme **/{lang}/\\*/** avec **lang** d√©signant la langue d√©sir√©e.\n\n#### Exemples de routes\n\n```\n/en/\n/en/about/\n/fr/\n/fr/about/\n```\n\n### Ajouter des liens dans les templates pour changer de langue\n\nCette action est d√©licate car, comme pr√©cis√© auparavant, **wget** essaye de retracer tous les liens entre pages.\n\nAinsi, si l‚Äôon cr√©e des liens entre les diff√©rentes langues, **wget** cr√©era un dossier _public_ pour chaque langue internationalis√©e.\n\nPour contourner ce probl√®me et ainsi factoriser les fichiers communs, **une solution est de cr√©er les liens √† posteriori**.\n\n#### Solution exemple avec play framework\n\n```\n\u003cp\u003e\n#{if lang!='en'}\n  \u003ca ADD_HERE_ENGLISH_LINK_ATTRIBUTES\u003eenglish version\u003c/a\u003e\n#{/if}\n#{if lang!='fr'}\n  \u003ca ADD_HERE_FRENCH_LINK_ATTRIBUTES\u003efrench version\u003c/a\u003e\n#{/if}\n\u003c/p\u003e\n```\n\n```bash\n#!/bin/bash\n# script bash\nwget -r -k -np \"http://localhost:9000/\"\nsed -i s/ADD_HERE_ENGLISH_LINK_ATTRIBUTES/href=\\\"\\\\/en\\\\/\\\"/g $(find . -name \"*.html\")\nsed -i s/ADD_HERE_FRENCH_LINK_ATTRIBUTES/href=\\\"\\\\/fr\\\\/\\\"/g $(find . -name \"*.html\")\n```\n\n### Pr√©parer une page de redirection\n\n**Placez √† posteriori un script de redirection √† l‚Äôurl /** pour rediriger vers la page ad√©quat.\n\n#### Par exemple un script php\n\n```php\n\u003c?php\n$langs = array();\n\nif (isset($_SERVER['HTTP_ACCEPT_LANGUAGE'])) {\n    preg_match_all('/([a-z]{1,8}(-[a-z]{1,8})?)\\s*(;\\s*q\\s*=\\s*(1|0\\.[0-9]+))?/i', $_SERVER['HTTP_ACCEPT_LANGUAGE'], $lang_parse);\n    if (count($lang_parse[1])) {\n        $langs = array_combine($lang_parse[1], $lang_parse[4]);\n        foreach ($langs as $lang =\u003e $val) {\n            if ($val === '') $langs[$lang] = 1;\n        }\n        arsort($langs, SORT_NUMERIC);\n    }\n}\n\nforeach ($langs as $lang =\u003e $val) {\n  if (strpos($lang, 'fr') === 0) {\n    header('Location: fr');\n    die();\n  }\n  else if (strpos($lang, 'en') === 0) {\n    header('Location: en');\n    die();\n  }\n}\nheader('Location: en');\n?\u003e\n```\n","data":{"title":"Automatiser l'exportation d'un site statique avec wget","author":"Gaetan","layout":"post","permalink":"/2010/04/automatiser-lexportation-dun-site-statique-avec-wget/","tags":["playframework"]}},{"id":"2010-04-03-realiser-une-maquette-web-avec-le-css-3","year":"2010","month":"04","day":"03","slug":"realiser-une-maquette-web-avec-le-css-3","content":"\n [1]: http://compass-style.org/\n [2]: /2010/03/sass-levolution-du-css\n [3]: /images/2010/css3/exemple_border_radius.png\n [4]: /images/2010/css3/exemple_gradient.png\n [5]: /images/2010/css3/exemple_text_shadow.png\n [6]: /images/2010/css3/exemple_box_shadow.png\n [7]: /images/2010/css3/university_nostalgia_exemple.png\n [8]: http://github.com/gre/University-nostalgia\n [9]: /images/2010/css3/triangle_01.png\n [10]: /images/2010/css3/triangle_02.png\n [11]: /images/2010/css3/triangle_03.png\n [12]: /images/2010/css3/triangle_menus.png\n\nLe CSS depuis sa version 3 constitue une **bonne √©volution technologique pour palier l‚Äôutilisation abusive d‚Äôimages** dans la r√©alisation d‚Äôune application web.\n\nIl n‚Äôest ainsi plus n√©cessaire de recourir √† des images pour r√©aliser des **ombres**, des **d√©grad√©s**, des **bordures arrondis** ou encore utiliser des **polices sp√©cifiques**.\n\nL‚Äôavenir du CSS3 nous promet encore plus : il est d‚Äôores et d√©j√† possible sur certains navigateurs d‚Äôeffectuer des **animations**, des **transitions**, des **effets de reflets**,‚Ä¶ sans devoir recourir au *javascript*, au *canvas* ou, pire encore, au *flash*.\n\n\u003c!--more--\u003e\n\n## G√©n√©ralit√©s\n\n### Pourquoi ?\n\nQuelques raisons pour utiliser le CSS 3‚Ä¶\n  \n#### dans une optique de maintenabilit√©\n\nEn utilisant des images pour r√©aliser des effets simples, **il faut mettre √† jour l‚Äôimage √† chaque fois**. Si bien qu‚Äôil est n√©cessaire d‚Äôutiliser un logiciel de dessin et de garder les sources pour pouvoir facilement modifier les couleurs √† l‚Äôavenir.  \n**La modification d‚Äôun th√®me est alors p√©nible** : Pour chaque image du th√®me : il faut ouvrir la source de l‚Äôimage avec son logiciel pr√©f√©r√© (GIMP, Photoshop, ‚Ä¶), modifier la couleur, exporter l‚Äôimage, recharger la page,‚Ä¶ A contrario, **il ne suffit que de modifier une ligne de code CSS pour changer sa couleur** (l‚Äôutilisation de variable avec SASS est int√©r√©ssant).\n\n#### par simplicit√©\n\nLe CSS 3 **offre plus de possibilit√©s** donc **simplifie bien des t√¢ches** qui √©taient complexe √† r√©aliser auparavant. Bordures arrondis, ombres, d√©grad√©s, animations, reflets,‚Ä¶ sont maintenant facilement r√©alisables avec le CSS 3.\n\nVoici un exemple frappant parmi tant d‚Äôautres :  \nAuparavant, on voyait fleurir √† foison des tableaux pour r√©aliser de simples bordures arrondis.\n\n##### On voyait auparavant\n\n```html\n\u003ctable class=\"radius\"\u003e\n\u003ctr\u003e\n\u003ctd class=\"topleft\"\u003e\u003c/td\u003e\n\u003ctd class=\"top\"\u003e\u003c/td\u003e\n\u003ctd class=\"topright\"\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd class=\"left\"\u003e\u003c/td\u003e\n\u003ctd class=\"content\"\u003econtent here...\u003c/td\u003e\n\u003ctd class=\"right\"\u003e\u003c/td\u003e\n\u003c/tr\u003e\u003ctr\u003e\n\u003ctd class=\"bottomleft\"\u003e\u003c/td\u003e\n\u003ctd class=\"bottom\"\u003e\u003c/td\u003e\n\u003ctd class=\"bottomright\"\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/table\u003e\n\u003cstyle\u003e\ntable.radius .topleft {\nbackground: url(topleft.png);\nwidth: 20px;\nheight: 20px;\n}\n/* DE MEME pour les 8 autres cases ... */\n\u003c/style\u003e\n```\n\n##### Maintenant avec le CSS3\n\n```html\n\u003cdiv class=\"radius\"\u003econtent here...\u003c/div\u003e\n\u003cstyle\u003e\n.radius {\n  border-radius: 20px;\n}\n\u003c/style\u003e\n```\n\n#### dans un objectif de scalabilit√© des th√®mes\n\nL‚Äôutilisation des possibilit√©s du CSS 3 au lieu d‚Äôimages va permettre d‚Äô**√©tendre rapidement son application web √† plusieurs th√®mes** et de modifier facilement ces th√®mes. Il suffira d‚Äôavoir un fichier pour chaque th√®me.\n\n### SASS et le framework Compass\n\nAfin de r√©soudre les probl√®mes de *cross-navigateur* abord√©s pr√©c√©demment, je vous conseille l‚Äôutilisation du **SASS** et du framework [Compass][1].  \nPour plus de renseignements sur le SASS, n‚Äôh√©sitez pas √† lire [cet article][2].\n\n### Principales nouveaut√©s du CSS 3\n\nDans la suite de cet article, **pour plus de simplicit√©s, nous nous arr√™tons √† la compatibilit√© de Firefox** (pr√©fix√© par -moz) mais bien entendu, il est possible de les rendre compatibles avec tous les navigateurs r√©cents. Comme pr√©cis√© avant, l‚Äôutilisation du framework *Compass* permet entre autres de r√©soudre ce probl√®me.\n\n#### Les bordures arrondis\n\n![][3]\n\n```css\n.exemple {  \n¬† -moz-border-radius: 10px;  \n¬† border: 1px solid black;  \n¬† padding: 2px 5px;  \n}  \n```\n\n#### Les d√©grad√©s\n\nCompatibles depuis Firefox 3.6.  \n![][4]\n\n```css\n.exemple {  \n¬† -moz-border-radius: 10px;  \n¬† border: 1px solid black;  \n¬† padding: 2px 5px;  \n¬† background: -moz-linear-gradient(-90deg, green, yellow);  \n}  \n```\n\n#### Les ombres\n\n##### Sous les textes\n\n![][5]\n\n\n```css\n.exemple {  \n¬† -moz-border-radius: 10px;  \n¬† border: 1px solid black;  \n¬† padding: 2px 5px;  \n¬† background: -moz-linear-gradient(-90deg, green, yellow);  \n¬† text-shadow: -1px -1px 1px yellow;  \n}\n```\n\n\n##### Sous les √©l√©ments\n\n![][6]\n\n```css\n.exemple {  \n¬† -moz-border-radius: 10px;  \n¬† border: 1px solid black;  \n¬† padding: 2px 5px;  \n¬† background: -moz-linear-gradient(-90deg, green, yellow);  \n¬† text-shadow: -1px -1px 1px yellow;  \n¬† -moz-box-shadow: 1px 1px 1px rgba(0, 0, 0, 0.5);  \n}  \n```\n\nA noter d‚Äôailleurs que la couleur de format **rgba(r, g, b, a)** est une nouveaut√© du CSS 3.\n\n### Optimiser la compatibilit√© avec les navigateurs plus anciens\n\nVoici une utilisation intelligente des d√©grad√©s (avec SASS) pour permettre une meilleure **compatibilit√©** avec, en dernier recours, une couleur ou une image alternative :\n\n```sass\n=vertical-gradient(!from, !to, !alt=(!from/2   !to/2))  \n¬† background = !alt  \n¬† background = -webkit-gradient(linear, left top, left bottom, from(!from), to(!to))  \n¬† background = -moz-linear-gradient(-90deg, !from, !to)  \n  \n/* exemples d'utilisation : */  \n#main \u003e header  \n¬†  vertical-gradient(#719369, #587451, url(header.png))  \n  \n#main \u003e header  \n¬†  vertical-gradient(green, yellow, #80FF80)\n```\n\n## Exemple de maquette\n\nVoici un **exemple d‚Äôutilisation du CSS3** utilis√© sur un projet r√©cent (r√©alis√© √† l‚Äôuniversit√©).  \n\n![][7]\n\nIl n‚Äôy a aucune image (sauf l‚Äôavatar tux) et l‚Äôapplication est enti√®rement compatible avec au minimum Firefox et Chrome.  \nSi l‚Äôapplication vous int√©resse, elle est sur [github][8].\n\n\n### Extrait du code SASS\n\n```sass\n@import compass.sass  \n@import compass/reset.sass  \n@import theme.sass  \n  \n=vertical-gradient(!from, !to, !alt=(!from/2   !to/2))  \n¬† background = !alt  \n¬† background = -webkit-gradient(linear, left top, left bottom, from(!from), to(!to))  \n¬† background = -moz-linear-gradient(-90deg, !from, !to)  \n  \n!back = #e9f3e7  \n!back_aside = #F8FAF5  \n!back\\_header\\_from=#719369  \n!back\\_header\\_to=#587451  \n!back\\_footer\\_from=#b4c1b1  \n!back\\_footer\\_to=#9da89a  \n  \n#wrapper  \n¬†  border-radius(!global\\_border\\_radius)  \n¬† :border 1px solid !back-#111  \n¬† \u003enav  \n¬† ¬†  border-top-radius(10px)  \n¬† ¬† :background white  \n¬† \u003eheader  \n¬† ¬†  vertical-gradient(!back\\_header\\_from, !back\\_header\\_to, url(/public/images/header.png))  \n¬† ¬† :color white  \n¬† ¬† :z-index 1  \n¬† \u003efooter  \n¬† ¬†  vertical-gradient(!back\\_footer\\_from, !back\\_footer\\_to)  \n¬† ¬†  border-bottom-radius(10px)  \n  \n¬† #main  \n¬† ¬† background = !back_aside  \n¬† ¬†  box-shadow(0px, -1px, 2px, rgba(,,,0.25))  \n¬† ¬† :z-index 2  \n¬† ¬† \u003esection  \n¬† ¬† ¬† :background white\n```\n\n### Un triangle en css ???\n\nCe n‚Äôest pas de la magie mais **il est possible de faire des triangles en CSS** (m√™me en CSS2) gr√¢ce √† une petite astuce. Explication en images :\n\n#### Une boite avec une couleur pour chaque bordure\n\n![][9]\n\n```css\n.box {  \n¬† background: black;  \n¬† width: 40px;  \n¬† height: 40px;  \n¬† border-width: 15px;  \n¬† border-color: yellow red blue green;  \n¬† border-style: solid;  \n}\n```\n\n\n#### width et height √† 0\n\n![][10]\n\n#### Bordures transparentes\n\n![][11]\n\n```css\n.box {  \n¬† background: black;  \n¬† width: 0;  \n¬† height: 0;  \n¬† border-width: 15px;  \n¬† border-color: transparent red transparent transparent;  \n¬† border-style: solid;  \n}  \n```\n\n#### Autre utilisation des triangles\n\nLe m√™me proc√©d√© m‚Äôa permis de faire ceci :  \n![][12]\n\n","data":{"title":"R√©aliser une maquette web avec le CSS 3","author":"Gaetan","layout":"post","permalink":"/2010/04/realiser-une-maquette-web-avec-le-css-3/","tags":["css","sass"]}},{"id":"2010-03-14-sass-levolution-du-css","year":"2010","month":"03","day":"14","slug":"sass-levolution-du-css","content":"\n[1]: http://sass-lang.com/\n[2]: http://www.playframework.org/\n[3]: http://compass-style.org/\n[4]: http://sass-lang.com/docs/yardoc/file.SASS_REFERENCE.html\n\n**SASS, Syntactically Awesome Stylesheets**, est un langage de feuille de style √©volu√© qui permet de factoriser beaucoup de code css et de rendre son √©criture et sa maintenance **rapide et moins contraignante**. Il est compil√© en css.\n\n\u003c!--more--\u003e\n\n## Pourquoi utiliser SASS ?\n\nSon utilisation a de nombreux avantages par rapport au CSS :\n\n- sa **simplicit√©** (pas de crochets, pas de point virgule mais juste de l‚Äôindentation)\n- l‚Äô**imbrication** des s√©lecteurs css (appliquant l‚Äôid√©e DRY : don‚Äôt repeat yourself)\n- l‚Äôutilisation de **variables**\n- l‚Äôutilisation d‚Äô**op√©rations √©l√©mentaires** (sur les pixels, les couleurs, ‚Ä¶)\n- la **factorisation** du code (au lieu de faire des copier-coller, on peux factoriser le code √† travers les ‚Äúmixins‚Äù).\n- La **r√©duction** css et la **clart√©** du code\n- La **compression du code** compil√© avec la possibilit√© de tout mettre dans un fichier (via l‚Äôh√©ritage) et de minimifier le code css.\n\nCe langage n‚Äôest pas difficile √† apprendre, cela ressemble au css, avec de nombreuses fonctionnalit√©s int√©ressantes en plus.\n\n## La syntaxe du langage\n\nLa syntaxe du sass est **compatible avec celle du css √† quelques exceptions pr√®s** :\n\n- Ne plus mettre de point virgule **;**\n- Ne plus mettre de crochets **{ }**\n- Respecter les conventions traditionnelles (**attribut: valeur** un espace apr√®s le deux points mais pas avant)\n- Respecter l‚Äôindentation : Il faut choisir une indentation et s‚Äôy tenir dans un m√™me fichier. Au choix : une tabulation, 2 espaces, 4 espaces, ‚Ä¶ Les lignes _propri√©t√©s: valeurs_ d‚Äôun s√©lecteur css doivent d√©passer d‚Äôune indentation ce s√©lecteur.\n\nEn respectant ces points, vous pouvez d√©j√† **coder en SASS comme en CSS**.\n\nMais cela ne serait pas int√©ressant sans les nouveaut√©s suivantes :\n\n### La factorisation des s√©lecteurs en plusieurs niveaux\n\nAu lieu d‚Äôavoir ce type d‚Äôarborescence √† un niveau :\n\n```sass\n.main .head\n¬† color: red\n.main .body\n¬† color: blue\n```\n\nNous pouvons factoriser le s√©lecteur ‚Äú_.main_‚Äù et se ramener √† deux niveaux :\n\n```sass\n.main\n¬† .head\n¬† ¬† color: red\n¬† .body\n¬† ¬† color: blue\n```\n\nCe proc√©d√© de factorisation bas√© sur l‚Äôesprit **DRY** (Don‚Äôt Repeat Yourself) est aussi applicable sur les attributs eux-m√™mes :\n\n```sass\na\n¬† font:\n¬† ¬† family: serif\n¬† ¬† weight: bold\n¬† ¬† size: 1.2em\n```\n\nsera compil√© en css par :\n\n```css\na {\n¬† font-family: serif;\n¬† font-weight: bold;\n¬† font-size: 1.2em;\n}\n```\n\n### Les variables\n\nLa possibilit√© d‚Äôutiliser des variables est un gros apport au css. Elle permet **une meilleure maintenance du code et une meilleure scalabilit√© d‚Äôune application** (en utilisant par exemple des fichiers sass de th√®mes d√©finissant toutes les couleurs, images, polices, ‚Ä¶).  \nIl existe plusieurs **types de variables** (nombre r√©el, pixels, couleurs, chaines de caract√®res ‚Ä¶) et il est possible d‚Äôutiliser des **op√©rations √©l√©mentaires**.\n\nLorsqu‚Äôon √©crit une ligne **attribut / value** avec l‚Äôutilisation de variables (dynamique),  \non utilise le caract√®re ‚Äò**=**‚Äò au lieu de ‚Äò**:**‚Äò pour l‚Äôaffectation.\n\n#### Les couleurs\n\n```sass\n!link_color = red\na\n¬† color = !link_color\n¬† \u0026:hover\n¬† ¬† color = !link_color #222\n```\n\nA noter que le symbole **\u0026** remplace le s√©lecteur parent.\n\nCe qui donne le code compil√© suivant :\n\n```css\na {\n¬† color: red;\n}\na:hover {\n¬† color: #ff2222;\n}\n```\n\n#### Les pixels\n\n```sass\n!margin = 16px\n.border\n¬† padding = !margin / 2\n¬† margin = !margin / 2\n```\n\ndonne le code compil√© :\n\n```css\n.border {\n¬† padding: 8px;\n¬† margin: 8px;\n}\n```\n\n### Les ‚Äúmixins‚Äù\n\nLes mixins sont des proc√©dures qui contiennent plusieurs lignes de sass.  \nIl est possible d‚Äôutiliser des arguments sur ces mixins.\n\n```sass\n=border-radius(!radius = 5px)\n¬† border-radius= !radius\n¬† -moz-border-radius= !radius\n¬† -webkit-border-radius= !radius\n```\n\n```sass\n#wrapper\n¬†  border-radius(10px)\n¬† \u003e footer\n¬† ¬†  border-radius()\n```\n\nCet exemple est typiquement int√©ressant car il permet d‚Äôutiliser **border-radius** de fa√ßon **cross-browser** et avec une ligne de code.\n\nA noter qu‚Äôil est possible d‚Äôaffecter des valeurs par d√©faut aux mixins.\n\nLe r√©sultat css compil√© est le suivant :\n\n```css\n#wrapper {\n¬† border-radius: 10px;\n¬† -moz-border-radius: 10px;\n¬† -webkit-border-radius: 10px;\n}\n#wrapper \u003e footer {\n¬† border-radius: 5px;\n¬† -moz-border-radius: 5px;\n¬† -webkit-border-radius: 5px;\n}\n```\n\n### Exemple complet\n\nVoici un exemple complet de l‚Äôutilisation du SASS\n\n```sass\n/* This is just an example */\n\n/* variables */\n!main_width = 900px\n!aside_width = 300px\n!section_width = 520px\n\n!link_color = red\n\n!font_title = \"Liberation\",\"Georgia\",\"serif\"\n\n/* mixins */\n=border-radius(!radius = 5px)\n¬† border-radius= !radius\n¬† -moz-border-radius= !radius\n¬† -webkit-border-radius= !radius\n\n=block()\n¬† display: block\n¬† overflow: auto\n\n/* colors */\na\n¬† color = !link_color\n¬† \u0026:hover\n¬† ¬† color = !link_color #222\n\n\n/* layout */\n\n#wrapper\n¬† margin:  auto\n¬† position: relative\n¬† width = !main_width\n¬†  border-radius(10px)\n¬† \u003enav\n¬† ¬†  block()\n¬† ¬† padding: 2px\n¬† ¬† ¬† top: 5px\n¬† ¬† font-size: 1.2em\n¬† ¬† font-family = !font_title\n¬† ¬† a\n¬† ¬† ¬† font-weight: bold\n¬† ¬† ¬† \u0026:hover\n¬† ¬† ¬† ¬† color: white\n\n¬† \u003eheader\n¬† ¬†  block()\n¬† ¬† clear: both\n¬† ¬† height: 48px\n¬† ¬† font-family = !font_title\n\n¬† \u003efooter\n¬† ¬†  block()\n¬† ¬† padding: 5px\n¬† ¬† text-align: center\n¬† ¬† clear: both\n\n¬† #main\n¬† ¬† position: relative\n¬† ¬† \u003esection\n¬† ¬† ¬†  block()\n¬† ¬† ¬† width = !section_width\n¬† ¬† ¬† padding: 20px\n\n¬† ¬† \u003easide\n¬† ¬† ¬†  block()\n¬† ¬† ¬† float: right\n¬† ¬† ¬† width = !aside_width\n¬† ¬† ¬† padding: 20px\n```\n\net le r√©sultat du fichier CSS compil√©\n\n```css\n/* This is just an example */\n/* variables */\n/* mixins */\n/* colors */\na {\n¬† color: red;\n}\n¬† a:hover {\n¬† ¬† color: #ff2222;\n}\n\n/* layout */\n#wrapper {\n¬† margin:  auto;\n¬† position: relative;\n¬† width: 900px;\n¬† border-radius: 10px;\n¬† -moz-border-radius: 10px;\n¬† -webkit-border-radius: 10px;\n}\n¬† #wrapper \u003enav {\n¬† ¬† display: block;\n¬† ¬† overflow: auto;\n¬† ¬† padding: 2px;\n¬† ¬† padding-top: 5px;\n¬† ¬† font-size: 1.2em;\n¬† ¬† font-family: Liberation, Georgia, serif;\n}\n¬† ¬† #wrapper \u003enav a {\n¬† ¬† ¬† font-weight: bold;\n}\n¬† ¬† ¬† #wrapper \u003enav a:hover {\n¬† ¬† ¬† ¬† color: white;\n}\n¬† #wrapper \u003eheader {\n¬† ¬† display: block;\n¬† ¬† overflow: auto;\n¬† ¬† clear: both;\n¬† ¬† height: 48px;\n¬† ¬† font-family: Liberation, Georgia, serif;\n}\n¬† #wrapper \u003efooter {\n¬† ¬† display: block;\n¬† ¬† overflow: auto;\n¬† ¬† padding: 5px;\n¬† ¬† text-align: center;\n¬† ¬† clear: both;\n}\n¬† #wrapper #main {\n¬† ¬† position: relative;\n}\n¬† ¬† #wrapper #main \u003esection {\n¬† ¬† ¬† display: block;\n¬† ¬† ¬† overflow: auto;\n¬† ¬† ¬† width: 520px;\n¬† ¬† ¬† padding: 20px;\n}\n¬† ¬† #wrapper #main \u003easide {\n¬† ¬† ¬† display: block;\n¬† ¬† ¬† overflow: auto;\n¬† ¬† ¬† float: right;\n¬† ¬† ¬† width: 300px;\n¬† ¬† ¬† padding: 20px;\n}\n```\n\nLe SASS offre **encore plus de possibilit√©s**, notamment l‚Äôinterpolation, les conditions, les boucles, ‚Ä¶  \nVous trouverez plus d‚Äôinformations sur la _documentation SASS_.\n\n## Utilisation\n\n### Pr√©-requis\n\n**Note**: Ce qui suit ne s‚Äôapplique pas pour le plugin sass du framework Play! .\n\nPour utiliser SASS, sous linux, installez les packets **ruby** et **rubygems** puis installez **haml** avec la commande :\n\n```bash\ngem install haml\n```\n\n### Avec le framework java Play!\n\nGr√¢ce au module sass de play framework, le SASS est **compil√© √† la vol√©e** au moment du chargement d‚Äôune page (en mode d√©veloppement) ou au chargement de l‚Äôapplication (en mode production).\n\n#### Installation\n\nDepuis play 1.1, il suffit de lancer la commande\n\n```bash\nplay install sass\n```\n\nEnsuite il faut activer le module dans la configuration de l‚Äôapplication (fichier _conf/application.conf_).\n\n### Avec le framework Ruby on Rails\n\n#### Installation\n\nPour activer le plugin SASS sur une application Rails, lancez :\n\n```bash\nhaml --rails path/to/rails/app\n```\n\n### Autrement\n\nVous pouvez toujours utiliser SASS en compilant vos fichier sass en css √† chaque modification (voir _Commandes pratiques_).  \nVous inclurez ensuite le fichier css compil√© dans votre html.\n\n## Convertir ses anciens CSS en SASS\n\nSi vous ne voulez pas repartir de z√©ro dans le design d‚Äôun projet, vous pouvez tout √† fait repartir avec les anciens CSS en les exportant en SASS.\n\n## Commandes pratiques\n\n- Pour convertir vos fichier CSS en SASS il vous suffit d‚Äôutiliser : `css2sass`\n\n- Pour compiler vos fichier SASS en CSS, utilisez: `sass`\n\n## Liens\n\n- [Site du langage SASS][1]\n- [Site du framework Play!][2]\n\n### Aller plus loin\n\n- [Compass : framework SASS][3]\n- [Documentation SASS][4]\n","data":{"title":"SASS : l'√©volution du CSS pour Play, Rails ou autres","author":"Gaetan","layout":"post","permalink":"/2010/03/sass-levolution-du-css/","tags":["css","sass"]}},{"id":"2010-02-20-the-same-game-in-html5-canvas","year":"2010","month":"02","day":"20","slug":"the-same-game-in-html5-canvas","content":"\n [2]: http://gre.github.io/same-game\n [3]: http://github.com/gre/same-game\n\nI've made a simple canvas game in **HTML5 Canvas**.\n\n**Canvas is a really great API** which allows to do awesome things combined with **javascript** language. It‚Äôs a **standard alternative to Flash** and for that kind of usage, I think it can fairly compete Flash.\n\n![screenshot](/images/2010/same_game_screenshot.png)\n\nYou can test the game in a **recent browser** *(canvas-compatible like firefox, chrome, ‚Ä¶)* **following [this link][2]**.\n\nIf you want to **hack the code**, see the javascript [here][3].\n\n\nThe game was simple to code, except maybe the animation managment which require some **optimization perspective**. The code source is still not perfect, I may improve soon.\n\n## Video demonstration\n\n\u003ciframe src=\"http://player.vimeo.com/video/9606570\" width=\"500\" height=\"375\" frameborder=\"0\" webkitAllowFullScreen mozallowfullscreen allowFullScreen\u003e\u003c/iframe\u003e\n\n*Released with the GNU General Public Licence.*\n\n","data":{"title":"The same game in HTML5 canvas","thumbnail":"/images/2010/same_game_screenshot.png","author":"Gaetan","layout":"post","permalink":"/2010/02/the-same-game-in-html5-canvas/","tags":["gamedev","canvas","javascript"]}},{"id":"2010-02-03-tutoriel-canvas-realiser-une-banniere-animee-en-quelques-lignes-de-code","year":"2010","month":"02","day":"03","slug":"tutoriel-canvas-realiser-une-banniere-animee-en-quelques-lignes-de-code","content":"\n [1]: /2011/03/html-canvas-pour-les-neophytes\n [2]: /demo/animate-banner/canvas-cartesian.png\n [3]: /demo/animate-banner/step1.html\n [4]: /demo/animate-banner/bezier-schema.png\n [5]: /demo/animate-banner/step2.html\n [6]: /demo/animate-banner/bezier-exemple.png\n [7]: http://paulirish.com/2011/requestanimationframe-for-smart-animating/\n [8]: /demo/animate-banner/step3.html\n [9]: /demo/animate-banner/step4.html\n [10]: /demo/animate-banner/step5.html\n [11]: /demo/animate-banner/step6.html\n [12]: /demo/animate-banner/final.html\n\n![](/images/2010/animated_banner.png)\n\n**Pr√©-requis conseill√©: [Voir la vid√©o : HTML Canvas pour les n√©ophytes][1]**\n\nLe **HTML 5** int√®gre de nouvelles technologies comme le **canvas**, une v√©ritable API graphique destin√©e √† remplacer le flash dans les ann√©es √† venir.\n\nCe tutoriel vise √† pr√©senter **canvas** comme une **librairie tr√®s simple d‚Äôutilisation et haut niveau** gr√¢ce au langage **javascript**.\n\nIl vous apprendra √† r√©aliser une animation similaire √† la banni√®re \u003cdel\u003eactuelle\u003c/del\u003e (ancienne maintenant) de mon blog en **quelques lignes de code**.\n\n\nIl est volontairement ordonnanc√© de mani√®re didactique, si vous maitriser les concepts, n‚Äôh√©sitez pas √† avancer.\n\n\u003c!--more--\u003e\n\n## Code de base (skeleton template)\n\nNous allons travailler avec ce code **html** de base :\n\n```html\n\u003c!DOCTYPE html\u003e\n\u003chtml\u003e\n  \u003chead\u003e\u003ctitle\u003e\u003c/title\u003e\u003c/head\u003e\n  \u003cbody\u003e\n    \u003ccanvas id=\"tuto\" width=\"500\" height=\"100\" style=\"border: 1px solid;\"\u003e\u003c/canvas\u003e\n    \u003cscript language=\"javascript\"\u003e\n      var canvas = document.getElementById('tuto');\n      var ctx = canvas.getContext('2d');\n\n      // Le code javascript ira ici\n\n    \u003c/script\u003e\n  \u003c/body\u003e\n\u003c/html\u003e\n```\n\nCe code nous servira dans tous le reste du tutoriel. \n\n### Deux choses notables :\n\n* Nous avons cr√©√© un √©l√©ment **canvas** et indiqu√© ses dimensions **(500√ó100)**. Pour mieux pouvoir rep√©rer ses dimensions, nous lui avons ajout√© une bordure.\n* En **javascript**, nous avons r√©cup√©r√© l‚Äô√©l√©ment DOM puis le contexte 2d qui nous servira pas la suite.\n\n## Quelques notions de base du canvas\n\n### La surface du canvas\n\nLe **canvas** occupe une surface dont la dimension est d√©finie par les param√®tres *width* et *height*.\n\nPour ceux qui ne serait pas familier avec les **biblioth√®ques graphiques**, cette surface peut √™tre vu comme un quadrillage de pixel sur **deux dimensions** : **x** variant de *0 √† width* et **y** variant de *0 √† height*.\n\nLe point d‚Äôorigine de ce rep√®re orthonorm√© est situ√© **dans le coin haut gauche du canvas**:\n\n![][2]\n\n### Le concept de contexte\n\nLe contexte 2d r√©cup√©r√© dans la variable **ctx** est en fait l‚Äô**interface de programmation** (API) de la biblioth√®que graphique **canvas**.\n\nC‚Äôest en quelque sorte l‚Äô**interm√©diaire entre le programmeur et la biblioth√®que graphique**.\n\nAinsi par exemple, si l‚Äôon veux dessiner un rectangle de dimension **20√ó10** √† la position **(5,6)**, il suffit simplement d‚Äô√©crire: \n\n```javascript\nctx.fillRect(5,6,20,10)\n```\n\n### Les param√®tres globaux du canvas\n\nPour dessiner, toute librairie graphique a besoin de connaitre un certain nombre de param√®tres tels que la *couleur du trait, la taille de la brosse, etc*.\n\nPlut√¥t que de devoir passer en param√®tre ces informations aux fonctions du contexte, **canvas** met directement √† disposition ses param√®tres afin de pouvoir les modifier facilement.\n\nAinsi, nous pourrons d√©finir la couleur de remplissage dans **ctx.fillStyle** et la couleur de trait dans **ctx.strokeStyle**.\n\n## Commen√ßons √† coder\n\nAvant d‚Äôattaquer l‚Äôanimation, nous allons commencer √† manier les **outils de tracage**.\n\n### Chemins et traits simples\n\nNous allons commencer par tracer un trait simple qui va traverser tous le canvas.\n\nEssayez le code suivant:\n\n```javascript\nctx.beginPath(); ¬† ¬† ¬† ¬† ¬† ¬† ¬†// commence √† tracer un chemin  \nctx.moveTo(, 20); ¬† ¬† ¬† ¬† ¬† ¬† // d√©fini le premier point de tracage √† la position (0, 20)  \nctx.lineTo(canvas.width, 30); // Tracer une ligne jusqu'√† la position (canvas.width, 30). canvas.width d√©signe la largeur du canvas (500 dans notre exemple).  \nctx.stroke(); ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† // Indique au canvas de dessiner le chemin trac√© depuis le beginPath\n```\n\n[Voir le r√©sultat][3]\n\n### Chemins et courbes de b√©zier\n\n#### La notion de courbe de b√©zier\n\nUne courbe de b√©zier est d√©finie par 4 points :\n\n*   Deux points d√©signant le d√©but et la fin du trait.\n*   Deux points appel√©s **poign√©es** permettant de contr√¥ler la courbe.\n\n![][4]\n\n#### Application en canvas\n\nEssayez le code suivant :\n\n```javascript\nctx.beginPath();  \nctx.moveTo(, 20);  \nctx.bezierCurveTo(canvas.width/3, canvas.height, 2*canvas.width/3, , canvas.width, 20);  \nctx.stroke();\n```\n\n[Voir le r√©sultat][5]\n\n**De la m√™me fa√ßon, la proc√©dure consiste √†:**\n\n* commencer un chemin,\n* se placer √† une certaine position,\n* effectuer un tra√ßage (courbe de b√©zier),\n* terminer le tra√ßage (**ctx.stroke()**).\n\nInt√©ressons nous plus particuli√®rement au tra√ßage de la courbe de b√©zier avec l‚Äôappel de **ctx.bezierCurveTo**.\n\n##### bezierCurveTo(cp1x, cp1y, cp2x, cp2y, x, y)\n\nCette fonction prends en argument les coordonn√©es du premier point de contr√¥le **(cp1x, cp1y)**, du deuxi√®me point de contr√¥le **(cp2x, cp2y)** et du point final **(x, y)**. A noter que le point de d√©but est le point sur lequel on s‚Äôest plac√© au moyen de **moveTo**.\n\n**Dans notre exemple, les deux points de contr√¥le sont plac√©s ainsi :**\n\n![][6]\n\n### Ajout de l‚Äôanimation\n\nPour animer notre courbe de b√©zier, nous allons faire varier les 4 points de notre courbe en fonction du temps.\n\nEn javascript, il y a deux moyen d‚Äôeffectuer une animation :\n\n* \u003cdel\u003eAu moyen de **setInterval** permettant d‚Äôappeler une fonction par interval de temps r√©gulier.\u003c/del\u003e\n* \u003cdel\u003eAu moyen de **setTimeout** permettant d‚Äôappeler une fonction apr√®s un temps donn√©.\u003c/del\u003e\n* **[UPDATE 2011]** Il est maintenant recommand√© d‚Äôutiliser **requestAnimationFrame** qui est l‚Äô√©quivalent de setTimeout mais destin√© √† l‚Äôanimation donc plus performant. [Plus d‚Äôinformations i√ßi (en)][7]\n\nLa premi√®re approche est parfaite pour une animation **‚Äústatique‚Äù, avec peu d‚Äôint√©raction**.\n\nLa seconde approche est int√©ressante lorsque l‚Äôanimation **doit √™tre contr√¥l√©e** (int√©raction). En effet, cette approche consiste √† appeler setTimeout √† chaque cycle d‚Äôanimation.\n\nNous choisirons d‚Äôutiliser **setInterval**, plus simple et plus adapt√©e √† notre tutoriel.\n\n#### Approche lin√©aire\n\nCommen√ßons simplement par une **√©volution lin√©aire des points**.\n\nEssayez le code suivant :\n\n```javascript\nvar i = 0; // variable fonction du temps\nvar cycle = function() {\n  ctx.clearRect(0,0,canvas.width,canvas.height); // clean the canvas\n  var y = Math.abs(canvas.height-i%(2*canvas.height)); // y √©volue par rebond entre 0 et canvas.height au cours du temps (lin√©arit√©)\n  ctx.beginPath();\n  ctx.moveTo(0, y);\n  ctx.bezierCurveTo(canvas.width/3, canvas.height/2, 2*canvas.width/3, canvas.height/2, canvas.width, y);\n  ctx.stroke();\n  i++;\n};\nsetInterval(cycle, 30); // lance le cycle chaque 30 millisecondes\n```\n\n[Voir le r√©sultat][8]\n\nPour l‚Äôinstant les poign√©es sont fixes et l‚Äô√©volution lin√©aire de **y** donne un effet de rebond peu int√©r√©ssant.\n\nC‚Äôest pour cela que nous abandonnons l‚Äôid√©e d‚Äôune √©volution lin√©aire des positions au cours du temps pour l‚Äôapproche sinuso√Ødale.\n\n#### Approche sinuso√Ødale\n\nComme nous l‚Äôavons vu, l‚Äô√©volution lin√©aire n‚Äôest pas adapt√©e pour ce genre d‚Äôanimation (effet de rebond). Il faudrait rendre l‚Äôanimation plus fluide.\n\nPour cela, nous allons utiliser **une √©volution sinuso√Ødale des positions au cours du temps**.\n\nEssayez le code suivant :\n\n```javascript\nvar i = 0; // variable fonction du temps\nvar cycle = function() {\n  ctx.clearRect(0,0,canvas.width,canvas.height);\n  var offset = i/20;\n  var y = (Math.sin(offset)+1)*canvas.height/2; // y varie de 0 √† canvas.height\n  var cpy1 = (Math.cos(offset)+0.5)*canvas.height; // les poign√©es √©voluent √©galement de fa√ßon sinuso√Ødale\n  var cpy2 = canvas.height - cpy1;\n  ctx.beginPath();\n  ctx.moveTo(0, y);\n  ctx.bezierCurveTo(canvas.width/3, cpy1, 2*canvas.width/3, cpy2, canvas.width, y);\n  ctx.stroke();\n  i++;\n};\nsetInterval(cycle, 30);\n```\n\n[Voir le r√©sultat][9]\n\n### Peaufinage\n\n#### Am√©lioration du style du trait\n\nEssayez le code suivant :\n\n```javascript\nctx.strokeStyle = 'rgba(80,150,240,0.5)'; // couleur bleu avec opacit√© de 50%\nctx.lineWidth = 5; // √©paisseur de trait de 5 pixels\nvar i = 0;\nvar cycle = function() {\n  ctx.clearRect(0,0,canvas.width,canvas.height);\n  var offset = i/20;\n  var y = (Math.sin(offset)+1)*canvas.height/2;\n  var cpy1 = (Math.cos(offset)+0.5)*canvas.height;\n  var cpy2 = canvas.height - cpy1;\n  ctx.beginPath();\n  ctx.moveTo(0, y);\n  ctx.bezierCurveTo(canvas.width/3, cpy1, 2*canvas.width/3, cpy2, canvas.width, y);\n  ctx.stroke();\n  i++;\n};\nsetInterval(cycle, 30);\n```\n\n[Voir le r√©sultat][10]\n\n#### Ajout de plusieurs courbes\n\nPour avoir un effet plus accrochant, nous allons ajouter plusieurs courbes de b√©zier avec un d√©calage temporel entre elles.\n\nNous allons √©galement attribuer plusieurs styles aux diff√©rentes courbes.\n\n```javascript\nvar numberOfLines = 5;\nvar i = 0;\nvar cycle = function() {\n  ctx.clearRect(0,0,canvas.width,canvas.height);\n  for(var j=0; j\u003cnumberOfLines; ++j) {\n    var offset = (i+j*10)/20;\n    ctx.lineWidth = 1+2*(numberOfLines-j); // √©paisseur variable en fonction de la ligne\n    ctx.strokeStyle = 'rgba(80,150,240,'+(j/5+0.1)+')'; // opacit√© variable en fonction de la ligne\n    var y = (Math.sin(offset)+1)*canvas.height/2;\n    var cpy1 = (Math.cos(offset)+0.5)*canvas.height;\n    var cpy2 = canvas.height - cpy1;\n    ctx.beginPath();\n    ctx.moveTo(0, y);\n    ctx.bezierCurveTo(canvas.width/3, cpy1, 2*canvas.width/3, cpy2, canvas.width, y);\n    ctx.stroke();\n  }\n  i++;\n};\nsetInterval(cycle, 30);\n```\n\n[Voir le r√©sultat][11]\n\n### Aller plus loin\n\nIl est possible de continuer encore plus loin en ajoutant l‚Äô**√©volution de plusieurs param√®tres en fonction du temps**.\nPour conclure, voici la d√©monstration finale:\n\n```javascript\nvar i = 0;\nvar cycle = function() {\n  ctx.clearRect(0, 0, canvas.width, canvas.height);\n  for(var j=0; j\u003cnumberOfLines; ++j) {\n    ctx.lineWidth = 1+2*(numberOfLines-j);\n    ctx.strokeStyle = 'rgba(100,200,'+Math.floor(Math.abs(Math.cos(i/80)*256))+','+(j/5+0.1)+')';\n    var offset = (i+j*10*Math.abs(Math.cos(i/100)))/20;\n    var y = (Math.sin(offset)+1)*canvas.height/2;\n    var cpy1 = (Math.cos(offset)+0.5)*canvas.height;\n    var cpy2 = canvas.height - cpy1;\n    ctx.beginPath();\n    ctx.moveTo(0, y);\n    ctx.bezierCurveTo(canvas.width/3, cpy1, 2*canvas.width/3, cpy2, canvas.width, y);\n    ctx.stroke();\n  }\n  i++;\n};\nsetInterval(cycle, 30);\n```\n[Voir le r√©sultat][12]\n\nNous avons ajout√©:\n\n* L‚Äô√©volution de la **couleur** au cours du temps.\n* L‚Äô√©volution du **d√©calage entre les courbes** au cours du temps.\n","data":{"title":"Tutoriel Canvas : R√©aliser une banni√®re anim√©e en quelques lignes de code","thumbnail":"/images/2010/animated_banner.png","description":"Ce tutoriel vise √† pr√©senter canvas comme une librairie tr√®s simple d‚Äôutilisation et haut niveau gr√¢ce au langage javascript.","author":"Gaetan","layout":"post","permalink":"/2010/02/tutoriel-canvas-realiser-une-banniere-animee-en-quelques-lignes-de-code","tags":["animation","canvas","javascript","bezier"]}}]},"__N_SSG":true},"page":"/posts","query":{},"buildId":"1XWF0K4yTNe9SzMffEm6x","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>