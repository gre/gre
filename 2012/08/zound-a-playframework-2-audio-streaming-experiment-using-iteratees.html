<!DOCTYPE html><html><head><meta charSet="utf-8"/><link rel="icon" href="/favicon.ico"/><meta name="author" content="Gaëtan Renaudeau"/><meta name="description" content="Zound uses an audio generator (JSyn, an audio synthesizer), encode the output and stream it all using Play Iteratees to pipe everything in real-time."/><meta name="keywords" content="audio, iteratee, playframework, hackday"/><meta name="HandheldFriendly" content="True"/><meta name="MobileOptimized" content="320"/><meta name="viewport" content="width=device-width, initial-scale=1"/><meta name="twitter:card" content="summary"/><meta name="twitter:site" content="@greweb"/><meta name="twitter:title" content="Zound, a PlayFramework 2 audio streaming experiment using Iteratees"/><meta name="og:title" content="Zound, a PlayFramework 2 audio streaming experiment using Iteratees"/><meta name="twitter:description" content="Zound uses an audio generator (JSyn, an audio synthesizer), encode the output and stream it all using Play Iteratees to pipe everything in real-time."/><meta name="twitter:creator" content="@greweb"/><meta name="og:image" content="http://greweb.me//images/2012/07/ZOUND.png"/><meta name="twitter:image" content="http://greweb.me//images/2012/07/ZOUND.png"/><link rel="image_src" href="http://greweb.me//images/2012/07/ZOUND.png"/><title>@greweb - Zound, a PlayFramework 2 audio streaming experiment using Iteratees</title><link href="http://fonts.googleapis.com/css?family=Fredericka+the+Great|Arapey|Roboto:400,700,400italic" rel="stylesheet" type="text/css"/><link rel="stylesheet" href="https://unpkg.com/@highlightjs/cdn-assets@10.7.2/styles/default.min.css"/><script src="https://unpkg.com/@highlightjs/cdn-assets@10.7.2/highlight.min.js"></script><script src="https://unpkg.com/@highlightjs/cdn-assets@10.7.2/languages/javascript.min.js"></script><script src="https://unpkg.com/@highlightjs/cdn-assets@10.7.2/languages/cpp.min.js"></script><script src="https://unpkg.com/@highlightjs/cdn-assets@10.7.2/languages/glsl.min.js"></script><link rel="stylesheet" href="/style/main.css"/><meta name="next-head-count" content="25"/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-381dbb3c33243b4920e6.js"></script><script src="/_next/static/chunks/webpack-17597d20e291f72b2439.js" defer=""></script><script src="/_next/static/chunks/framework-bdc1b4e5e48979e16d36.js" defer=""></script><script src="/_next/static/chunks/main-4ac108dd57980e4159e9.js" defer=""></script><script src="/_next/static/chunks/pages/_app-c981e0e3ce59f13eb8d0.js" defer=""></script><script src="/_next/static/chunks/5988-738c1ea5f97353b6463e.js" defer=""></script><script src="/_next/static/chunks/2242-e809902ac2d7a524d02b.js" defer=""></script><script src="/_next/static/chunks/pages/%5Byear%5D/%5Bmonth%5D/%5Bslug%5D-18d367ef37e4735025ef.js" defer=""></script><script src="/_next/static/vIdAyYeXch4v51HIYB7Zz/_buildManifest.js" defer=""></script><script src="/_next/static/vIdAyYeXch4v51HIYB7Zz/_ssgManifest.js" defer=""></script><style id="__jsx-2519965637">.block.jsx-2519965637{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.block.jsx-2519965637 .right.jsx-2519965637{padding:10px;}.block.jsx-2519965637 .social.jsx-2519965637{margin-top:10px;}.block.jsx-2519965637 .social.jsx-2519965637 a.jsx-2519965637{padding:10px;}.block.jsx-2519965637 .social.jsx-2519965637 img.jsx-2519965637{height:20px;}</style><style id="__jsx-3621368397">.container.jsx-3621368397{min-height:100vh;padding:0 0.5rem;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><style id="__jsx-3469673304">html,body{padding:0;margin:0;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto, Oxygen,Ubuntu,Cantarell,Fira Sans,Droid Sans,Helvetica Neue, sans-serif;}*{box-sizing:border-box;}a{color:inherit;-webkit-text-decoration:none;text-decoration:none;}a:hover,a:active{-webkit-text-decoration:underline;text-decoration:underline;}</style></head><body><div id="__next"><div class="jsx-3621368397 container"><div id="container"><div id="main"><div id="content"><article><header><h1><a href="/">Zound, a PlayFramework 2 audio streaming experiment using Iteratees</a></h1><time class="date" dateTime="2012-08-01">2012-08-01</time><span class="tags"><a class="tag">audio</a><a class="tag">iteratee</a><a class="tag">playframework</a><a class="tag">hackday</a></span></header><div class="entry-content"><p><img src="/images/2012/07/ZOUND.png" alt="ZOUND"></p>
<p>Last Friday was HackDay #7 at <a href="http://zenexity.com">Zenexity</a>, and we decided to work on a real-time audio <a href="http://github.com/gre/zound">experiment</a> made with <a href="http://playframework.org">Play Framework</a>. The plan was to use an audio generator (<a href="http://www.softsynth.com/jsyn/">JSyn</a>, an audio synthesizer), encode the output and stream it all using Play Iteratees to pipe everything in real-time.</p>
<iframe width="640" height="360" src="http://www.youtube.com/embed/taDLKTcNHnQ?feature=player_embedded" frameborder="0" allowfullscreen></iframe>

<p><strong>First of all, let’s highlight some interesting part of the project, then get into some of the details.</strong></p>
<p>Thanks to <a href="https://twitter.com/Sadache">@Sadache</a> for his Iteratee expertise, we ended up with a simple line of code that does all of the hard work:</p>
<pre><code class="language-scala">val chunkedAudioStream = rawStream &amp;&gt; chunker &amp;&gt; audioEncoder
</code></pre>
<p>You can think of the <code>&amp;&gt;</code> operator as the UNIX pipe <code>|</code>. So we simply take the <code>rawStream</code>, chunk it with a <code>chunker</code> and encode it with an <code>audioEncoder</code>.</p>
<p>Now, <strong>rawStream</strong> is the raw stream of audio samples (numbers between -1 and 1) generated by the audio synthesizer. Next, the <strong>chunker</strong> buffers a data stream into chunk of bytes. For instance, if you send data stream at 1Kb/s to a 10Kb chunker, it will output one chunk of size 10Kb every 10 seconds. And finally, the <strong>audioEncoder</strong> takes <strong>audio samples</strong> and outputs encoded bytes implementing an audio format (like WAVE).</p>
<p>We can then make a broadcast of the stream:</p>
<pre><code class="language-scala">val (sharedChunkedAudioStream, _) =   
  Concurrent.broadcast(chunkedAudioStream)
</code></pre>
<p>And then the <strong>sharedChunkedAudioStream</strong> is now a shared stream for every consumer (clients). All that’s left to do is to stream it over HTTP:</p>
<pre><code class="language-scala">def stream = Action {  
  Ok.stream(audioHeader &gt;&gt;&gt; sharedChunkedAudioStream).  
     withHeaders( (&quot;Content-Type&quot;, audio.contentType) )  
}
</code></pre>
<p>The <code>&gt;&gt;&gt;</code> operator means “concatenation”, so here we’re concatenating the <strong>audio header</strong> (given by the format like WAVE) with the current <strong>chunked audio stream</strong>. We also send the right HTTP <strong>Content-Type</strong> header (like “audio/wav” for WAVE).</p>
<p>Another interesting part of the project is the <strong>multi-user web user interface: allowing users to interact with the sound synthesis</strong>.</p>
<p>Using <a href="https://twitter.com/mrspeaker">@mrspeaker</a>‘s audio synthesis expertise, we started creating a synthesizer generator – 3 oscillators, various wave shapes, frequency and volumes, and finally flowing through a high pass filter before entering our “rawStream” above.</p>
<p>Thanks to the Play framework goodness, <strong>this audio stream can be both consumed by the web page with an HTML audio tag, and with a stream player such as VLC!</strong> Ok, that’s the project – let’s have a closer look at some of the concepts…</p>
<!--more-->


<h2 id="what-is-sound">What is sound?</h2>
<blockquote>
<p>Sound is a mechanical wave that is an oscillation of pressure transmitted through a solid, liquid, or gas, composed of frequencies within the range of hearing and of a level sufficiently strong to be heard, or the sensation stimulated in organs of hearing by such vibrations. <em><strong>Wikipedia</strong></em></p>
</blockquote>
<p>We can represents the sound like any wave as a graphic of the amplitude (the oscillation pressure) as a function of time. Here you see it in Audacity:</p>
<p><img src="/images/2012/07/sound-audacity.png" alt="sound-audacity"></p>
<p>Electricity is used to pump these amplitudes to your speakers, over time.</p>
<h3 id="about-primitive-wave-sounds">About primitive wave sounds</h3>
<p>There are some patterns – some primitives waves – we can easily generate with computers (or before with analog oscillators). Those are well known by mathematicians and physicians: Sine wave, Triangle wave, Square wave,…</p>
<p><a href="http://en.wikipedia.org/wiki/File:Waveforms.svg"><img src="/images/2012/07/557px-Waveforms.svg_.png" alt=""></a></p>
<p>A sine wave produce a smooth tone, whereas Triangle and Square wave are more aggressive sounds. The shorter a wave period is, the lower the note you hear: it’s called the frequency.</p>
<h3 id="how-is-sound-represented-by-computer">How is sound represented by computer?</h3>
<p>Whereas analog oscillators generate sounds in an <em>almost</em>* continuous stream of electricity, computers are not able to generate continuous stream of data. This is why with computers the sound is divided in to discrete <strong>samples</strong>, usually <strong>44100 samples per second</strong> for standard CD quality audio. Each sample is a value (amplitude) for a given time position.</p>
<p><a href="http://en.wikipedia.org/wiki/File:Waveforms.svg">See Sampling (wikipedia)</a>.</p>
<p>If you zoom in Audacity, you can actually see each sample:<br><img src="/images/2012/07/audacity-zoom-ah.png" alt="audacity-zoom">
This is an “AH” timbre of my voice. A timbre is unique to everyone, it’s the pattern the sound wave take when you speak. <strong>The amplitude of an audio sample is usually represented as a Real number between -1.0 and 1.0</strong>.</p>
<p>** electricity is not strictly continuous, we have electrons out there!*</p>
<p>Ok, Let’s go back to our experiment now!</p>
<h2 id="the-experiment">The experiment</h2>
<p>Our experiment is using <a href="http://www.playframework.org/">Play Framework</a> and is written in <a href="scala-lang.org/">Scala language</a>. Specifically, our project takes advantage of Play framework’s powerful <strong>Iteratee</strong>s.</p>
<blockquote>
<p>Take the expressivity of UNIX pipes, bring the power of Scala, mix it with Play Framework and you got a powerful framework for handling real-time and web streaming.</p>
</blockquote>
<p>The iteratee (and related constructs) can take a bit of getting used to. I recommend checking out <a href="http://sadache.tumblr.com/post/26784721867/is-socket-push-bytes-all-what-you-need-to-program">this article on Iteratees in Play</a> and/or <a href="http://www.infoq.com/presentations/Play-I-ll-See-Your-Async-and-Raise-You-Reactive">this presentation</a> if you are interested in learning more about Play2 and reactive programming with Iteratees. And if you just want to see how it work – you can read the source code at <a href="https://github.com/playframework/Play20/tree/master/framework/src/play/src/main/scala/play/api/libs/iteratee">Play20 Github source code</a>.</p>
<h3 id="generating-the-audio-stream">Generating the audio stream</h3>
<pre><code class="language-scala">val (rawStream, channel) = Concurrent.broadcast[Array[Double]]  
val zound = new ZoundGenerator(channel).start()
</code></pre>
<p>We create an <code>Array[Double]</code> broadcast which return two values: the <strong>rawStream</strong> will be used to read the generated data, and the <strong>channel</strong> used by the generator to push generated audio samples. We give this channel to the <strong>ZoundGenerator</strong>. The <code>.start()</code> then starts the audio generation. All of the generation is done using the JSyn library.</p>
<p>Here’s a snippet from the <strong>ZoundGenerator</strong> class showing the connection between <strong>JSyn</strong> and <strong>Channel</strong>:</p>
<pre><code class="language-scala">class ZoundGenerator(output: Channel[Array[Double]]) {  
  val out = new MonoStreamWriter()  
  
  val synth = {  
    val synth = JSyn.createSynthesizer()  
    synth.add(out)  
    out.setOutputStream(new AudioOutputStream(){  
      def close() {}  
      def write(value: Double) {  
        output.push(Array(value))  
      }  
      def write(buffer: Array[Double]) {  
        write(buffer, , buffer.length)  
      }  
      def write(buffer: Array[Double], start: Int, count: Int) {  
        output.push(buffer.slice(start, start count))  
      }  
    })  
    synth  
  }  
  // ...
</code></pre>
<p>We have to implement the methods of AudioOutputStream – but it’s just a matter of pushing each audio sample to the channel. It’s that simple!</p>
<h3 id="encoding-the-raw-audio-stream">Encoding the raw audio stream</h3>
<p>For now, we have only implemented the <a href="http://en.wikipedia.org/wiki/WAV">WAVE format</a>. Basically, WAVE has 2 parts; the WAVE header which describes important information (like the framerate and the bits per sample), and the data.<br>The data is encoded in a simple manner I won’t describe here but you can look to the encoder I made here: </p>
<p>Now more interesting, let’s wrap it with Play Iteratees:</p>
<pre><code class="language-scala">val audio = MonoWaveEncoder() // instanciate the WAV encoder  
val audioHeader = Enumerator(audio.header)  
val audioEncoder = Enumeratee.map[Array[Double]](audio.encodeData)
</code></pre>
<p><em><strong>N. B.</strong></em>: <em>Remember that Scala is a typed language but where the type declaration is optional because the compiler can infer the type.</em></p>
<p><strong>audioHeader</strong> is an <code>Enumerator</code> which means it can <em>produce</em> data, and here the data is the audio header. More precisely it’s an <code>Enumerator[Array[Byte]]</code> because audio.header is an <code>Array[Byte]</code>. Note that contained data is not “consumed” like it would be for an <code>InputStream</code>. Each time you use this enumerator, it gives you its entire content.</p>
<p><strong>audioEncoder</strong> is an <code>Enumeratee[Array[Double], Array[Byte]]</code>. It takes an <code>Array[Double]</code> from input and returns an <code>Array[Byte]</code> as output. The input is a raw array of audio samples (double numbers between -1.0 and 1.0). The output is the encoded array of bytes.</p>
<p>More formally, an <code>Enumeratee[A, B]</code> is an <em>adapter</em> which maps some data of type A to new data of type B. You can implement the way the data is transformed with the map function. Here we just give it the <code>audio.encodeData</code> function.</p>
<h3 id="streaming-it">Streaming it</h3>
<p>We can basically stream the audio stream with Play2 like so:</p>
<pre><code class="language-scala">def stream = Action {  
  val audioStream = rawStream &amp;&gt; audioEncoder  
  Ok.stream(audioHeader &gt;&gt;&gt; audioStream).  
     withHeaders( (CONTENT_TYPE, audio.contentType) )  
}
</code></pre>
<p>The <code>rawStream &amp;&gt; audioEncoder</code> takes the raw stream and <strong>pipes</strong> it into the encoder which results in the encoded audio stream. <code>audioHeader &gt;&gt;&gt; audioStream</code> will <strong>concatenate</strong> <code>audioHeader</code> with <code>audioStream</code>. Hence, the first thing the server will do is start sending the audio header to the client <strong>and then</strong> stream the audio in real-time.</p>
<p><strong>A client can connect at any time and will hear current stream</strong>, so it should simultaneously hear the same thing as any other client (with some delay depending on the client buffer). If the generator stops emitting audio samples, the http client will stop receiving audio data – but it will still be waiting for the server, so the audio play will pause until the server re-sends new audio samples. <strong>That is pretty cool!</strong> – because of the way iteratees work, the stream doesn’t just die when all of the input is consumed.</p>
<h4 id="a-chunker-to-reduce-http-packet-numbers">A chunker to reduce HTTP packet numbers</h4>
<p>Up to now we’ve been streaming the audio in <strong>very small chunks</strong> because by default JSyn writes out arrays of just 8 audio samples and the <code>.stream()</code> function consumes all data as it comes. This means <em>a lot</em> of HTTP chunks per second are sent – which is less efficient and take more bandwidth.</p>
<p>In order to fix this, we need to use a <strong>buffer on the server side</strong>. In other words, instead of sending audio samples as they come we need to <strong>group audio samples</strong>. We have currently grouped audio samples in arrays of 5000 which is quite reasonable (it’s about 10 chunks per second using 44100 samples/s). We can easily change this later. This logic is implemented in an <code>Enumeratee</code> we called “chunker”. In that sense, it is reusable and modular:</p>
<pre><code class="language-scala">val chunker = Enumeratee.grouped(  
  Traversable.take[Array[Double]](5000) &amp;&gt;&gt; Iteratee.consume()  
)
</code></pre>
<p>And now, we can easily plug it in like this:</p>
<pre><code class="language-scala">def stream = Action {  
  val chunkedAudioStream = rawStream &amp;&gt; chunker &amp;&gt; audioEncoder  
  Ok.stream(audioHeader &gt;&gt;&gt; chunkedAudioStream).  
     withHeaders( (CONTENT_TYPE, audio.contentType) )  
}
</code></pre>
<h3 id="broadcast">Broadcast</h3>
<p>Now, another improvement we made was to <strong>factorize this chunking and encoding part: avoiding having this computing tasks done for every stream consumer</strong>.</p>
<p>Basically, we move it out of the stream function:</p>
<pre><code class="language-scala">val chunkedAudioStream = rawStream &amp;&gt; chunker &amp;&gt; audioEncoder  
def stream = Action {  
  Ok.stream(audioHeader &gt;&gt;&gt; chunkedAudioStream).  
     withHeaders( (CONTENT_TYPE, audio.contentType) )  
}
</code></pre>
<p>But to allow broadcasting, we have to use a broadcast:</p>
<pre><code class="language-scala">val chunkedAudioStream = rawStream &amp;&gt; chunker &amp;&gt; audioEncoder  
val (sharedChunkedAudioStream, _) =  =   
  Concurrent.broadcast(chunkedAudioStream)  
def stream = Action {  
  Ok.stream(audioHeader &gt;&gt;&gt; sharedChunkedAudioStream).  
     withHeaders( (CONTENT_TYPE, audio.contentType) )  
}
</code></pre>
<p>Here we only care about the enumerator (the left argument in the Tuple2), we put the wildcard &quot;_&quot; to ignore the return value.</p>
<p><a href="http://en.wikipedia.org/wiki/Broadcasting_(networking)"><img src="/images/2012/07/320px-Broadcast.svg_.png" alt=""></a></p>
<p>Using a broadcast, generated audio samples pushed by the audio generator can be simultaneously spread to multiple consumers. This is perfect for our needs, multiple players can connect to this web radio!</p>
<h3 id="avoiding-the-server-load">Avoiding the server load</h3>
<p>The last important fix we made was to <strong>avoid the server load</strong>:</p>
<pre><code class="language-scala">  def stream = Action {  
    Ok.stream(audioHeader &gt;&gt;&gt; sharedChunkedAudioStream   
      &amp;&gt; Concurrent.dropInputIfNotReady(50)).  
       withHeaders( (CONTENT_TYPE, audio.contentType) )  
  }
</code></pre>
<p>If a client is opening the stream connection but doesn’t consume enough or doesn’t consume it at all (download is paused), the server will fill in memory the chunks to send to the client and the server can reach an <em>out of memory</em> exception. To avoid that <strong>we have to drops chunks if the consumer is not ready</strong>. Then the client will just lose messages if it is not ready (in our case, we give them 50 milliseconds).</p>
<p>And this is what <code>Concurrent.dropInputIfNotReady(50)</code> is actually doing – with yet another <strong>Enumeratee</strong>! <strong>Dropping old chunks is really what we want in an audio streaming application</strong>: We want the consumer to subscribe to the current audio stream and not to continue from where they stopped.</p>
<h3 id="client-consumers">Client consumers</h3>
<h4 id="html5-audio-tag">HTML5 Audio tag</h4>
<p>In HTML5, we have the Audio tag – and we can just consume our stream like this:</p>
<pre><code class="language-html">&lt;audio src=&quot;/stream.wav&quot;&gt;&lt;/audio&gt;
</code></pre>
<p>Or if we want to make it auto loading:</p>
<pre><code class="language-html">&lt;audio src=&quot;/stream.wav&quot; preload autoplay controls&gt;&lt;/audio&gt;
</code></pre>
<p>It may be a bit “wrong” to use `` for streaming, but it works because we are using it as if the server was hosting a static audio file. The only disputable hack is to have to set the max ChunkSize in the WAVE header which is 2147483647 (it’s about 6 hours 45mn!), so the browser believes the audio is not finished.</p>
<p>The issue we are currently facing is this crazy latency (a few seconds) between user actions and the produced sound. This problem is due to the browser audio cache buffer: if we were able to minimize it we would have an almost real-time audio player.</p>
<h4 id="playing-it-with-vlc">Playing it with VLC</h4>
<p>This stream is spread through HTTP so we need a HTTP client to consume it. But a HTTP client doesn’t mean only browsers! We can also use VLC for this, as if it was a web radio! One advantage of using VLC is it suffers far less latency (presumably because the cache buffer is smaller than the audio tag).</p>
<p><img src="/images/2012/07/vlc.png" alt="vlc"></p>
<h2 id="making-the-real-time-control-ui">Making the real-time control UI</h2>
<p>Our experiment <strong>mixes different oscillators to generator one sound</strong>. The web user interface allows a user to control the parameters of those. Two knobs control the <strong>volume</strong> and the <strong>pitch</strong> (tuned to a dorian mode scale) and you can select the oscillator <strong>wave primitive</strong> (sine, sawtooth, square, noise). It’s not fancy at the moment – but JSYN offers a lot of features for expanding our simple demo.</p>
<p><img src="/images/2012/07/Capture-d%E2%80%99%C3%A9cran-2012-07-31-%C3%A0-16.42.15.png" alt=""></p>
<p>This interface is <strong>multi-users</strong>, so if you use it with other people, <strong>the interface will stay synchronized over multiple browsers</strong> (turn the knobs, change the wave primitive, …). All this is done with WebSockets, and on the server-side it’s using, again, <strong>Iteratees</strong>!</p>
<p>The workflow is simple: When someone does some action on the user interface, events are sent to the server. These events are interpreted by the <em>ZoundGenerator</em> resulting in updates to the audio synthesis configuration. These events are then broadcast to each client, and some Javascript handlers are called in order to keep the interface synchronized.</p>
<h2 id="source-code">Source code</h2>
<p><strong><a href="http://github.com/gre/zound">Fork me on Github</a></strong></p>
<h2 id="whats-next">What’s next?</h2>
<p>This was just a simple demo to show the power and flexibility of Play2′s Iteratee concept. Because of the modular nature, extending the demo is easy. For example, we could plug a new audio encoder such an OGG encoder. The code would be simple and we could even choose on a request-by-request basis which encoder to use:</p>
<pre><code class="language-scala">import Concurrent.broadcast  
val (chunkedWaveStream, _) =   
  broadcast(rawStream &amp;&gt; chunker &amp;&gt; waveEncoder)  
val (chunkedOggStream, _) =   
  broadcast(rawStream &amp;&gt; chunker &amp;&gt; oggEncoder)  
  
def stream(format: String) = Action {  
  val stream = format match {  
    case &quot;wav&quot; =&gt; waveHeader &gt;&gt;&gt; chunkedWaveStream  
    case &quot;ogg&quot; =&gt; oggHeader &gt;&gt;&gt; chunkedOffStream  
  }  
  Ok.stream(stream).  
     withHeaders( (CONTENT_TYPE, audio.contentType) )  
}
</code></pre>
<p><strong>Now it’s up to you!</strong></p>
<p>Hopefully you get a feel for the possibilities of stream processing and piping with Play. You can now reuse these concepts and make your own stuff: Maybe you don’t need to generate sounds on the fly, but instead you simply want to play a collection of audio files and stream them like radio? <strong>Well you can make a web radio engine now!</strong>. </p>
<p>But that’s just the beginning – I would love to see someone taking the concept, and running even further… Do you know that in Youtube, during the time you are uploading a video, Youtube is already re-encoding it and can start streaming it <em>before</em> the file has finished uploading? Hmm, that’s starting to sound almost simple…</p>
</div><footer><div class="jsx-2519965637 block"><img src="http://greweb.me/logo.svg" width="100" class="jsx-2519965637"/><div class="jsx-2519965637 right"><div class="jsx-2519965637 description">As a generative plotter artist, I use code to create art (creative coding), draw with fountain pens on robots (plotting), and explore the boundaries of abstract art using algorithms in pursuit of increasingly realistic imagery. I do not produce prints; instead, I create unique &#x27;plots&#x27; - physical works of art that are truly one-of-a-kind.</div><div class="jsx-2519965637 social"><a href="https://twitter.com/greweb" class="jsx-2519965637"><img alt="" src="/icons/twitter.svg" class="jsx-2519965637"/></a><a href="https://instagram.com/greweb" class="jsx-2519965637"><img alt="" src="/icons/instagram.svg" class="jsx-2519965637"/></a><a href="https://twitch.tv/greweb" class="jsx-2519965637"><img alt="" src="/icons/twitch.svg" class="jsx-2519965637"/></a><a href="https://github.com/gre" class="jsx-2519965637"><img alt="" src="/icons/github.svg" class="jsx-2519965637"/></a><a href="https://opensea.io/greweb?tab=created" class="jsx-2519965637"><img alt="" src="/icons/eth.svg" class="jsx-2519965637"/></a><a href="https://fxhash.xyz/u/greweb" class="jsx-2519965637"><img alt="" src="/icons/tz.svg" class="jsx-2519965637"/></a><a href="https://greweb.itch.io" class="jsx-2519965637"><img alt="" src="/icons/iconmonstr-gamepad-3.svg" class="jsx-2519965637"/></a></div></div></div></footer></article></div></div></div><script>hljs.highlightAll();</script></div></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"id":"2012-08-01-zound-a-playframework-2-audio-streaming-experiment-using-iteratees","year":"2012","month":"08","day":"01","slug":"zound-a-playframework-2-audio-streaming-experiment-using-iteratees","content":"\u003cp\u003e\u003cimg src=\"/images/2012/07/ZOUND.png\" alt=\"ZOUND\"\u003e\u003c/p\u003e\n\u003cp\u003eLast Friday was HackDay #7 at \u003ca href=\"http://zenexity.com\"\u003eZenexity\u003c/a\u003e, and we decided to work on a real-time audio \u003ca href=\"http://github.com/gre/zound\"\u003eexperiment\u003c/a\u003e made with \u003ca href=\"http://playframework.org\"\u003ePlay Framework\u003c/a\u003e. The plan was to use an audio generator (\u003ca href=\"http://www.softsynth.com/jsyn/\"\u003eJSyn\u003c/a\u003e, an audio synthesizer), encode the output and stream it all using Play Iteratees to pipe everything in real-time.\u003c/p\u003e\n\u003ciframe width=\"640\" height=\"360\" src=\"http://www.youtube.com/embed/taDLKTcNHnQ?feature=player_embedded\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\n\n\u003cp\u003e\u003cstrong\u003eFirst of all, let’s highlight some interesting part of the project, then get into some of the details.\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThanks to \u003ca href=\"https://twitter.com/Sadache\"\u003e@Sadache\u003c/a\u003e for his Iteratee expertise, we ended up with a simple line of code that does all of the hard work:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-scala\"\u003eval chunkedAudioStream = rawStream \u0026amp;\u0026gt; chunker \u0026amp;\u0026gt; audioEncoder\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou can think of the \u003ccode\u003e\u0026amp;\u0026gt;\u003c/code\u003e operator as the UNIX pipe \u003ccode\u003e|\u003c/code\u003e. So we simply take the \u003ccode\u003erawStream\u003c/code\u003e, chunk it with a \u003ccode\u003echunker\u003c/code\u003e and encode it with an \u003ccode\u003eaudioEncoder\u003c/code\u003e.\u003c/p\u003e\n\u003cp\u003eNow, \u003cstrong\u003erawStream\u003c/strong\u003e is the raw stream of audio samples (numbers between -1 and 1) generated by the audio synthesizer. Next, the \u003cstrong\u003echunker\u003c/strong\u003e buffers a data stream into chunk of bytes. For instance, if you send data stream at 1Kb/s to a 10Kb chunker, it will output one chunk of size 10Kb every 10 seconds. And finally, the \u003cstrong\u003eaudioEncoder\u003c/strong\u003e takes \u003cstrong\u003eaudio samples\u003c/strong\u003e and outputs encoded bytes implementing an audio format (like WAVE).\u003c/p\u003e\n\u003cp\u003eWe can then make a broadcast of the stream:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-scala\"\u003eval (sharedChunkedAudioStream, _) =   \n  Concurrent.broadcast(chunkedAudioStream)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd then the \u003cstrong\u003esharedChunkedAudioStream\u003c/strong\u003e is now a shared stream for every consumer (clients). All that’s left to do is to stream it over HTTP:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-scala\"\u003edef stream = Action {  \n  Ok.stream(audioHeader \u0026gt;\u0026gt;\u0026gt; sharedChunkedAudioStream).  \n     withHeaders( (\u0026quot;Content-Type\u0026quot;, audio.contentType) )  \n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003e\u0026gt;\u0026gt;\u0026gt;\u003c/code\u003e operator means “concatenation”, so here we’re concatenating the \u003cstrong\u003eaudio header\u003c/strong\u003e (given by the format like WAVE) with the current \u003cstrong\u003echunked audio stream\u003c/strong\u003e. We also send the right HTTP \u003cstrong\u003eContent-Type\u003c/strong\u003e header (like “audio/wav” for WAVE).\u003c/p\u003e\n\u003cp\u003eAnother interesting part of the project is the \u003cstrong\u003emulti-user web user interface: allowing users to interact with the sound synthesis\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eUsing \u003ca href=\"https://twitter.com/mrspeaker\"\u003e@mrspeaker\u003c/a\u003e‘s audio synthesis expertise, we started creating a synthesizer generator – 3 oscillators, various wave shapes, frequency and volumes, and finally flowing through a high pass filter before entering our “rawStream” above.\u003c/p\u003e\n\u003cp\u003eThanks to the Play framework goodness, \u003cstrong\u003ethis audio stream can be both consumed by the web page with an HTML audio tag, and with a stream player such as VLC!\u003c/strong\u003e Ok, that’s the project – let’s have a closer look at some of the concepts…\u003c/p\u003e\n\u003c!--more--\u003e\n\n\n\u003ch2 id=\"what-is-sound\"\u003eWhat is sound?\u003c/h2\u003e\n\u003cblockquote\u003e\n\u003cp\u003eSound is a mechanical wave that is an oscillation of pressure transmitted through a solid, liquid, or gas, composed of frequencies within the range of hearing and of a level sufficiently strong to be heard, or the sensation stimulated in organs of hearing by such vibrations. \u003cem\u003e\u003cstrong\u003eWikipedia\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eWe can represents the sound like any wave as a graphic of the amplitude (the oscillation pressure) as a function of time. Here you see it in Audacity:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/2012/07/sound-audacity.png\" alt=\"sound-audacity\"\u003e\u003c/p\u003e\n\u003cp\u003eElectricity is used to pump these amplitudes to your speakers, over time.\u003c/p\u003e\n\u003ch3 id=\"about-primitive-wave-sounds\"\u003eAbout primitive wave sounds\u003c/h3\u003e\n\u003cp\u003eThere are some patterns – some primitives waves – we can easily generate with computers (or before with analog oscillators). Those are well known by mathematicians and physicians: Sine wave, Triangle wave, Square wave,…\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://en.wikipedia.org/wiki/File:Waveforms.svg\"\u003e\u003cimg src=\"/images/2012/07/557px-Waveforms.svg_.png\" alt=\"\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eA sine wave produce a smooth tone, whereas Triangle and Square wave are more aggressive sounds. The shorter a wave period is, the lower the note you hear: it’s called the frequency.\u003c/p\u003e\n\u003ch3 id=\"how-is-sound-represented-by-computer\"\u003eHow is sound represented by computer?\u003c/h3\u003e\n\u003cp\u003eWhereas analog oscillators generate sounds in an \u003cem\u003ealmost\u003c/em\u003e* continuous stream of electricity, computers are not able to generate continuous stream of data. This is why with computers the sound is divided in to discrete \u003cstrong\u003esamples\u003c/strong\u003e, usually \u003cstrong\u003e44100 samples per second\u003c/strong\u003e for standard CD quality audio. Each sample is a value (amplitude) for a given time position.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://en.wikipedia.org/wiki/File:Waveforms.svg\"\u003eSee Sampling (wikipedia)\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eIf you zoom in Audacity, you can actually see each sample:\u003cbr\u003e\u003cimg src=\"/images/2012/07/audacity-zoom-ah.png\" alt=\"audacity-zoom\"\u003e\nThis is an “AH” timbre of my voice. A timbre is unique to everyone, it’s the pattern the sound wave take when you speak. \u003cstrong\u003eThe amplitude of an audio sample is usually represented as a Real number between -1.0 and 1.0\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003e** electricity is not strictly continuous, we have electrons out there!*\u003c/p\u003e\n\u003cp\u003eOk, Let’s go back to our experiment now!\u003c/p\u003e\n\u003ch2 id=\"the-experiment\"\u003eThe experiment\u003c/h2\u003e\n\u003cp\u003eOur experiment is using \u003ca href=\"http://www.playframework.org/\"\u003ePlay Framework\u003c/a\u003e and is written in \u003ca href=\"scala-lang.org/\"\u003eScala language\u003c/a\u003e. Specifically, our project takes advantage of Play framework’s powerful \u003cstrong\u003eIteratee\u003c/strong\u003es.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eTake the expressivity of UNIX pipes, bring the power of Scala, mix it with Play Framework and you got a powerful framework for handling real-time and web streaming.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eThe iteratee (and related constructs) can take a bit of getting used to. I recommend checking out \u003ca href=\"http://sadache.tumblr.com/post/26784721867/is-socket-push-bytes-all-what-you-need-to-program\"\u003ethis article on Iteratees in Play\u003c/a\u003e and/or \u003ca href=\"http://www.infoq.com/presentations/Play-I-ll-See-Your-Async-and-Raise-You-Reactive\"\u003ethis presentation\u003c/a\u003e if you are interested in learning more about Play2 and reactive programming with Iteratees. And if you just want to see how it work – you can read the source code at \u003ca href=\"https://github.com/playframework/Play20/tree/master/framework/src/play/src/main/scala/play/api/libs/iteratee\"\u003ePlay20 Github source code\u003c/a\u003e.\u003c/p\u003e\n\u003ch3 id=\"generating-the-audio-stream\"\u003eGenerating the audio stream\u003c/h3\u003e\n\u003cpre\u003e\u003ccode class=\"language-scala\"\u003eval (rawStream, channel) = Concurrent.broadcast[Array[Double]]  \nval zound = new ZoundGenerator(channel).start()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe create an \u003ccode\u003eArray[Double]\u003c/code\u003e broadcast which return two values: the \u003cstrong\u003erawStream\u003c/strong\u003e will be used to read the generated data, and the \u003cstrong\u003echannel\u003c/strong\u003e used by the generator to push generated audio samples. We give this channel to the \u003cstrong\u003eZoundGenerator\u003c/strong\u003e. The \u003ccode\u003e.start()\u003c/code\u003e then starts the audio generation. All of the generation is done using the JSyn library.\u003c/p\u003e\n\u003cp\u003eHere’s a snippet from the \u003cstrong\u003eZoundGenerator\u003c/strong\u003e class showing the connection between \u003cstrong\u003eJSyn\u003c/strong\u003e and \u003cstrong\u003eChannel\u003c/strong\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-scala\"\u003eclass ZoundGenerator(output: Channel[Array[Double]]) {  \n  val out = new MonoStreamWriter()  \n  \n  val synth = {  \n    val synth = JSyn.createSynthesizer()  \n    synth.add(out)  \n    out.setOutputStream(new AudioOutputStream(){  \n      def close() {}  \n      def write(value: Double) {  \n        output.push(Array(value))  \n      }  \n      def write(buffer: Array[Double]) {  \n        write(buffer, , buffer.length)  \n      }  \n      def write(buffer: Array[Double], start: Int, count: Int) {  \n        output.push(buffer.slice(start, start count))  \n      }  \n    })  \n    synth  \n  }  \n  // ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWe have to implement the methods of AudioOutputStream – but it’s just a matter of pushing each audio sample to the channel. It’s that simple!\u003c/p\u003e\n\u003ch3 id=\"encoding-the-raw-audio-stream\"\u003eEncoding the raw audio stream\u003c/h3\u003e\n\u003cp\u003eFor now, we have only implemented the \u003ca href=\"http://en.wikipedia.org/wiki/WAV\"\u003eWAVE format\u003c/a\u003e. Basically, WAVE has 2 parts; the WAVE header which describes important information (like the framerate and the bits per sample), and the data.\u003cbr\u003eThe data is encoded in a simple manner I won’t describe here but you can look to the encoder I made here: \u003c/p\u003e\n\u003cp\u003eNow more interesting, let’s wrap it with Play Iteratees:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-scala\"\u003eval audio = MonoWaveEncoder() // instanciate the WAV encoder  \nval audioHeader = Enumerator(audio.header)  \nval audioEncoder = Enumeratee.map[Array[Double]](audio.encodeData)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eN. B.\u003c/strong\u003e\u003c/em\u003e: \u003cem\u003eRemember that Scala is a typed language but where the type declaration is optional because the compiler can infer the type.\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eaudioHeader\u003c/strong\u003e is an \u003ccode\u003eEnumerator\u003c/code\u003e which means it can \u003cem\u003eproduce\u003c/em\u003e data, and here the data is the audio header. More precisely it’s an \u003ccode\u003eEnumerator[Array[Byte]]\u003c/code\u003e because audio.header is an \u003ccode\u003eArray[Byte]\u003c/code\u003e. Note that contained data is not “consumed” like it would be for an \u003ccode\u003eInputStream\u003c/code\u003e. Each time you use this enumerator, it gives you its entire content.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eaudioEncoder\u003c/strong\u003e is an \u003ccode\u003eEnumeratee[Array[Double], Array[Byte]]\u003c/code\u003e. It takes an \u003ccode\u003eArray[Double]\u003c/code\u003e from input and returns an \u003ccode\u003eArray[Byte]\u003c/code\u003e as output. The input is a raw array of audio samples (double numbers between -1.0 and 1.0). The output is the encoded array of bytes.\u003c/p\u003e\n\u003cp\u003eMore formally, an \u003ccode\u003eEnumeratee[A, B]\u003c/code\u003e is an \u003cem\u003eadapter\u003c/em\u003e which maps some data of type A to new data of type B. You can implement the way the data is transformed with the map function. Here we just give it the \u003ccode\u003eaudio.encodeData\u003c/code\u003e function.\u003c/p\u003e\n\u003ch3 id=\"streaming-it\"\u003eStreaming it\u003c/h3\u003e\n\u003cp\u003eWe can basically stream the audio stream with Play2 like so:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-scala\"\u003edef stream = Action {  \n  val audioStream = rawStream \u0026amp;\u0026gt; audioEncoder  \n  Ok.stream(audioHeader \u0026gt;\u0026gt;\u0026gt; audioStream).  \n     withHeaders( (CONTENT_TYPE, audio.contentType) )  \n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe \u003ccode\u003erawStream \u0026amp;\u0026gt; audioEncoder\u003c/code\u003e takes the raw stream and \u003cstrong\u003epipes\u003c/strong\u003e it into the encoder which results in the encoded audio stream. \u003ccode\u003eaudioHeader \u0026gt;\u0026gt;\u0026gt; audioStream\u003c/code\u003e will \u003cstrong\u003econcatenate\u003c/strong\u003e \u003ccode\u003eaudioHeader\u003c/code\u003e with \u003ccode\u003eaudioStream\u003c/code\u003e. Hence, the first thing the server will do is start sending the audio header to the client \u003cstrong\u003eand then\u003c/strong\u003e stream the audio in real-time.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eA client can connect at any time and will hear current stream\u003c/strong\u003e, so it should simultaneously hear the same thing as any other client (with some delay depending on the client buffer). If the generator stops emitting audio samples, the http client will stop receiving audio data – but it will still be waiting for the server, so the audio play will pause until the server re-sends new audio samples. \u003cstrong\u003eThat is pretty cool!\u003c/strong\u003e – because of the way iteratees work, the stream doesn’t just die when all of the input is consumed.\u003c/p\u003e\n\u003ch4 id=\"a-chunker-to-reduce-http-packet-numbers\"\u003eA chunker to reduce HTTP packet numbers\u003c/h4\u003e\n\u003cp\u003eUp to now we’ve been streaming the audio in \u003cstrong\u003every small chunks\u003c/strong\u003e because by default JSyn writes out arrays of just 8 audio samples and the \u003ccode\u003e.stream()\u003c/code\u003e function consumes all data as it comes. This means \u003cem\u003ea lot\u003c/em\u003e of HTTP chunks per second are sent – which is less efficient and take more bandwidth.\u003c/p\u003e\n\u003cp\u003eIn order to fix this, we need to use a \u003cstrong\u003ebuffer on the server side\u003c/strong\u003e. In other words, instead of sending audio samples as they come we need to \u003cstrong\u003egroup audio samples\u003c/strong\u003e. We have currently grouped audio samples in arrays of 5000 which is quite reasonable (it’s about 10 chunks per second using 44100 samples/s). We can easily change this later. This logic is implemented in an \u003ccode\u003eEnumeratee\u003c/code\u003e we called “chunker”. In that sense, it is reusable and modular:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-scala\"\u003eval chunker = Enumeratee.grouped(  \n  Traversable.take[Array[Double]](5000) \u0026amp;\u0026gt;\u0026gt; Iteratee.consume()  \n)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAnd now, we can easily plug it in like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-scala\"\u003edef stream = Action {  \n  val chunkedAudioStream = rawStream \u0026amp;\u0026gt; chunker \u0026amp;\u0026gt; audioEncoder  \n  Ok.stream(audioHeader \u0026gt;\u0026gt;\u0026gt; chunkedAudioStream).  \n     withHeaders( (CONTENT_TYPE, audio.contentType) )  \n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"broadcast\"\u003eBroadcast\u003c/h3\u003e\n\u003cp\u003eNow, another improvement we made was to \u003cstrong\u003efactorize this chunking and encoding part: avoiding having this computing tasks done for every stream consumer\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eBasically, we move it out of the stream function:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-scala\"\u003eval chunkedAudioStream = rawStream \u0026amp;\u0026gt; chunker \u0026amp;\u0026gt; audioEncoder  \ndef stream = Action {  \n  Ok.stream(audioHeader \u0026gt;\u0026gt;\u0026gt; chunkedAudioStream).  \n     withHeaders( (CONTENT_TYPE, audio.contentType) )  \n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eBut to allow broadcasting, we have to use a broadcast:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-scala\"\u003eval chunkedAudioStream = rawStream \u0026amp;\u0026gt; chunker \u0026amp;\u0026gt; audioEncoder  \nval (sharedChunkedAudioStream, _) =  =   \n  Concurrent.broadcast(chunkedAudioStream)  \ndef stream = Action {  \n  Ok.stream(audioHeader \u0026gt;\u0026gt;\u0026gt; sharedChunkedAudioStream).  \n     withHeaders( (CONTENT_TYPE, audio.contentType) )  \n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere we only care about the enumerator (the left argument in the Tuple2), we put the wildcard \u0026quot;_\u0026quot; to ignore the return value.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://en.wikipedia.org/wiki/Broadcasting_(networking)\"\u003e\u003cimg src=\"/images/2012/07/320px-Broadcast.svg_.png\" alt=\"\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eUsing a broadcast, generated audio samples pushed by the audio generator can be simultaneously spread to multiple consumers. This is perfect for our needs, multiple players can connect to this web radio!\u003c/p\u003e\n\u003ch3 id=\"avoiding-the-server-load\"\u003eAvoiding the server load\u003c/h3\u003e\n\u003cp\u003eThe last important fix we made was to \u003cstrong\u003eavoid the server load\u003c/strong\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-scala\"\u003e  def stream = Action {  \n    Ok.stream(audioHeader \u0026gt;\u0026gt;\u0026gt; sharedChunkedAudioStream   \n      \u0026amp;\u0026gt; Concurrent.dropInputIfNotReady(50)).  \n       withHeaders( (CONTENT_TYPE, audio.contentType) )  \n  }\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf a client is opening the stream connection but doesn’t consume enough or doesn’t consume it at all (download is paused), the server will fill in memory the chunks to send to the client and the server can reach an \u003cem\u003eout of memory\u003c/em\u003e exception. To avoid that \u003cstrong\u003ewe have to drops chunks if the consumer is not ready\u003c/strong\u003e. Then the client will just lose messages if it is not ready (in our case, we give them 50 milliseconds).\u003c/p\u003e\n\u003cp\u003eAnd this is what \u003ccode\u003eConcurrent.dropInputIfNotReady(50)\u003c/code\u003e is actually doing – with yet another \u003cstrong\u003eEnumeratee\u003c/strong\u003e! \u003cstrong\u003eDropping old chunks is really what we want in an audio streaming application\u003c/strong\u003e: We want the consumer to subscribe to the current audio stream and not to continue from where they stopped.\u003c/p\u003e\n\u003ch3 id=\"client-consumers\"\u003eClient consumers\u003c/h3\u003e\n\u003ch4 id=\"html5-audio-tag\"\u003eHTML5 Audio tag\u003c/h4\u003e\n\u003cp\u003eIn HTML5, we have the Audio tag – and we can just consume our stream like this:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-html\"\u003e\u0026lt;audio src=\u0026quot;/stream.wav\u0026quot;\u0026gt;\u0026lt;/audio\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eOr if we want to make it auto loading:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-html\"\u003e\u0026lt;audio src=\u0026quot;/stream.wav\u0026quot; preload autoplay controls\u0026gt;\u0026lt;/audio\u0026gt;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIt may be a bit “wrong” to use `` for streaming, but it works because we are using it as if the server was hosting a static audio file. The only disputable hack is to have to set the max ChunkSize in the WAVE header which is 2147483647 (it’s about 6 hours 45mn!), so the browser believes the audio is not finished.\u003c/p\u003e\n\u003cp\u003eThe issue we are currently facing is this crazy latency (a few seconds) between user actions and the produced sound. This problem is due to the browser audio cache buffer: if we were able to minimize it we would have an almost real-time audio player.\u003c/p\u003e\n\u003ch4 id=\"playing-it-with-vlc\"\u003ePlaying it with VLC\u003c/h4\u003e\n\u003cp\u003eThis stream is spread through HTTP so we need a HTTP client to consume it. But a HTTP client doesn’t mean only browsers! We can also use VLC for this, as if it was a web radio! One advantage of using VLC is it suffers far less latency (presumably because the cache buffer is smaller than the audio tag).\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/2012/07/vlc.png\" alt=\"vlc\"\u003e\u003c/p\u003e\n\u003ch2 id=\"making-the-real-time-control-ui\"\u003eMaking the real-time control UI\u003c/h2\u003e\n\u003cp\u003eOur experiment \u003cstrong\u003emixes different oscillators to generator one sound\u003c/strong\u003e. The web user interface allows a user to control the parameters of those. Two knobs control the \u003cstrong\u003evolume\u003c/strong\u003e and the \u003cstrong\u003epitch\u003c/strong\u003e (tuned to a dorian mode scale) and you can select the oscillator \u003cstrong\u003ewave primitive\u003c/strong\u003e (sine, sawtooth, square, noise). It’s not fancy at the moment – but JSYN offers a lot of features for expanding our simple demo.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/2012/07/Capture-d%E2%80%99%C3%A9cran-2012-07-31-%C3%A0-16.42.15.png\" alt=\"\"\u003e\u003c/p\u003e\n\u003cp\u003eThis interface is \u003cstrong\u003emulti-users\u003c/strong\u003e, so if you use it with other people, \u003cstrong\u003ethe interface will stay synchronized over multiple browsers\u003c/strong\u003e (turn the knobs, change the wave primitive, …). All this is done with WebSockets, and on the server-side it’s using, again, \u003cstrong\u003eIteratees\u003c/strong\u003e!\u003c/p\u003e\n\u003cp\u003eThe workflow is simple: When someone does some action on the user interface, events are sent to the server. These events are interpreted by the \u003cem\u003eZoundGenerator\u003c/em\u003e resulting in updates to the audio synthesis configuration. These events are then broadcast to each client, and some Javascript handlers are called in order to keep the interface synchronized.\u003c/p\u003e\n\u003ch2 id=\"source-code\"\u003eSource code\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e\u003ca href=\"http://github.com/gre/zound\"\u003eFork me on Github\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"whats-next\"\u003eWhat’s next?\u003c/h2\u003e\n\u003cp\u003eThis was just a simple demo to show the power and flexibility of Play2′s Iteratee concept. Because of the modular nature, extending the demo is easy. For example, we could plug a new audio encoder such an OGG encoder. The code would be simple and we could even choose on a request-by-request basis which encoder to use:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-scala\"\u003eimport Concurrent.broadcast  \nval (chunkedWaveStream, _) =   \n  broadcast(rawStream \u0026amp;\u0026gt; chunker \u0026amp;\u0026gt; waveEncoder)  \nval (chunkedOggStream, _) =   \n  broadcast(rawStream \u0026amp;\u0026gt; chunker \u0026amp;\u0026gt; oggEncoder)  \n  \ndef stream(format: String) = Action {  \n  val stream = format match {  \n    case \u0026quot;wav\u0026quot; =\u0026gt; waveHeader \u0026gt;\u0026gt;\u0026gt; chunkedWaveStream  \n    case \u0026quot;ogg\u0026quot; =\u0026gt; oggHeader \u0026gt;\u0026gt;\u0026gt; chunkedOffStream  \n  }  \n  Ok.stream(stream).  \n     withHeaders( (CONTENT_TYPE, audio.contentType) )  \n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eNow it’s up to you!\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eHopefully you get a feel for the possibilities of stream processing and piping with Play. You can now reuse these concepts and make your own stuff: Maybe you don’t need to generate sounds on the fly, but instead you simply want to play a collection of audio files and stream them like radio? \u003cstrong\u003eWell you can make a web radio engine now!\u003c/strong\u003e. \u003c/p\u003e\n\u003cp\u003eBut that’s just the beginning – I would love to see someone taking the concept, and running even further… Do you know that in Youtube, during the time you are uploading a video, Youtube is already re-encoding it and can start streaming it \u003cem\u003ebefore\u003c/em\u003e the file has finished uploading? Hmm, that’s starting to sound almost simple…\u003c/p\u003e\n","data":{"title":"Zound, a PlayFramework 2 audio streaming experiment using Iteratees","description":"Zound uses an audio generator (JSyn, an audio synthesizer), encode the output and stream it all using Play Iteratees to pipe everything in real-time.","thumbnail":"/images/2012/07/ZOUND.png","author":"Gaetan","layout":"post","permalink":"/2012/08/zound-a-playframework-2-audio-streaming-experiment-using-iteratees/","tags":["audio","iteratee","playframework","hackday"]}},"__N_SSG":true},"page":"/[year]/[month]/[slug]","query":{"year":"2012","month":"08","slug":"zound-a-playframework-2-audio-streaming-experiment-using-iteratees"},"buildId":"vIdAyYeXch4v51HIYB7Zz","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>